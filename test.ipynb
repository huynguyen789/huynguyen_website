{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ResponseFunctionToolCall(arguments='{\"location\":\"Paris, France\"}', call_id='call_zseZCNwp4IA1s9Dc8EuA8Lyc', name='get_weather', type='function_call', id='fc_68040508d8288192b930c012d459fd6f0cc15c2f5b761137', status='completed')]\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"get_weather\",\n",
    "    \"description\": \"Get current temperature for a given location.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"location\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"City and country e.g. Bogot√°, Colombia\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\n",
    "            \"location\"\n",
    "        ],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}]\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    input=[{\"role\": \"user\", \"content\": \"What is the weather like in Paris today?\"}],\n",
    "    tools=tools\n",
    ")\n",
    "\n",
    "print(response.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linh tinh test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'transcript': 'cool okay let\\'s get on with it so hi everyone my name is Elan i\\'m on the developer experience team at OpenAI um unfortunately I can\\'t be there in person as much as I would love to i\\'m in a wedding in Costa Rica um which is happening later today so I just wanted to take this opportunity to just talk through um one of my favorite concepts in maybe all of like AI and language models so uh title of this talk is function calling is all you need it\\'s a talk workshop there\\'s going to be um a lot of coding please save your questions no I\\'m kidding like just interrupt at any point uh we have a Slack send them here um yeah just send them at any point uh and if you want to like unmute yourself and or raise your hand I\\'ll call you off like as we go the idea is to keep this super super dynamic um since we have a bit of time um I\\'ll be fielding a lot of like questions and requests and trying to be coding as much as possible so yeah this talk is going to need some lecturing uh a lot of coding from scratch and then some debugging hopefully not a lot uh so this is a little bit of what the uh workshop is going to look like we\\'re going to go over a little brief history of the tool formers of of I\\'m sorry of function calling um then do a little crash course on function calling um talk about just agents how they\\'re just loops how rag workflows and more are just function calls delegation and asynchrony couple random things I found and then we\\'re going to do a Q&A um if you just want to see like the meat of it this is pretty much it we\\'re going to do like this is everything I want to talk about uh and everything that we\\'re going to implement so this is uh just a little bit upfront great so a little history um if we look at the abstractions and sort of patterns that we managed to do with language models it started as text completion right like the original GPT uh and the GPT2 and the GPT3 were all just base models where you gave them some input text um and then they would just continue the sentence um this was at the time really really like interesting this was the first time we were able to do like very like uh English sounding like real sounding language um but getting it to follow instructions was pretty hard so if any of you were testing this back then you might remember how setting up a chatbot was um non-trivial right you had to like get it to answer questions but if you just say like you know what is the best way to uh get to the like park or something it would continue like like what is the best way to get to the park um that is what Sally said yesterday right and you wouldn\\'t actually get a a response um so you had to like structure it in a way where you would say like this is the question this is the answer question answer few shot and then give it um then they introduced uh I think this was actually us we introduced uh function um instruction following with uh instruct GPT now you could give it some input uh and it would actually do what you\\'re asking as opposed to just completing um finally we started to introduce this notion of like users and assistants and roles um and this was all done through through post training where you actually gain these personas um and then finally uh we we eventually landed on this like you can give it additional tools um in order to do like external um any like interact with external states so this is what the previous playground used to look like but as you can see there\\'s no chat this is just like a window and it\\'ll complete now looking over at like the original papers which is pretty interesting um one of the first times that we actually started to do this like function calling was through this web GPT um which was this uh version of GPT3 that we trained to be able to um Elan I have to cut in here a little bit we got the Zoom working yay sweet um all right we\\'re going to cut over in the audio so can everyone hear Elon when he speaks elon say something hello hi everyone hello hello how\\'s it going okay so so we\\'re going to cut over a little bit in terms of like um people having their own personal audio situation going on so you can mute your machine you should be able to hear it on the room uh audio whatever you want to do but you can also obviously connect but uh yeah now we have him on the big screen so okay yeah uh let\\'s let\\'s keep going for a bit i I\\'ll cut in again if there\\'s audio issues or maybe you want to say some test words uh you\\'re all great thank you for coming can we bump up the audio where\\'s the guy [Laughter] it\\'s not a conference if you don\\'t have AV issues okay uh it\\'s it\\'s still too soft we can\\'t we can\\'t hear you um I\\'m going to try and bump up the audio i\\'m so sorry i\\'ll put up a more interesting slide in the meantime you guys can look at this while we figure this out where\\'s Where\\'s the video hey um so anybody got uh good jokes maybe I always find a joke it\\'s always the same one i think it\\'s uh What is it why did the No I had it i had it here wait wait wait wait wait wait there we go why can\\'t you trust an atom yeah because they make up everything these are all real by the way and for for those who like have a keen eye the first one is like actually from the GPT2 paper um we like gave the model this like description of like unicorns in the Andes Mountains and that was like the big like first time that it was like doing multi- paragraph completion like continuations that referenced earlier parts of the conversation it\\'s cool stuff it\\'s cool history can you guys hear me i feel like some people are having a great time some people don\\'t know what\\'s going on can like I don\\'t know am I good to keep going or should I keep waiting uh I think I think you\\'re good okay great great i do think this is the vibe of the whole talk by the way there\\'s it doesn\\'t get more structured from here um okay so as I was saying uh we did like this web GPT paper um essentially we trained a GPT3 version of the model uh or like a GP3 model to be able to use this like this very specific set of functions um to do web search and this was like back in 2021 so really we had like webg a long time ago um but this is like one of the first times or maybe the first time that like we were having like it\\'s not just generating text but it\\'s generating actions and then we\\'re parsing those actions and then introducing it back into context so it can use the responses itself um and how we trained it there um there\\'s like these these like you know clever ways of like we we essentially gave people uh an interface and let them do the searching um and then I think we took it like Reddit um yeah explain like five um and then just like had people complete tasks and they could use these commands and so we taught the model and this was GPT3 right um how to essentially imitate users behavior and then produce responses that were preferred um and and this was pretty cool like this is how you we like start to saw um to see like this uh this use of like structured like actions essentially so but this was very specific right we were training like very specific um tools so then you might be familiar with this paper this was from Meta at the time um where they essentially had a way to teach the models how to use any tools um and they taught like they they used a few tools i think it was like QA calculator um what was this like translation um couple other couple other tools but um it was actually a pretty clever way where they like looked at the log props at each spot to like see where it was like best suited to like retroactively put in a function call given some like completion um so here we can see a few examples like um essentially it\\'s like if you have a calculator call if you insert a calculator call that like you know it\\'ll it\\'ll insert the um the actual call to the calculator which then you know you\\'re pretty familiar at this point will get the answer if you insert it in the right spot in the sentence it actually reduces the perplexity of the sentence um and so they didn\\'t actually have a lot of human-labeled examples or I think it was just a few but it was uh really cool because it was this way of like um it could learn to use uh any of these tools through this like crazy like log props technique um and it was uh I was pretty excited when I saw this paper now this is like how it learns to use any of these tools um but then finally uh in June of 2023 uh OpenAI launched just general function calling where we essentially like pre-trained it to be able to use these tools or actually post-trained it to be able to use uh tools so now you don\\'t actually have to like give it like you can give it examples but uh we just showed it like this like syntax in with functions that we still use today and it\\'s just able to call functions so this is a brief history of of function calling um and my I guess my argument is this is really most of what you need for all the exciting stuff that\\'s happening today um there\\'s there\\'s obviously like additional like systems you can use and like um more post training you can do but fundamentally like functions are so so so powerful and we\\'re going to look at a few cases today um I\\'m going to try to keep an eye on questions but um yeah okay I think you\\'re good so far cool cool so let\\'s do a super quick crash course on function calling uh two main purposes and I\\'m ripping a lot of this from the docs so um there the fetching data right reading APIs retrieval memory or taking action any APIs you can use to write managing application state which is actually pretty overloaded that can be like UI front end backend whatever you want and then workflow actions which is um any like multi-step processes or even like meta actions like switching its own prompt or like loading in different tools or like handing off a conversation right um so this is also a diagram straight from the docs but um I\\'ll quickly brush past this i\\'m going to assume like most people have at least seen this but here it is it\\'s uh you you essentially tell the model which functions you you want it to be able to use um and you also provide like whatever the user input is um the f the model tells you what it wants to do with that function but it doesn\\'t actually do it this is one of the like a big sticking point function calling it doesn\\'t actually use the function itself it tells you like it tells you the intent of what it wants to do with a function you are then responsible for parsing that executing the code doing whatever you want with it and then providing the result back to the model and then the model can use that respon like result in uh in the generation uh take a look at a quick question here oh no it\\'s just it\\'s just swings okay cool um these are just a few uh best practices this is all taken from the docs as well um you want to write clear functions an important one is you got to apply software engineering best practices when you write these functions so um you know what maybe I\\'ll pull up the docs for this um so there\\'s a lot of big text but essentially um this is a lot of value here i tried to pack as much like useful information here as I could so I\\'m going to quickly go over it right um you had to explain the purpose of each parameter use a system prompt and include examples that\\'s you know pretty pretty like not non-controversial um software engineering best practices is a little bit uh more interesting right you got to make functions obviously intuitive and they got to follow the function of lease principle um like if you give this to a person and they don\\'t know how to use it then the model might not either right models are getting smarter than us but still you got to make it uh got to make it easy um also you got to use enums and object structure to make sure that like you are not letting the model make invalid calls right um like here you you have this like toggle light uh um function that takes in like two two boolean params and like obviously like this is pretty wrong but um there\\'s actually many many more like subtle cases where um like the you can like you\\'re letting it represent invalid states um okay still no questions it\\'s Swixs okay um great and then there\\'s Sorry there are a couple questions in the professor convers i see i see um here maybe let me see if I can pull up the Slack okie dokie between functions and tools in my opinion hey Sam yes um I I think we\\'ve all been kind of like gravitating toward functions and tools as like the two main ways originally it was just functions tools was like later we renamed it um I think now uh and the way that I tried to specify in the docs is functions are like the the raw function calling right like you provide an interface and you\\'re responsible for executing the code um tools and this is sort of how we treat it in our API is a superset of functions tools include functions but it also includes things like um code interpreter or file search or any of these like um like hosted solutions i\\'d say this is not be like end all beall definition but this is a definition that we\\'ve adopted right there\\'s like tools which are like hosted tools uh including functions and then functions as a subset cool maybe we can just jump into it um if people have questions on this happy to happy to feel them as well but I kind of want to get coding okay uh when you start to approach dozens or hundreds of functions what technically should we apply in order to effectively tool call permissions is one technique okay interesting and then question two when you require one tool to provide inputs to another I have seen tools become layered how should a reasoner hardcode great okay these are actually great questions and um I think to to answer them I might I might like do a little hack and like uh use some existing uh code i\\'m going to use swarm for a little bit of this um because it does some nice function calling actually no you know what i\\'m going to get to these fun uh in a second um let\\'s just go straight into the uh and as always we should start from the docs uh cool uh so this is where I always step in can you zoom in on screen every single page yeah yeah bigger yeah yeah sorry i always do yeah yeah no this is a good call out uh and then I think the terminal here should be good okay sweet uh we have we have a function great um and then just run it and we have uh the function call right now we\\'re not handling it yet so this is where like I might skip ahead a little bit um and start doing some of the like agentic stuff but first uh first off uh we we got to have a right like you know you you I can sort of hear myself um I think so there we go perfect okay so um I think the idea here is like let\\'s make a very very very simple like uh input loop like let\\'s do uh while you also light mode light mode Jesus okay sure I\\'m just very experienced at this we go well that\\'s the least painful this is good right this works Yeah oh my poor eyes um actually maybe I I I can\\'t quite see the room but show of hands like who has used or implemented function calling in the past and someone\\'s going to have to gauge this for me okay it\\'s everyone great i\\'m going to skip ahead a little there\\'s like 10 20% 20% that hasn\\'t done it okay 103% um the important part that you have to know is you can define the function um schema right then the tool will specify what you want and then in this case you know if I have like a get weather tool actually I\\'m going to grab this from the docs as well when in doubt just go to the docs you know it\\'s always good so step one no this is all node yeah and this will be useful for later too um step one call the function step two execute your code right so here what we\\'re doing is we\\'re taking we\\'re parsing out what the what the function told us we\\'re parsing out the args and then we\\'re calling this get weather function which we don\\'t have yet um but conveniently it\\'s up here right so we have it up here get weather requests and then the last step is we provide the result back to the model um uh provide the result back to the model and then ask for a completion right so just in order specify the tools call it the first time get the tool calls parse them out call the function append and do that right so if we do that and we just add maybe like add I\\'m going to do this by hand why not yeah so I\\'ll print the completion here sort of by hand and then print the last completion then what we can see is we\\'ll get the first one that includes a tool call it\\'ll call the actual temperature uh call the weather API and then we\\'ll get a response and then it says the current temperature in Paris is something I can\\'t see it because of Zoom um cool so this is like the very very basic setup for a function calling let\\'s take a step forward so this is an agent uh very very very basic implementation of an agent um that I\\'m going to go through really really quickly cuz we\\'re going to start using it this is very familiar to what you\\'ll see in Swarm or any of the other like like basic frameworks um but the idea here is in uh as you can see like in the original one essentially like when it had a tool call I wanted to provide it back so what I do here is while like just keep looping and this is like the very famous like agents are a loop this is this is that loop um specify the tools call the model get the message print it out handle the tool calls append it and once we have no more tool calls break this is the whole loop i called it run full turn in my head one turn is just like you let the model do everything um and then we have this like execute tool call yada yada whatever um so now we can use this so agents py did I export it there we go i love it okay so now we\\'re just going to specify one um you know what yeah we um and we\\'ll we\\'ll do it we\\'ll do it with a with a simple loop so we have this that and we have this okay so now we can just do Asians run full turn the one other thing that I\\'m adding here that I didn\\'t show is um this simple utils uh that defines this very very very useful function so functions to schema essentially takes in a raw python object function and then provides it into the uh like correct schema um so that you can just define functions directly and this is the same thing that we have in swarm and there\\'s an a few other frameworks now as well um so as an example we can do like you know get weather and I\\'m just going to like you know return 20 like degrees Celsius Right i\\'ll do that here then I\\'ll have my messages and messages is this emergency reference i think it is what did I call it oh oh oh I see this is not part of the Asian class cool so it called the weather printed it out uh gave us a nice little completion um this is essentially what we want to see if I say um and then I can just keep keep going right so we we we have this basic um basic loop um now let\\'s get back to the presentation real quick that was like a very very immediate crash course now let\\'s let\\'s get interesting um and and just for convenience I\\'m going to like pivot to the swarm implementation um the main reason being it\\'s pretty much the same um except we have like some convenient uh like looping tools so uh swine imports agents and then There we go so now we can do a simple agent and run a demo loop and let\\'s see this work um we have this very basic setup let\\'s do everything now so um another show of hands how who here has implemented uh rag okay what about memory okay fewer and then what about like you know multi multi-step things and workflows cool um how about this there\\'s a lot of stuff we could talk about but I want to keep this valuable to you all out of this list can you like just type in the Slack what you wanna what you want to see and I can just change the order in which we\\'ll cover this um because they\\'re pretty they\\'re pretty interchangeable we can we can build up um but essentially there\\'s there\\'s more interesting things later on but I want to make sure we we can build up to them so just uh if you can pull up the Slack and just dump in you know what you\\'re interested in seeing okay lots of memory i see um function generation from the docs delegation async okay build delegation and random cool stuff i will get to the random cool stuff um okie dokie back to light mode um yeah let\\'s start with like a very very basic form of memory right um how would we how do we do this let\\'s see honestly like we can have um just a list right memory can just be this list and I\\'m going to implement this similar to how it\\'s done in chatbt um but ju just to show like I think the whole point of this talk is like this workshop is like doing things from first principles and just really removing the complexity i think there\\'s like a lot of like not fake complexity but like added complexity on things that like doesn\\'t really need to be there a lot of the time like concepts are a lot more simple and it\\'s all about function calling so uh we can do you know add like add to memory and I\\'ll append it and then get memory uh like this is super super simple um is this is this good enough let\\'s see maybe maybe and so what we can do is give it the tools let\\'s see and then what when will we want to use them let\\'s say you like I can just say so this comment is going to be used as the uh string uh as the as the description in the function so you can say like you know like when the user tells you something like factual about themselves their life or I can\\'t see anything okay or their preferences call this function um memory um and we\\'ll add a couple more cool things like expiration which is one that I\\'ve kind of wanted to add for a while so you know for now let\\'s say false um memory.append uh think And then we have I guess what we call it uh you know memory text and we can say you know keep the memory text short size great cursor knows what I want and then we can just return maybe like this is a super super super naive implementation um but now uh when we start off I guess we could even start off with [Music] a I mean this is going to be kind of hacky but I can just like um in your first turn always call get memory this is not uh great it\\'s just because of the demo loop but maybe I\\'ll break I\\'ll break the demo loop out so we can actually do this by hand but so how how could we prove this um maybe let\\'s say like you know write this memory bank or like keep this memory bank in a local file JSON file read it in at beginning and write it out at every Nice okay I trust this shall we test it out so wait someone uh Okay we got the memory memory can any can anyone see any bugs because we\\'re about to test this out so we got the loop it\\'s going to call it um and maybe like let\\'s memory okay sure yeah let\\'s try it let\\'s try it out see what happens so hi did I not give it the functions is it hallucinating this oh yes because I think we called it functions hi there we go okay so called get memory there\\'s nothing there i can just say like um I am 6 feet tall despite what people think uh cool uh so now let\\'s just check right it should have written this out there we go so now we have it in the memory bank so now I can actually uh end this right and then be like you know uh how tall am I tada we\\'ve implemented memory right there\\'s a Yes I\\'ll take a clap i saw Louis Lewis you\\'re like my proxy for the Luis uh Luis Costa you\\'re my proxy for the audience you\\'re like the only person I can really see so please don\\'t uh turn off your camera um amazing now we can do more interesting things right we can uh if we want do a little bit like smart querying uh where instead of just like loading in all of the memory um we can like do a little bit of like retrieval uh to load in the right ones um and use like semantic similarity or use some kind of search um I I could try to implement that um that might take a little bit longer but not that long but I do want to pause here see like given this and like this is going to be the style of things that we do like what um what do we want to see next i can just keep going with this example i can pivot uh it could be fun just to keep building on this see how far we can get let\\'s see uh delegation async have it chat and work in the background we\\'ll get there we\\'ll get there i got that working this morning on Python because I didn\\'t want to switch to to Node yet uh delegation async okay let\\'s get into delegation then so there\\'s a few different ways we can do this i\\'m actually going to leave the memory and we\\'re just going to keep building in this on this on this agent uh and I am using the swarm agent just cuz I didn\\'t want to debug the one that I implemented but like if we actually look at what this is um like it is very very very simple and then the like run demo loop itself is like uh just printing out messages um and what it\\'s doing is like appending client.run whatever and like if we look at this client.run run um if it doesn\\'t stream it essentially does exactly what we did before like keeps looping get the completion throw the messages in uh append them handle responses etc there\\'s a couple more things around context and handoffs that we like don\\'t don\\'t really have to look at today um but we can so cool we have memory let\\'s do delegation um so there\\'s a couple ways we can do this right um there\\'s if you think of like functions and agents and and everything you can um maybe I\\'ll skip to this slide skip skip skip skip these are like a few of the forms of like agents on delegation that people might be familiar with we have handoffs which is like the swarm style you take a conversation and fully swap it to a different agent um and what that means is just like replacing the system prompt replacing the tools um you can have nested calls which are the easiest to implement and like often somewhat overlooked um and then you can have manager tasks that\\'s more async uh we will get to that today so let\\'s do a very basic one right let\\'s say I want to do like um maybe let\\'s give it a chance to like call a bigger model to do a harder task right so we can say like you know uh I can delegate to smarter model and and I\\'m going to give like task description [Laughter] um so I\\'m laughing because this is saying like it\\'s just going to make it smarter by telling it to be smarter that\\'s not despite how well that would work usually we\\'re not going to do that so we\\'re we can just make an API request directly actually let\\'s just do that let\\'s just do that so um let\\'s do from OpenI client and here we could do client chat completions create let\\'s call 01 um I won\\'t provide a system message i think that\\'s okay content is the description look at that and we did it so now um man I love cursor okay did everyone catch all of that by the way all we did was like implement this this function that calls opening API and then here so now I can say like uh so now I can add a bit of a description here i\\'m like like uh if you know if the user has to like um I don\\'t know do something like that seems difficult or says it\\'s hard use this instead of try uh and it infers you know how to use this based on like the fact that I called it test description it\\'s you know so let\\'s give this a shot hi um let\\'s see uh you know give me a poem about I don\\'t know dogs and the earth where each other starts with the next letter yeah it might just try it because I know GPD4 can do a variation of this but um Oh it\\'s giving it a shot okay let\\'s see if it gets it right okay i don\\'t have uh I don\\'t have the patience let\\'s say answer briefly one subkins max okay great and then to you know uh write a haik coup there we go it\\'s shorts where each other word starts with the next letter of the alphabet and ends with the previous letter of the alphabet i don\\'t know you guys want to try this while this does it is it around did I lose Did I lose something let\\'s see hi okay you there cool um did I not copy it oh no okay uh give me a thank you now make sure each word starts with letter of the alphabet starting with a and each word ends with the previous letter but this is hard okay so it should be making the function call oh I see i see i see okay so I\\'m only printing the function call when it returns so that\\'s probably why we\\'re waiting so long here um but this is actually a really good uh example of like okay we are doing um this like task delegation technically and like it is happening in the background and I\\'m going to give this a sec to figure this out um but like this is obviously a bad experience right like you don\\'t want to be waiting here you essentially want to keep doing other stuff so wow it\\'s still not still not back we\\'ll we\\'ll we\\'ll we\\'ll let it figure it out um so maybe let\\'s skip straight to async um we\\'re actually got like both more and less time than I thought so I don\\'t let this keep turning away um now let\\'s think about this for a sec right if we want to do something async it means that like what do we want to happen there we go so call the model da ya yada buzz breezy whispered is this correct i don\\'t know wow yeah this is sort of correct what did it say note great great so it it did something right um and it did something that 40 probably could not have which is great that\\'s delegation but we were just sitting there waiting for it um so let\\'s try doing this async now I I I I actually want everyone to like kind of just stop and think like how would you implement this um in terms of like what behavior do you want and maybe I want to see people like drop this in the Slack just like take a couple minutes um and like drop in the Slack like a proposal for how you would want to do this and then I might just pick one or we\\'ll like talk about them let\\'s call 03 yeah not not yet not yet yes import async.io definitely feels like uh important but I guess the questions are like you know when we delegate something uh obviously we want to have this happening in the background um do we want like when it finishes do we want to be like do we want it to be injected into the conversation do we want it to give us a response um how many like do we want to be able to like interact with tasks that are running um like do we want to be able to batch stuff i am not setting up a Kafka cluster i\\'m going to give people maybe a couple more couple more minutes to just like dump some ideas here um batch calls yeah I guess B but how how would batch calls work maybe Stephania if you want to add some like detail there just keep working until it generates a stop word right okay yeah this is this is a good idea js and set timeout yeah switching to JS is always a good option because you already have the event loop implemented smaller model smaller model okay you guys are all suggesting some good ideas um but we can actually go simpler right like like essentially what we would want and I can implement like the basic interface of this where\\'s my code um is like instead of actually doing this right I can like return delegated right response pending um and then later I can say like you know check tasks or something so this this pattern is actually one I I quite like a lot where you um you call a model it\\'s a function it\\'s non-blocking and then later you can you can check up on them now the thing is Python is like single threaded and uh async is real uh it\\'s just a little bit tricky um but let\\'s uh let\\'s give it a shot so I do have some async emergency reference now here\\'s the here\\'s the um thing to notice when something like for this to work correctly we have this loop that asks for our input and blocks on our input um and like is displaying that on the screen and we don\\'t want to have that be displayed while it\\'s also like injecting messages um so we essentially want to separate out like where you give user input and where actually interesting stuff happens so that\\'s what I\\'ve done in this in the basics of this async folder so let\\'s take a look at that cool um it looks complicated it\\'s not that really that bad so um I\\'m using websockets essentially what I\\'m doing or sockets sorry essentially what I\\'m doing is um the I have a server and all it does is like has a handle user input function uh and a start message processor um these aren\\'t super important and we can take a look at them in a second but essentially it just like waits uh on a specific like uh port and and host um and then I have a client that connects to that and it just does while true loops and lets me enter in right so if we look at what that looks like I have it here um so let\\'s go to async let me zoom in here a little bit also um while I do this is there are there any questions that I\\'ve that I\\'ve missed um and if there are just feel free to shout them out no great cool covered everything everyone is uh perfectly up to date with everything I\\'ve said okay um sorry okay so I have this and now I can say like hi let me show you the the rest of this i guess that\\'s probably important uh D so we have the server we have the client we also have the a very very very basic agent imple implementation it it looks just like the old one with the main exception that I\\'m using async openai instead um handling tool calls is happening in parallel so for each tool call grab it create the task um await for them and then just like await for them all at the same time um and this just lets it like create a bunch of tasks that are all going to run in parallel and once they\\'re done like if they if they yield back at any point we can keep doing other stuff um by the way if these are just like heavy functions that are doing heavy processing it\\'s not going to matter the fact that they\\'re in parallel like in sync it means that it\\'s still going to happen one after the other but if they\\'re like network calls to other models then it\\'s perfect um the runful turn is the same one you like call a model check to see if there\\'s any function calls if not break return the response um the only difference here is we\\'re awaiting uh the chat completion right this is the only difference and then if we take a look at our agent handler we\\'ve declared we\\'ve declared our agent like normal right right now it has no instructions it looks very familiar and this loop is also pretty familiar it\\'s like get the messages um here here\\'s the only difference right um and you\\'ll see why this matters in a sec i have a message Q uh and this is will be useful because we don\\'t want to process multiple messages in the same conversation at the same time like we want work to happen at the same time but we don\\'t want um like multiple generations to happen with the same history because then you\\'ll get conflicting like like if two messages if two functions return and they both need to be handled uh you essentially while you want them to happen in parallel the results should should only come in one after the other so we have a cue for that uh and it treats user messages as well like that so this is the handle user input that is being called from the server essentially just throws in a user input message into the message Q uh and all we\\'re doing here is like pulling off the queue run it like you know uh put put the messages back in the in the message array uh and then like sleep right this is like just a very simple uh loop but it does it with async so this is what we saw and and it feels pretty normal right i can say like you know my name is Elan and I can say you know what is my name great um now this still doesn\\'t exactly answer how we\\'re going to do things asynchronously but it does give us the space to play with it because now things are happening async and we can do a few more delegations that that are actually async so let\\'s start with a you know simple blocking get weather function i think I required all functions to be async here um yeah 67 and 70 uh and then skip that and let\\'s test it out the only annoying part is we have to like restart two things now so hi uh what\\'s the weather great so we can see it called a function got the response so far everything is normal now let\\'s do some delegation stuff so let\\'s say let\\'s say we want to call this function uh three times for different places so we have location [Music] um yeah and let\\'s say you know like pick a random number from 50 to 80 to return um so if we do it in the previous case do I still have the round yeah maybe maybe not okay uh so if we look at okay this is the non async one do we have weather we don\\'t have weather let\\'s give it weather let\\'s give it the same weather function cool [Music] um we only need random we don\\'t need this and this not async okay so now we have this weather function let\\'s test this out in the non- async case um I\\'m going to get rid of the rest for now um so if I say you know uh weather in SF cools it fantastic and now it\\'s like weather in SF New York uh you know and the five other cities is it\\'s um it\\'s still going to do the parallel function calling and here it\\'s fine because they all return immediately right but now let\\'s add like an artificial weight so let\\'s say time do sleep one right um cool so now what\\'s going to happen is we\\'re going to ask for that again but it\\'s going to take so long weather in five random cities of your choosing so it\\'s going to take a while and the reason is each of these is having to run um in like one after the other there you go it took like over five seconds now let\\'s do the same thing here so the equivalent is um there\\'s async io sleep someone let me know if I\\'m doing this wrong but um this should emulate like very very similar behavior so if we instead run it here that\\'s and that\\'s I can say you know give me the weather of five random cities of York um we should see it return okay okay so it called all of them and it got back because they all happened in parallel um this is the magic of async.io right um anything that can be parallelized or I guess scheduled in a way where where it\\'s non-blocking uh like sleeps these sleeps are non-blocking um essentially we can we can do and so you can imagine switching this sleep for like an actual API call um so uh we can actually do that right now with 01 right like we could call 01 multiple times and they would all run in parallel now the main problem is that we\\'re still going to be waiting back like waiting to to get all of them together right like like the fact that it we can run them in parallel means that like we can have five 10-second tasks running in parallel so it\\'ll take 10 seconds but it\\'s still going to take 10 seconds where I can\\'t talk to the model so instead let\\'s have this notion of tasks there\\'s many ways to do this um I\\'m going to do a pattern that I quite like so let\\'s do this um let\\'s define a you know create task so I have a create task function that makes a task um no and I want to create like a task ID and I want to keep it short okay so now what I\\'ve done is I have this function that makes a random task ID sets it creates it and then calls like let\\'s say get weather or something um and then it\\'s you know it\\'s suggesting this check tasks which is the next thing that I want to do so the next one is check tasks and so maybe I can just say like check task you know do for amazing um okay so let\\'s take a moment to to look at what just happened we now create a task with a random ID we give back the ID to the model and then later we can call check task get that task see what the status is and see if it\\'s done um so let\\'s like add I don\\'t know 5-second delay here and right now let\\'s just get weather so description would be maybe create task for you know let\\'s see again this is all like sort of live coding so we\\'ll see if it works i can say like you know create a task um and set the description to the to be San Francisco let\\'s see what happens oh I didn\\'t give it these functions there we go um let me just check cool cool cool cool okay let\\'s do it again say you know create a task where the location or the description is just so now it creates the task and I can say hi and look at that it That\\'s not working correctly maybe it was Maybe it just took a while to respond um here let\\'s add a longer delay to know for sure 10 seconds let\\'s run it back so create a task with description se uh cool so I can say hello there cool so it can still respond to me right um check the task so I can still interact with it and it can check to see if it\\'s not done um I guess we need to keep checking to see if it\\'ll work so you know how about or you know tell me a joke who would have thought who would have thought um cool so now like check them again so it called the task it is done and we have 78 and sunny i will take a clap for that i want to see everyone clap please clap thank you thank you thank you thank you so what you just saw was uh live asynchronous uh programming uh it\\'s very impressive the models can also you know do pretty well anyway here\\'s why this is interesting we now have a system where we can give it tasks and it\\'ll ceue them up uh and then we can check on their progress so so already we have the basis for like a really really interesting thing right like this this thing here right right now it\\'s just get weather but if we say just like you know uh run what do it call like um you know call model and then no but we want AI port sync open AI get the client um I guess we Actually we can just leverage the agents they\\'re already async let\\'s not deal with that again um or whatever let\\'s do Let\\'s do it this way i did all did it all fantastic okay so now we can switch get weather for call model and let\\'s try this again so let\\'s give it the same task as before which is you know hi just make sure it\\'s running um you know write me a uh ha coup about I don\\'t know uh a coup where the theme is so incredible it makes me cry make this a task okay so it\\'s created the task and now I can actually say you Now make a second one but pick a theme yourself so now I have this interface where I can essentially keep chatting with this and it can essentially spin off additional tasks um you check check all tasks um let\\'s see what the progress is like look at that so we have we have both oh was this a one that\\'s pretty fast um but yeah so now we have this system that you can actually just call call multiple ones so I\\'m going to pause here just open up for questions for a little bit conversation like other directions we can take this other ways we could have done this um but what I want to look at next is like how can we do this in a way where we don\\'t actually have to check um cuz right now like we don\\'t need these two terminals this is just over complicated like I think the the the point of having the second terminal is so that we can push things to the conversation without a user having sent a message first so I\\'m going to pause here i\\'m going to field some questions and then we can get back to it okay um would you not use generators for nested tool calls um Tom do you want to I don\\'t know if there\\'s a mic in the room or like uh just get get it get it to them yeah there is also they can unmute themselves uh who\\'s your Uh sorry so is the one that was uh answering is So in the room yeah can Can you hear him no I can\\'t hear him hello can you hear me in the Zoom yeah I can hear you now i can\\'t hear you anymore i don\\'t know if you\\'re saying a question right now no I was just wondering if it\\'s possible to treat agents objects as a generator in Python so you can nest like you can yield from individual agents and then yield those tool calls does that make sense yeah yeah that makes sense this is also very impressive that you have to do this i think someone needs to mute themselves i don\\'t know if it\\'s Alain or uh like someone in the room unmuted and uh put put this man through through hell um No it\\'s me it\\'s me oh is your laptop unmuted i can\\'t hear anyone anymore maybe just speak into your laptop okay but I\\'ll I\\'ll answer your original question of like you know if you put um can you like essentially make agents into generators so that you can like yield the results as you go the answer is yes and this is actually like um the right way to do this the way I\\'m doing it now is I\\'m only exposing the final response mostly because um implementing like the the generator gets a little bit tricky and I don\\'t want to have to like debug that not too much but you can essentially just surface each of the steps each of the function calls each of the everything um that would um that would let you essentially keep track of more agents at the same time and you could maybe have like a bit of a flat structure where you just have multiple agents going yielding events and then you can essentially see like which one\\'s coming from where um but I guess the thing is like there you have to deal with a bit of the complexity of um handling all the like like essentially listening to like all the different events and like uh associating them back to like a specific agent and like maybe there is an agent in the front maybe there isn\\'t um here the idea is just I have one agent that I\\'m interacting with i\\'m always just going to interact with like that one agent and it\\'s responsible for spinning off other other ones and like dealing with dealing with that as well but you can absolutely do it the way that you\\'re saying and like if you wanted to build this out like more fully you probably would in order to be able to surface progress um did that answer your question i\\'m now scared to get people\\'s talking in the room but I\\'ll just read them from Slack um is there any is there any good design patterns to create projects with agents there\\'s like a million um you know like I like prototyping with swarm um I know paidantic AI is also like a really nice one i think Sam\\'s there in the room or around um there\\'s Yeah I don\\'t want to like I don\\'t really use uh any myself mostly because like you saw it\\'s pretty simple to implement your own loop so that\\'s what I end up doing i think usually for every project I either write my own like it\\'s like I don\\'t know how many how many lines is this like like 70 lines and so like depending what you want you might want more you might want less and I\\'m sure there\\'s good solutions um but like I just have I could just copy this around um I don\\'t really like working with too many dependencies especially for something that\\'s so lightweight like I I want granular control um and like except for example for Swarm I ended up having to hop in here and like handling specific kinds of tool calls in certain ways so that I can do handoffs um which we can actually look at and we can implement this without too much trouble um but yeah my answer is like there\\'s many you can choose from i don\\'t have many I would recommend personally um but I I am a I am a fan of of Pyantic AI it\\'s pretty cool i haven\\'t used it too much but like the interface looks nice um it reminds me a lot of Swarm the one I use the most for prototyping is Swarm i think it\\'s just cuz it\\'s the one that like I\\'m most familiar with um when you have to start to approach dozens or hundreds of functions what techniques should we apply in order to effectively tool call there\\'s there\\'s a few answers there right um you can have multiple agents and essentially like split up the responsibilities or like the the groupings of the functions um and so into into yeah like clusters where you have like a set of related functions that are needed for specific tasks and then you can invoke the correct agent uh and this is where like multi- aent patterns start to make sense is like specifically when you have tons and tons of functions like how do you uh go to the right ones if you for some reason need them all at the same time uh you could try fine-tuning um in in projects for OpenAI I\\'ve ended up fine-tuning up to like hundreds of fun like it was like 120 functions this was with uh GPT3.5 so the fact that that works gives me like pretty high confidence like you can fine-tune uh smaller models with a lot of functions and get them to work pretty well um the last one is like some kind of dynamic function loading where based on the input or based on the conversation you load into memory or like you load into context um the most likely relevant functions um and there\\'s a few different ways to do this you can do this with embeddings you can do this with like um having like a two-step function call at that point you\\'re essentially having agents like if if you call a function to then load more functions that\\'s what a handoff is essentially so a lot of these start to look very similar um it\\'s just like how are you loading multiple different ones um okay for visioning models are there tools being called within the thought text um so because we don\\'t expose the thoughts like the the chain of thought um that\\'s a little bit hard to answer for for for 01 right now in the API the answer is no um I\\'m trying to see how much I can say here um it is something that is technically possible right like you can do anything you want with with post training um we do not currently allow you to call functions within the chain of thought um so yeah right now the function calls happen at the very end uh do you have any good code examples for router patterns in these lots of functions cases yeah so so my like my answer there is some one of these like if you really just want to route um then like the the idea of having like multiple agents and handing off to one of them is actually really nice um like you can just define multiple agents each of them with like multiple functions um and then have the first one have like a we can actually do this quickly like we I\\'ll do this in swarm but like I said you can implement this yourself um I don\\'t actually know who else supports handoffs the same way Swarm does but um essentially here let\\'s start a new file like uh routing so see swarm ripple demo loop I can say like you know triage equals one and then I can have my other two it\\'s like you know maybe I have like some collection of like um you can call them an agent but I can also just call them yeah like uh what it be like you know uh email functions agent and you can have like you know like send email check email I don\\'t know what else does cursor want me to write there um cool we have these two and then we can do like maybe we have the emails and then we have a calendar i don\\'t know so make like create event can do like calendar i\\'m just going to say like you know finish what I\\'m doing update the prompts and the tools i\\'m lazy so let\\'s see what uh let\\'s see what cursor thinks great so now we have an email functions agent and a calendar agent call them whatever you want um now like let\\'s pretend that instead of just having three functions we have 30 and each one of these has 10 or 15 um then like if you maybe give all 30 to one agent first of all try it like if that works amazing right you don\\'t have to deal with the extra complexity um so you don\\'t really want to do handoffs and stuff until you really really need to through like evals um but um yeah so so and then the special like kind of functions here is like you know uh transfer to email agent and I\\'ll return email agent and then cool so now I have my two transfer functions oh oh it did it okay cool so I think this should just work right so I\\'ve defined like the actual functions and agents that in your case would be many many more functions um I\\'ve defined the transfer functions and I\\'ve given them to the triage agent so now if I run this say hi um you know I want to send an email cool so now I\\'m talking to the email agent now if you want to do like more you know like transfers uh this is not delegation okay so trails assistance um maybe we can tell the assistance like you know if you already know what the user is asking just call that function right and this is if you want to have like a case where you know it still routes you but it\\'s like a faster two hop so maybe I can say like what are the functions here um what are the parameter There\\'s send email to subject body so I can say like know send an email to bobgmail.com about taxes what was the other one body um saying yo do your taxes so it should transfer me to the email one and then there we go it immediately sends the email so this felt like an immediate function call but there was a transfer in the way so this is kind of an example of like triaging does work it\\'s really convenient to model it with agents and handoffs um I\\'d say it\\'s the primary use case for agents and handoffs is just a glorified triage through uh multiple functions um yeah let\\'s see let\\'s go back to questions [Music] uhhuh consider having something like streamllet grad until you are you considering having something like streamlink radio util that allows porting all interaction functionality with one command i don\\'t think so i don\\'t know uh what did people answer there we go sam\\'s in with pantic so check that out um how many tool calls can you get in one iteration you like parallel function calls i don\\'t think we have hard limits on either the number of functions or the number of parallel function calls how big a tool library will the models perform well with super super general rule of thumb is like 10 to 20 you shouldn\\'t really pass over um but like I said I\\'ve gotten like this was a very specific case where like it was extremely latency sensitive and so like we had to have flat like flat function calling and we did 120 functions with GPT 3.5 so you can go pretty far um with fine-tuning but like I\\'d say reliably without without extensive prompting yeah probably like 10 to 20 like past that point you really ask yourself like what are you trying to do like why are you putting so many functions is it super latency sensitive can you split it up like yeah um was there a follow-up here okay um rather than tool calls I feel like we\\'re moving to generated code agents will I soon be able to supply my tools functions ah okay that\\'s a good idea we should try that um so essentially have something write its own function and then use it uh yeah I feel like we can probably How would we do that yeah we can try to we can we can find a way to do that uh I I\\'ll do that actually let\\'s do right now why not let\\'s do right now uh I\\'ve never done this before but I feel like it shouldn\\'t be too hard so um okay uh you know what do we call this like bootstraps okay so I\\'ll keep using swarm because I I will use um handoffs from swarm import agent cool so now it\\'s always so let\\'s see we want an agent that writes it own functions so agent was this one um we want it to write its own functions so maybe we can have an hand off to itself so we can do you know like refresh you know what to be refresh functions so we can actually return the same agents um and then okay is it not declared is it unhappy uh yeah it\\'s not defined yet okay okay so now we can define it down here okay um bootstrap bootstraps bootstraps okay so we have this and we want it to write its own functions so how are we going to do this we wanted to produce Python does anyone have any ideas uh if you want to shut them out like make just pass around the mic while I code just feel free to I can\\'t I can\\'t read while you\\'re uh what you write but we can we can try this so um let\\'s say we want it to you know um you know add tool there we go and then let\\'s call this you know Pythonation okay i\\'m going to do something very unsafe so you know what would it be it\\'s like function obj equals eval of Python don\\'t do this don\\'t do this kids um and then will this work is this how Python works sam\\'s there uh okay so if I have this and I eval like you know def a does it um okay you know Um so I guess it\\'s like uh I want to write a write a function that takes a string representing an implementation of a Python function and returns the actual rule Python function as interpreted i don\\'t know let\\'s see what it does um so essentially we want to do that once we have the function then we can just append it to the agent and then we might need to reload it uh so we can then just return the agent and this might be it this might be it so add tool exec not eval why didn\\'t anyone say that you guys can shout out okay okay so it\\'s exec and then what is this 80 i don\\'t know someone save me like uh Hey there\\'s a couple implementations you miss sorry check the flash did someone do this hey okay it\\'s almost like someone fired no no no but this is not Okay no but this is not what I want right like I don\\'t want it to run it in a subprocess i wanted to I wanted to eval like evaluate the function and then turn it into a is that Oh go up oh Sam of course amazing uh Jesus Christ okay um make it so the add tool function uh evaluates the implementation and adds it to the tools similar to now here\\'s a great reference for how to Let\\'s And then we want to grab Was this a good idea this is a terrible idea i hope people are enjoying this um okay let\\'s see um and let me just read this because it might not be that complicated to add in so I can\\'t okay it got in the way no not Zoom where\\'s Slack was this it is this it okay I\\'m going to I\\'m going to put this on hold for now it sort of ignored your implementation Sam i\\'m sorry um let\\'s see so if you have parse function can I just do this does this work uh okay let\\'s see i\\'ve first time for everything uh wrong one sub bootstraps hi um add a tool that prints hello when called it oh but no no no no make it print it not just return it my heart\\'s pounding okay um so say hello to call it look at that look at that okay so now we have Yes I will take a clap for that we did this together guys um so this is actually a lot less code than I was expecting but now we have a system look at that it\\'s tiny um that can write its own tools um and so like maybe we can do something like you know um what is you know like make yourself a little calculator i can\\'t believe this just worked uh you know what is 2 * 3 * Can someone check this uh wait wait wait 34698 34698 this is This is crazy this is This is so fun uh yeah i hadn\\'t done this before and this is a lot less code than I thought look at this look this is all you need i mean it\\'s this is super dangerous code like this this is not good don\\'t do this but it\\'s fun uh I would put this squarely in fun things we can we can now transition to looking at other fun things um they\\'re definitely not as fun as this one I think um they\\'re just a couple of like random things that I found related to real time since I did sort of put it in this uh title so whatever okay these are the two main tricks that I that I kind of thought were pretty cool and you might have already seen them um one is if you\\'ve ever dealt with like the real time API um like jumping in uh before you\\'re done with an idea or like done talking or something um like what I was thinking is like if if really what you want is you wanted to like use the model\\'s own intelligence to decide if you\\'re done talking or not um and you can treat our VAD like our voice detection as like a trigger happy version of that it\\'s like a it\\'ll always tell you when maybe the user is ready to stop talking but if you want the model to check you can have a stay silent function or something else that essentially handles the other side of that where it\\'s like okay we\\'re definitely going to let you know whenever the user might be done talking but then you can actually verify with a function call and so this implementation is like super super simple you literally just give you give the the function itself to the real time API and tell it to call it when the user is not quite done talking and I\\'m not going to try to demo this live like real time demos are tough audio is tough all that fun stuff\\'s tough but um it works like surprisingly well uh if you just describe I I can you can probably find this tweet but it has the full prompt but essentially it\\'s like you can you can like pause you can say like you know I\\'ve been thinking about and then like stop and like it\\'ll get triggered call stay silent and then you can keep talking um so pretty pretty cool uh useful thing um and then the other one is also related to real time API and if again if you if you uh are following me you might have already seen this but um someone at Devday just like came up to me and asked me like hey um is there a way to like make the real time API talk in like a specific way uh and I was like I don\\'t know let\\'s try it and so like we had a demo booth and I sort of pushed someone out of the demo booth and grabbed the laptop and I was like let\\'s just like try try this stuff and apparently if you ask the model to just you give it like a a script and you\\'re just like \"Hey read this out according to these XML tags um you can have it follow them here let me let me find um the original the original one.\" Yada yada yada sorry this is I\\'m not trying to show you the whole timeline where is it okay this is the stay silent one um where is it where is it there we go um uh it\\'s impossible that you\\'ll hear this right you guys don\\'t hear this yes no uh we don\\'t hear it but you can hear your computer audio can I do the thing i\\'m I\\'m I\\'m actually I\\'m not going to try that\\'s fine um wait can I same no no no it\\'s fine whatever anyway um yeah so if if you if you like give it like a script like this um we didn\\'t actually train it for this right like this is just a really nice consequence of like behavior it\\'s technically not function calling i\\'m realizing now but it\\'s like it\\'s very function callingesque it\\'s real time the title of this talk does include real time so this is uh this is it um I will pause here i have so much more I can go through but I want to give you guys a chance to like ask questions poke around with some of these ideas uh Swarm is public you can just try it out um we can also just try creating other functions uh this this is one of the this is easily one of the coolest like little programs I\\'ve ever made um so yeah let let me see if there\\'s any other questions of the Slack uh it\\'s a room full of people auto completing make yeah yeah yeah okay let\\'s not do that uh okay revisiting memory let\\'s see do you have any suggestions for trying to enforce consistency of stored memories identifying inconsistencies and figure out how to resolve them okay and then the second part is what data structures do you suggest for more structured memory helping enable more meaningful comparison of objects i think I mean this question opens like you start to go down a path where you can get as complex as you would like right like like you can start simple and you can end up with like an entire operating system to manage memory right there\\'s like the whole range um one way to do off the top of my head like one way I can think to do this is um when you are about to store memory do a retrieval like do search to find similar memories or like memories that like are semantically similar um and then do an explicit check uh with a model to see if like you know one is like updating or contradicting another right an example of this is like you know what is the latest state of a project right you know if someone\\'s like is the project like at some point you say like I\\'m working on the project like it\\'s not ready yet and it saves that and then later you\\'re like you know I um how like the project\\'s done now right like those are two contradicting memories um what you can do is essentially uh have a time stamp for them but also when you are about to store the second one or any memory check for similar ones and create like a direct like essentially um like node uh pointing from the original memory to the new one and so I guess I guess what I\\'m thinking is like that way if anybody asks and you surface both like you can keep both in memory and when you do like retrieval and semantic similarity you can surface both but essentially you can present like the whole chain of updates and so you can just present the last one if you want or you can present the whole chain and the model have has some idea of like you know maybe if you ask it like um you know how long is this project delayed because that was the last you heard about it but somebody at some point said \"Oh it\\'s actually done.\" Um like if it raises both and has one with a later date you know maybe but if you like if you\\'ve already made this chain explicitly um like you can actually represent that and you can choose to not show the previous ones so that\\'s one idea like I said there\\'s so many you can like so many ways to do this uh and and like as soon as I stop talking you guys walk out someone\\'s gonna like be like \"Oh there\\'s actually this like much easier way that like this idiot didn\\'t think of.\" So anything you think of probably works um here\\'s a real time API prompt yeah cool uh any other any other questions about anything um it\\'s been like an hour and 40 minutes so happy to like keep going with random content um there\\'s a couple demos that I have that are like demoing some public repos that we have that you can use um yeah but I want to I want to give anyone a chance to like speak up say something if you want ask questions um I I don\\'t see I don\\'t see a lot happening in I I can\\'t You guys are tiny on my screen uh cool okay then I\\'ll pose this to you sorry what oh no go ahead it sounds like you\\'re wrapping up yeah the the last thing we can do I guess is like either I can um try and pull up a real time uh demo that shows how to do the 01 stuff uh in a in a slightly easier way okay cool people want to do that demo okay let\\'s do it so um the reason I\\'m a little iffy about it is because yeah it\\'s real time uh and those demos are tough to do publicly but um there is this repo OpenAI um real time Twilio demo um that essentially sets up your whole um like phone calling assistant uh and it\\'s just works right out of the box it walks you through the steps um I may end up sharing a phone publicly don\\'t call it during this demo and don\\'t share it and I will delete it but um yeah I trust you guys so let\\'s see okay so it looks like this is already running oh I do have it already running okay um Oh never mind i I did uh I did make it all dots but yeah essentially this um this little checklist is like live updates so as you set up the account as you set up the phone number as you set up your local web server etc it\\'ll like get checked so setting up Twilio and Grock and everything has been one of the most annoying things I\\'ve had to do multiple times um related to the real time API so this hopefully makes that process like a lot a lot easier you can do most of it directly from here and it lets you know like when things are done so you don\\'t actually have to do a lot from the Twe console at all um anyway so um I let me show you the back end so there is a handler function there we go function handlers so um you can either implement tools uh locally as a JSON schema and we can we can try that really quick so this is the real time playground and I can say like you know make a function to you know get the answer to answers to everything no params maybe one param okay so it gave it gave us this nice little uh nice little function and we can just paste it in here save it and cool so now we have this get universe answers and I can save this configuration and call the phone number so let\\'s see hello hello okay this is why I love them let me let me try once more okay let\\'s go tool paste this save changes save config [Music] um hello hello i don\\'t think it\\'s happy is someone else no I didn\\'t show the number hello okay it\\'s not working that\\'s fine sort of expected it um essentially what you can do like when you when you use this um it\\'s cool because these tools you can either specify local ones with schema and they appear here or you can define them in the back end um so if you define them in the back end you can actually give like code to execute um if not they\\'ll just show up here uh and you can enter in like what the response will be uh like a mock response um but you can also like set up backend functions that will actually get handled in the back end uh the sample one is like a real implemented get weather one but as you can see there\\'s like it\\'s pretty straightforward to implement like the 01 one now the main difference between the real time API here and like what we were doing before is the real time API does actually allow asynchronous functions natively so you can the model can call a function um get no response and you can keep talking with the model um until a response is back so that that is behavior that like we specifically had to teach the real-time models because unlike a chat conversation you can\\'t really enforce um like you can\\'t really halt the whole conversation until the functioning response comes back so we had to do that for for for launch um because originally when we hadn\\'t done that you know we hadn\\'t ever shown it how to do asynchronous functions and so it just couldn\\'t couldn\\'t handle them correctly anyway sadly uh demo didn\\'t quite work maybe I can try this one more time we\\'ll see third time\\'s a charm hello hello no okay great anyway um I believe that is most of what I wanted to cover today um I\\'m going to play with this hyper unsafe function making agent a bit but um yeah thank you all for coming i\\'ll hang around for questions for a bit if anyone has any otherwise I think we\\'re ending a little bit early um like I said yeah mo most of you can can hang out or or leave as you please um but I\\'ll maybe I\\'ll call it wrapped up here um maybe what we can do is if you\\'re interested you can stay and like hack on some of the stuff that you saw i\\'m happy to hang around uh and answer questions as well so thank you all for coming uh hope you hope you got something out of this um the new uh to help in sports uh otherwise I guess you can share i\\'m sharing a little bit in case people have questions i\\'m actually going to dump this but this may be a really bad idea but um here you go here\\'s the code for the super unsafe self like tool writing agent and I\\'m probably going to stop sharing my screen um yeah I can I can I can share the repo and the slides there\\'s not a lot on the slides but yeah I\\'m happy to happy to share them after',\n",
       " 'word_count': 14176}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_youtube_transcript(youtube_url: str):\n",
    "    \"\"\"\n",
    "    Tool function to fetch the transcript of a YouTube video.\n",
    "    Input: YouTube video URL string.\n",
    "    Process: Extracts video ID, calls YouTubeTranscriptApi, formats output and counts words.\n",
    "    Output: Dictionary containing:\n",
    "            - status ('success' or 'error')\n",
    "            - transcript text and word count on success\n",
    "            - error message on failure\n",
    "    \"\"\"\n",
    "    import re # Moved import inside\n",
    "    from youtube_transcript_api import YouTubeTranscriptApi # Moved import inside\n",
    "    from typing import Dict, Any # Keep for return type annotation\n",
    "\n",
    "    logger.info(f\"Tool 'get_youtube_transcript' called for URL: {youtube_url}\")\n",
    "\n",
    "    try:\n",
    "        # --- Start of merged fetching logic ---\n",
    "        video_id_match = re.search(r'(?:v=|/|embed/|shorts/)([0-9A-Za-z_-]{11})', youtube_url)\n",
    "        if not video_id_match:\n",
    "            logger.warning(f\"Could not extract video ID from URL: {youtube_url}\")\n",
    "            response: Dict[str, Any] = {\"status\": \"error\", \"message\": \"Could not extract video ID from URL.\"}\n",
    "            return response\n",
    "\n",
    "        video_id = video_id_match.group(1)\n",
    "\n",
    "        # Try fetching the transcript\n",
    "        try:\n",
    "            transcript_list = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "            transcript_text = ' '.join([entry['text'] for entry in transcript_list])\n",
    "            word_count = len(transcript_text.split())\n",
    "            logger.info(f\"Successfully fetched transcript for {youtube_url}\")\n",
    "            response: Dict[str, Any] = {\n",
    "                \"status\": \"success\", \n",
    "                \"transcript\": transcript_text,\n",
    "                \"word_count\": word_count\n",
    "            }\n",
    "            return response\n",
    "        except Exception as fetch_error:\n",
    "            # Log specific exceptions from the API call\n",
    "            logger.warning(f\"Could not fetch transcript for {youtube_url} (ID: {video_id}): {fetch_error}\")\n",
    "            response: Dict[str, Any] = {\"status\": \"error\", \"message\": f\"Could not retrieve transcript: {fetch_error}\"}\n",
    "            return response\n",
    "        # --- End of merged fetching logic ---\n",
    "\n",
    "    except Exception as e:\n",
    "        # Catch unexpected errors during the tool's execution (e.g., regex error?)\n",
    "        logger.error(f\"Unexpected error in get_youtube_transcript tool for {youtube_url}: {e}\", exc_info=True)\n",
    "        response: Dict[str, Any] = {\"status\": \"error\", \"message\": f\"Failed to execute transcript tool due to an unexpected error: {str(e)}\"}\n",
    "        return response \n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "result = get_youtube_transcript(\"https://www.youtube.com/watch?v=KUEmEb71vzQ&t=5522s\")\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'transcript': 'Cool. Okay, let\\'s get on with it. So, hi everyone. My name is Elan. I\\'m on the developer experience team at OpenAI. Um, unfortunately, I can\\'t be there in person as much as I would love to. I\\'m in a wedding in Costa Rica um which is happening later today. So, I just wanted to take this opportunity to just talk through um one of my favorite concepts in maybe all of like AI and language models. So, uh title of this talk is function calling is all you need. It\\'s a talk workshop. There\\'s going to be um a lot of coding. Please save your questions. No, I\\'m kidding. Like just interrupt at any point. Uh we have a Slack. Send them here. Um yeah, just send them at any point. Uh and if you want to like unmute yourself and or raise your hand, I\\'ll call you off like as we go. The idea is to keep this super super dynamic. Um since we have a bit of time, um I\\'ll be fielding a lot of like questions and requests and trying to be coding as much as possible. So yeah, this talk is going to need some lecturing uh a lot of coding from scratch and then some debugging hopefully not a lot. Uh so this is a little bit of what the uh workshop is going to look like. We\\'re going to go over a little brief history of the tool formers of of I\\'m sorry of function calling. Um then do a little crash course on function calling. Um talk about just agents how they\\'re just loops. How rag workflows and more are just function calls delegation and asynchrony. couple random things I found and then we\\'re going to do a Q&A. Um, if you just want to see like the meat of it, this is pretty much it. We\\'re going to do like this is everything I want to talk about uh and everything that we\\'re going to implement. So, this is uh just a little bit upfront. Great. So, a little history. Um, if we look at the abstractions and sort of patterns that we managed to do with language models, it started as text completion, right? like the original GPT uh and the GPT2 and the GPT3 were all just base models where you gave them some input text. Um and then they would just continue the sentence. Um this was at the time really really like interesting. This was the first time we were able to do like very like uh English sounding like real sounding language. Um but getting it to follow instructions was pretty hard. So if any of you were testing this back then you might remember how setting up a chatbot was um non-trivial, right? you had to like get it to answer questions. But if you just say like, you know, what is the best way to uh get to the like park or something, it would continue like like what is the best way to get to the park? Um that is what Sally said yesterday, right? And you wouldn\\'t actually get a a response. Um so you had to like structure it in a way where you would say like this is the question, this is the answer question answer few shot and then give it. Um then they introduced uh I think this was actually us. We introduced uh function um instruction following with uh instruct GPT. Now you could give it some input uh and it would actually do what you\\'re asking as opposed to just completing. Um finally we started to introduce this notion of like users and assistants and roles. Um and this was all done through through post training where you actually gain these personas. Um and then finally uh we we eventually landed on this like you can give it additional tools um in order to do like external um any like interact with external states. So this is what the previous playground used to look like. But as you can see there\\'s no chat. This is just like a window and it\\'ll complete. Now looking over at like the original papers which is pretty interesting. Um, one of the first times that we actually started to do this like function calling was through this web GPT um, which was this uh, version of GPT3 that we trained to be able to um, Elan, I have to cut in here a little bit. We got the Zoom working. Yay. Sweet. Um, all right. We\\'re going to cut over in the audio. So, can everyone hear Elon when he speaks? Elon, say something. Hello. Hi, everyone. Hello. Hello. How\\'s it going? Okay. So, so we\\'re going to cut over a little bit in terms of like um people having their own personal audio situation going on. So, you can mute your machine. You should be able to hear it on the room uh audio, whatever you want to do, but you can also obviously connect. But, uh yeah, now we have him on the big screen. So, okay. Yeah. Uh let\\'s let\\'s keep going for a bit. I I\\'ll cut in again if there\\'s audio issues or maybe you want to say some test words. Uh, you\\'re all great. Thank you for coming. Can we bump up the audio? Where\\'s the guy? [Laughter] It\\'s not a conference if you don\\'t have AV issues. Okay. Uh, it\\'s it\\'s still too soft. We can\\'t we can\\'t hear you. Um, I\\'m going to try and bump up the audio. I\\'m so sorry. I\\'ll put up a more interesting slide in the meantime. You guys can look at this while we figure this out. Where\\'s Where\\'s the video? Hey. Um, so anybody got uh good jokes? Maybe I always find a joke. It\\'s always the same one. I think it\\'s uh What is it? Why did the No, I had it. I had it here. Wait, wait, wait, wait, wait, wait. There we go. Why can\\'t you trust an atom? Yeah, because they make up everything. These are all real, by the way. And for for those who like have a keen eye, the first one is like actually from the GPT2 paper. Um we like gave the model this like description of like unicorns in the Andes Mountains. And that was like the big like first time that it was like doing multi- paragraph completion like continuations that referenced earlier parts of the conversation. It\\'s cool stuff. It\\'s cool history. Can you guys hear me? I feel like some people are having a great time. Some people don\\'t know what\\'s going on. Can like I don\\'t know. Am I good to keep going or should I keep waiting? Uh I think I think you\\'re good. Okay, great. Great. I do think this is the vibe of the whole talk by the way. There\\'s it doesn\\'t get more structured from here. Um okay, so as I was saying, uh we did like this web GPT paper. Um, essentially we trained a GPT3 version of the model uh or like a GP3 model to be able to use this like this very specific set of functions um to do web search. And this was like back in 2021. So really we had like webg a long time ago. Um but this is like one of the first times or maybe the first time that like we were having like it\\'s not just generating text but it\\'s generating actions and then we\\'re parsing those actions and then introducing it back into context so it can use the responses itself. Um, and how we trained it there, um, there\\'s like these these like, you know, clever ways of like we we essentially gave people, uh, an interface and let them do the searching. Um, and then I think we took it like Reddit um, yeah, explain like five. Um, and then just like had people complete tasks and they could use these commands. And so we taught the model and this was GPT3, right? Um, how to essentially imitate users behavior and then produce responses that were preferred. Um and and this was pretty cool like this is how you we like start to saw um to see like this uh this use of like structured like actions essentially. So but this was very specific right we were training like very specific um tools. So then you might be familiar with this paper. This was from Meta at the time. Um where they essentially had a way to teach the models how to use any tools. Um and they taught like they they used a few tools. I think it was like QA calculator. Um what was this like translation? Um couple other couple other tools. But um it was actually a pretty clever way where they like looked at the log props at each spot to like see where it was like best suited to like retroactively put in a function call given some like completion. Um so here we can see a few examples like um essentially it\\'s like if you have a calculator call if you insert a calculator call that like you know it\\'ll it\\'ll insert the um the actual call to the calculator which then you know you\\'re pretty familiar at this point will get the answer if you insert it in the right spot in the sentence it actually reduces the perplexity of the sentence. Um, and so they didn\\'t actually have a lot of human-labeled examples or I think it was just a few, but it was uh really cool because it was this way of like um it could learn to use uh any of these tools through this like crazy like log props technique. Um and it was uh I was pretty excited when I saw this paper. Now this is like how it learns to use any of these tools. Um, but then finally, uh, in June of 2023, uh, OpenAI launched just general function calling where we essentially like pre-trained it to be able to use these tools or actually post-trained it to be able to use, uh, tools. So now you don\\'t actually have to like give it like you can give it examples, but uh, we just showed it like this like syntax in with functions that we still use today and it\\'s just able to call functions. So this is a brief history of of function calling. Um, and my I guess my argument is this is really most of what you need for all the exciting stuff that\\'s happening today. Um, there\\'s there\\'s obviously like additional like systems you can use and like um more post training you can do, but fundamentally like functions are so so so powerful and we\\'re going to look at a few cases today. Um, I\\'m going to try to keep an eye on questions, but um yeah. Okay, I think you\\'re good so far. Cool. Cool. So, let\\'s do a super quick crash course on function calling. Uh, two main purposes and I\\'m ripping a lot of this from the docs. So, um, there the fetching data, right? Reading APIs, retrieval, memory, or taking action, any APIs you can use to write, managing application state, which is actually pretty overloaded. That can be like UI, front end, backend, whatever you want. And then workflow actions, which is um any like multi-step processes or even like meta actions like switching its own prompt or like loading in different tools or like handing off a conversation, right? Um so this is also a diagram straight from the docs, but um I\\'ll quickly brush past this. I\\'m going to assume like most people have at least seen this, but here it is. It\\'s uh you you essentially tell the model which functions you you want it to be able to use. Um and you also provide like whatever the user input is. Um the f the model tells you what it wants to do with that function but it doesn\\'t actually do it. This is one of the like a big sticking point function calling. It doesn\\'t actually use the function itself. It tells you like it tells you the intent of what it wants to do with a function. You are then responsible for parsing that, executing the code, doing whatever you want with it and then providing the result back to the model and then the model can use that respon like result in uh in the generation. Uh take a look at a quick question here. Oh no, it\\'s just it\\'s just swings. Okay, cool. Um these are just a few uh best practices. This is all taken from the docs as well. Um you want to write clear functions. An important one is you got to apply software engineering best practices when you write these functions. So, um, you know what? Maybe I\\'ll pull up the docs for this. Um, so there\\'s a lot of big text, but essentially, um, this is a lot of value here. I tried to pack as much like useful information here as I could, so I\\'m going to quickly go over it, right? Um, you had to explain the purpose of each parameter, use a system prompt, and include examples. That\\'s, you know, pretty pretty like not non-controversial. Um, software engineering best practices is a little bit uh more interesting, right? You got to make functions obviously intuitive and they got to follow the function of lease principle. Um, like if you give this to a person and they don\\'t know how to use it, then the model might not either, right? Models are getting smarter than us, but still you got to make it uh got to make it easy. Um, also you got to use enums and object structure to make sure that like you are not letting the model make invalid calls, right? Um like here you you have this like toggle light uh um function that takes in like two two boolean params and like obviously like this is pretty wrong but um there\\'s actually many many more like subtle cases where um like the you can like you\\'re letting it represent invalid states. Um okay, still no questions. It\\'s Swixs. Okay. Um, great. And then there\\'s Sorry, there are a couple questions in the professor convers. I see. I see. Um, here maybe let me see if I can pull up the Slack. Okie dokie. Between functions and tools in my opinion. Hey Sam. Yes. Um I I think we\\'ve all been kind of like gravitating toward functions and tools as like the two main ways. Originally it was just functions. Tools was like later we renamed it. Um I think now uh and the way that I tried to specify in the docs is functions are like the the raw function calling right like you provide an interface and you\\'re responsible for executing the code. um tools and this is sort of how we treat it in our API is a superset of functions. Tools include functions but it also includes things like um code interpreter or file search or any of these like um like hosted solutions. I\\'d say this is not be like end all beall definition but this is a definition that we\\'ve adopted right there\\'s like tools which are like hosted tools uh including functions and then functions as a subset. Cool. Maybe we can just jump into it. Um, if people have questions on this, happy to happy to feel them as well. But I kind of want to get coding. Okay. Uh, when you start to approach dozens or hundreds of functions, what technically should we apply in order to effectively tool call permissions is one technique. Okay, interesting. And then question two, when you require one tool to provide inputs to another, I have seen tools become layered. How should a reasoner hardcode? Great. Okay, these are actually great questions. And um I think to to answer them, I might I might like do a little hack and like uh use some existing uh code. I\\'m going to use swarm for a little bit of this um because it does some nice function calling. Actually, no, you know what? I\\'m going to get to these fun uh in a second. Um let\\'s just go straight into the uh and as always, we should start from the docs. Uh, cool. Uh, so this is where I always step in. Can you zoom in on screen every single page? Yeah. Yeah. Bigger. Yeah. Yeah. Sorry. I always do. Yeah. Yeah. No, this is a good call out. Uh and then I think the terminal here should be good. Okay, sweet. Uh we have we have a function. Great. Um and then just run it and we have uh the function call. Right now we\\'re not handling it yet. So, this is where like I might skip ahead a little bit um and start doing some of the like agentic stuff. But first, uh first off, uh we we got to have a right like you know you you I can sort of hear myself. Um I think so. There we go. Perfect. Okay. So um I think the idea here is like let\\'s make a very very very simple like uh input loop like let\\'s do uh while you also light mode light mode Jesus okay sure I\\'m just very experienced at this we go well that\\'s the least painful this is good right this works Yeah. Oh, my poor eyes. Um, actually, maybe I I I can\\'t quite see the room, but show of hands, like, who has used or implemented function calling in the past, and someone\\'s going to have to gauge this for me. Okay, it\\'s everyone. Great. I\\'m going to skip ahead a little. There\\'s like 10 20% 20% that hasn\\'t done it. Okay, 103%. Um the important part that you have to know is you can define the function um schema, right? Then the tool will specify what you want. And then in this case, you know, if I have like a get weather tool. Actually, I\\'m going to grab this from the docs as well. When in doubt, just go to the docs. You know, it\\'s always good. So, step one. No, this is all node. Yeah. And this will be useful for later too. Um, step one, call the function. Step two, execute your code. Right? So here what we\\'re doing is we\\'re taking we\\'re parsing out what the what the function told us. We\\'re parsing out the args and then we\\'re calling this get weather function which we don\\'t have yet. Um, but conveniently it\\'s up here, right? So we have it up here. get weather requests and then the last step is we provide the result back to the model. Um uh provide the result back to the model and then ask for a completion. Right? So just in order specify the tools, call it the first time, get the tool calls, parse them out, call the function, append and do that. Right? So if we do that and we just add maybe like add I\\'m going to do this by hand. Why not? Yeah. So I\\'ll print the completion here sort of by hand and then print the last completion. Then what we can see is we\\'ll get the first one that includes a tool call. It\\'ll call the actual temperature uh call the weather API and then we\\'ll get a response and then it says the current temperature in Paris is something I can\\'t see it because of Zoom. Um cool. So this is like the very very basic setup for a function calling. Let\\'s take a step forward. So this is an agent uh very very very basic implementation of an agent um that I\\'m going to go through really really quickly cuz we\\'re going to start using it. This is very familiar to what you\\'ll see in Swarm or any of the other like like basic frameworks. Um but the idea here is in uh as you can see like in the original one essentially like when it had a tool call I wanted to provide it back. So what I do here is while like just keep looping and this is like the very famous like agents are a loop. This is this is that loop. Um, specify the tools, call the model, get the message, print it out, handle the tool calls, append it, and once we have no more tool calls, break. This is the whole loop. I called it run full turn in my head. One turn is just like you let the model do everything. Um, and then we have this like execute tool call, yada yada, whatever. Um, so now we can use this. So agents py did I export it. There we go. I love it. Okay, so now we\\'re just going to specify one. Um, you know what? Yeah, we um and we\\'ll we\\'ll do it we\\'ll do it with a with a simple loop. So we have this that and we have this. Okay. So now we can just do Asians run full turn. The one other thing that I\\'m adding here that I didn\\'t show is um this simple utils uh that defines this very very very useful function. So functions to schema essentially takes in a raw python object function and then provides it into the uh like correct schema um so that you can just define functions directly and this is the same thing that we have in swarm and there\\'s an a few other frameworks now as well um so as an example we can do like you know get weather and I\\'m just going to like you know return 20 like degrees Celsius Right. I\\'ll do that here. Then I\\'ll have my messages and messages. Is this emergency reference? I think it is. What did I call it? Oh. Oh. Oh, I see. This is not part of the Asian class. Cool. So, it called the weather, printed it out, uh, gave us a nice little completion. Um, this is essentially what we want to see. If I say, um, and then I can just keep keep going, right? So, we we we have this basic um basic loop. Um, now let\\'s get back to the presentation real quick. That was like a very very immediate crash course. Now let\\'s let\\'s get interesting. Um and and just for convenience, I\\'m going to like pivot to the swarm implementation. Um the main reason being it\\'s pretty much the same. Um except we have like some convenient uh like looping tools. So uh swine imports agents and then There we go. So now we can do a simple agent and run a demo loop. And let\\'s see this work. Um we have this very basic setup. Let\\'s do everything now. So um another show of hands. How who here has implemented uh rag? Okay. What about memory? Okay. Fewer. And then what about like you know multi multi-step things and workflows? Cool. Um how about this? There\\'s a lot of stuff we could talk about, but I want to keep this valuable to you all. Out of this list, can you like just type in the Slack what you wanna what you want to see and I can just change the order in which we\\'ll cover this. Um because they\\'re pretty they\\'re pretty interchangeable. We can we can build up. Um but essentially there\\'s there\\'s more interesting things later on, but I want to make sure we we can build up to them. So, just uh if you can pull up the Slack and just dump in, you know, what you\\'re interested in seeing. Okay. Lots of memory. I see. Um function generation from the docs. Delegation async. Okay. build delegation and random cool stuff. I will get to the random cool stuff. Um, okie dokie. Back to light mode. Um, yeah, let\\'s start with like a very very basic form of memory, right? Um, how would we how do we do this? Let\\'s see. Honestly, like we can have um just a list, right? Memory can just be this list. And I\\'m going to implement this similar to how it\\'s done in chatbt. Um but ju just to show like I think the whole point of this talk is like this workshop is like doing things from first principles and just really removing the complexity. I think there\\'s like a lot of like not fake complexity but like added complexity on things that like doesn\\'t really need to be there a lot of the time. like concepts are a lot more simple and it\\'s all about function calling. So, uh we can do you know add like add to memory and I\\'ll append it and then get memory. Uh like this is super super simple. Um is this is this good enough? Let\\'s see. Maybe maybe. And so what we can do is give it the tools. Let\\'s see and then what when will we want to use them? Let\\'s say you like I can just say so this comment is going to be used as the uh string uh as the as the description in the function. So you can say like you know like when the user tells you something like factual about themselves their life or I can\\'t see anything okay or their preferences call this function um memory. Um and we\\'ll add a couple more cool things like expiration, which is one that I\\'ve kind of wanted to add for a while. So, you know, for now, let\\'s say false um memory.append uh think And then we have I guess what we call it uh you know memory text and we can say you know keep the memory text short size. Great. Cursor knows what I want. And then we can just return maybe like this is a super super super naive implementation. Um, but now, uh, when we start off, I guess we could even start off with [Music] a I mean, this is going to be kind of hacky, but I can just like um in your first turn always call get memory. This is not uh great. It\\'s just because of the demo loop. But maybe I\\'ll break I\\'ll break the demo loop out so we can actually do this by hand. But so how how could we prove this? Um maybe let\\'s say like you know write this memory bank or like keep this memory bank in a local file JSON file read it in at beginning and write it out at every Nice. Okay, I trust this. Shall we test it out? So, wait, someone uh Okay, we got the memory memory. Can any can anyone see any bugs? Because we\\'re about to test this out. So, we got the loop. It\\'s going to call it um and maybe like let\\'s memory. Okay. Sure. Yeah. Let\\'s try it. Let\\'s try it out. See what happens. So, hi. Did I not give it the functions? Is it hallucinating this? Oh yes, because I think we called it functions. Hi. There we go. Okay, so called get memory. There\\'s nothing there. I can just say like um I am 6 feet tall despite what people think. Uh, cool. Uh, so now let\\'s just check, right? It should have written this out. There we go. So now we have it in the memory bank. So now I can actually uh end this, right? And then be like, you know, uh, how tall am I? Tada. We\\'ve implemented memory, right? There\\'s a Yes, I\\'ll take a clap. I saw Louis Lewis, you\\'re like my proxy for the Luis uh Luis Costa, you\\'re my proxy for the audience. You\\'re like the only person I can really see. So, please don\\'t uh turn off your camera. Um, amazing. Now, we can do more interesting things, right? We can uh if we want do a little bit like smart querying uh where instead of just like loading in all of the memory, um we can like do a little bit of like retrieval uh to load in the right ones. um and use like semantic similarity or use some kind of search. Um I I could try to implement that. Um that might take a little bit longer but not that long. But I do want to pause here. See like given this and like this is going to be the style of things that we do like what um what do we want to see next? I can just keep going with this example. I can pivot. Uh it could be fun just to keep building on this. See how far we can get. Let\\'s see. uh delegation async have it chat and work in the background. We\\'ll get there. We\\'ll get there. I got that working this morning on Python because I didn\\'t want to switch to to Node yet. Uh delegation async. Okay. Let\\'s get into delegation then. So, there\\'s a few different ways we can do this. I\\'m actually going to leave the memory and we\\'re just going to keep building in this on this on this agent. Uh, and I am using the swarm agent just cuz I didn\\'t want to debug the one that I implemented. But like if we actually look at what this is, um, like it is very very very simple. And then the like run demo loop itself is like uh just printing out messages. Um, and what it\\'s doing is like appending client.run whatever. And like if we look at this client.run run. Um, if it doesn\\'t stream, it essentially does exactly what we did before, like keeps looping, get the completion, throw the messages in, uh, append them, handle responses, etc. There\\'s a couple more things around context and handoffs that we like don\\'t don\\'t really have to look at today. Um, but we can. So, cool, we have memory. Let\\'s do delegation. Um, so there\\'s a couple ways we can do this, right? Um, there\\'s if you think of like functions and agents and and everything you can, um, maybe I\\'ll skip to this slide. Skip skip skip skip. These are like a few of the forms of like agents on delegation that people might be familiar with. We have handoffs, which is like the swarm style. You take a conversation and fully swap it to a different agent. Um, and what that means is just like replacing the system prompt, replacing the tools. Um, you can have nested calls which are the easiest to implement and like often somewhat overlooked. Um, and then you can have manager tasks that\\'s more async. Uh, we will get to that today. So, let\\'s do a very basic one, right? Let\\'s say I want to do like um maybe let\\'s give it a chance to like call a bigger model to do a harder task, right? So we can say like you know uh I can delegate to smarter model and and I\\'m going to give like task description. [Laughter] Um so I\\'m laughing because this is saying like it\\'s just going to make it smarter by telling it to be smarter. That\\'s not despite how well that would work. Usually we\\'re not going to do that. So we\\'re we can just make an API request directly. Actually, let\\'s just do that. Let\\'s just do that. So um let\\'s do from OpenI client. And here we could do client chat completions create. Let\\'s call 01. Um I won\\'t provide a system message. I think that\\'s okay. content is the description. Look at that. And we did it. So now um man, I love cursor. Okay, did everyone catch all of that? By the way, all we did was like implement this this function that calls opening API and then here. So now I can say like uh so now I can add a bit of a description here. I\\'m like like uh if you know if the user has to like um I don\\'t know do something like that seems difficult or says it\\'s hard use this instead of try uh and it infers you know how to use this based on like the fact that I called it test description. It\\'s, you know, so let\\'s give this a shot. Hi. Um, let\\'s see. Uh, you know, give me a poem about, I don\\'t know, dogs and the earth where each other starts with the next letter. Yeah, it might just try it because I know GPD4 can do a variation of this, but um Oh, it\\'s giving it a shot. Okay. Let\\'s see if it gets it right. Okay. I don\\'t have uh I don\\'t have the patience. Let\\'s say answer briefly. one subkins max. Okay, great. And then to you know uh write a haik coup. There we go. It\\'s shorts where each other word starts with the next letter of the alphabet and ends with the previous letter of the alphabet. I don\\'t know. You guys want to try this while this does it? Is it around? Did I lose Did I lose something? Let\\'s see. Hi. Okay, you there? Cool. Um, did I not copy it? Oh no. Okay. Uh, give me a thank you. Now make sure each word starts with letter of the alphabet starting with a and each word ends with the previous letter. But this is hard. Okay. So it should be making the function call. Oh, I see. I see. I see. Okay. So I\\'m only printing the function call when it returns. So that\\'s probably why we\\'re waiting so long here. Um but this is actually a really good uh example of like okay we are doing um this like task delegation technically and like it is happening in the background and I\\'m going to give this a sec to figure this out. Um but like this is obviously a bad experience right like you don\\'t want to be waiting here. You essentially want to keep doing other stuff. So wow it\\'s still not still not back. We\\'ll we\\'ll we\\'ll we\\'ll let it figure it out. Um, so maybe let\\'s skip straight to async. Um, we\\'re actually got like both more and less time than I thought. So I don\\'t let this keep turning away. Um, now let\\'s think about this for a sec, right? If we want to do something async, it means that like what do we want to happen? There we go. So call the model da ya yada buzz breezy whispered. Is this correct? I don\\'t know. Wow. Yeah, this is sort of correct. What did it say? Note. Great. Great. So, it it did something, right? Um and it did something that 40 probably could not have, which is great. That\\'s delegation, but we were just sitting there waiting for it. Um so, let\\'s try doing this async. Now, I I I I actually want everyone to like kind of just stop and think like how would you implement this um in terms of like what behavior do you want? And maybe I want to see people like drop this in the Slack. Just like take a couple minutes um and like drop in the Slack like a proposal for how you would want to do this and then I might just pick one or we\\'ll like talk about them. Let\\'s call 03. Yeah. Not not yet. Not yet. Yes. Import async.io definitely feels like uh important. But I guess the questions are like you know when we delegate something uh obviously we want to have this happening in the background. Um, do we want like when it finishes, do we want to be like do we want it to be injected into the conversation? Do we want it to give us a response? Um, how many like do we want to be able to like interact with tasks that are running? Um, like do we want to be able to batch stuff? I am not setting up a Kafka cluster. I\\'m going to give people maybe a couple more couple more minutes to just like dump some ideas here. Um, batch calls. Yeah, I guess B. But how how would batch calls work? Maybe Stephania if you want to add some like detail there. Just keep working until it generates a stop word, right? Okay. Yeah, this is this is a good idea. JS and set timeout. Yeah, switching to JS is always a good option because you already have the event loop implemented. Smaller model. Smaller model. Okay, you guys are all suggesting some good ideas. Um, but we can actually go simpler, right? Like like essentially what we would want and I can implement like the basic interface of this, where\\'s my code? Um, is like instead of actually doing this, right, I can like return delegated, right? response pending. Um and then later I can say like you know check tasks or something. So this this pattern is actually one I I quite like a lot where you um you call a model it\\'s a function it\\'s non-blocking and then later you can you can check up on them. Now the thing is Python is like single threaded and uh async is real. Uh it\\'s just a little bit tricky. Um but let\\'s uh let\\'s give it a shot. So I do have some async emergency reference. Now here\\'s the here\\'s the um thing to notice. when something like for this to work correctly, we have this loop that asks for our input and blocks on our input um and like is displaying that on the screen and we don\\'t want to have that be displayed while it\\'s also like injecting messages. Um so we essentially want to separate out like where you give user input and where actually interesting stuff happens. So that\\'s what I\\'ve done in this in the basics of this async folder. So let\\'s take a look at that. Cool. Um, it looks complicated. It\\'s not that really that bad. So, um, I\\'m using websockets. Essentially, what I\\'m doing or sockets, sorry. Essentially, what I\\'m doing is, um, the I have a server and all it does is like has a handle user input function, uh, and a start message processor. Um, these aren\\'t super important. and we can take a look at them in a second, but essentially it just like waits uh on a specific like uh port and and host. Um and then I have a client that connects to that and it just does while true loops and lets me enter in. Right? So if we look at what that looks like, I have it here. Um so let\\'s go to async. Let me zoom in here a little bit. Also, um, while I do this, is there are there any questions that I\\'ve that I\\'ve missed? Um, and if there are, just feel free to shout them out. No. Great. Cool. Covered everything. Everyone is uh perfectly up to date with everything I\\'ve said. Okay. Um, sorry. Okay, so I have this and now I can say like, hi, let me show you the the rest of this. I guess that\\'s probably important. Uh, D. So we have the server, we have the client, we also have the a very very very basic agent imple implementation. It it looks just like the old one with the main exception that I\\'m using async openai instead. Um handling tool calls is happening in parallel. So for each tool call grab it create the task um await for them and then just like await for them all at the same time. Um, and this just lets it like create a bunch of tasks that are all going to run in parallel and once they\\'re done, like if they if they yield back at any point, we can keep doing other stuff. Um, by the way, if these are just like heavy functions that are doing heavy processing, it\\'s not going to matter. The fact that they\\'re in parallel like in sync, it means that it\\'s still going to happen one after the other. But if they\\'re like network calls to other models, then it\\'s perfect. Um, the runful turn is the same one. You like call a model, check to see if there\\'s any function calls, if not break, return the response. Um the only difference here is we\\'re awaiting uh the chat completion, right? This is the only difference. And then if we take a look at our agent handler, we\\'ve declared we\\'ve declared our agent like normal, right? Right now it has no instructions. It looks very familiar. And this loop is also pretty familiar. It\\'s like get the messages. Um here here\\'s the only difference, right? Um and you\\'ll see why this matters in a sec. I have a message Q. Uh and this is will be useful because we don\\'t want to process multiple messages in the same conversation at the same time. Like we want work to happen at the same time but we don\\'t want um like multiple generations to happen with the same history because then you\\'ll get conflicting like like if two messages if two functions return and they both need to be handled uh you essentially while you want them to happen in parallel the results should should only come in one after the other. So we have a cue for that. Uh and it treats user messages as well like that. So this is the handle user input that is being called from the server. Essentially just throws in a user input message into the message Q. Uh and all we\\'re doing here is like pulling off the queue, run it like you know uh put put the messages back in the in the message array uh and then like sleep, right? This is like just a very simple uh loop but it does it with async. So this is what we saw and and it feels pretty normal right. I can say like you know my name is Elan and I can say you know what is my name. Great. Um, now this still doesn\\'t exactly answer how we\\'re going to do things asynchronously, but it does give us the space to play with it because now things are happening async and we can do a few more delegations that that are actually async. So, let\\'s start with a, you know, simple blocking get weather function. I think I required all functions to be async here. Um, yeah. 67 and 70. Uh, and then skip that and let\\'s test it out. The only annoying part is we have to like restart two things now. So, hi. Uh, what\\'s the weather? Great. So we can see it called a function got the response. So far everything is normal. Now let\\'s do some delegation stuff. So let\\'s say let\\'s say we want to call this function uh three times for different places. So we have location [Music] um yeah and let\\'s say you know like pick a random number from 50 to 80 to return. Um, so if we do it in the previous case, do I still have the round? Yeah, maybe, maybe not. Okay. Uh so if we look at okay this is the non async one. Do we have weather? We don\\'t have weather. Let\\'s give it weather. Let\\'s give it the same weather function. Cool. [Music] Um we only need random. We don\\'t need this. And this not async. Okay. So now we have this weather function. Let\\'s test this out in the non- async case. Um I\\'m going to get rid of the rest for now. Um so if I say you know uh weather in SF cools it fantastic and now it\\'s like weather in SF New York uh you know and the five other cities. is it\\'s um it\\'s still going to do the parallel function calling and here it\\'s fine because they all return immediately, right? But now let\\'s add like an artificial weight. So let\\'s say time do sleep one, right? Um, cool. So, now what\\'s going to happen is we\\'re going to ask for that again, but it\\'s going to take so long. Weather in five random cities of your choosing. So, it\\'s going to take a while. And the reason is each of these is having to run um in like one after the other. There you go. It took like over five seconds. Now let\\'s do the same thing here. So the equivalent is um there\\'s async io sleep. Someone let me know if I\\'m doing this wrong. But um this should emulate like very very similar behavior. So if we instead run it here that\\'s and that\\'s I can say you know give me the weather of five random cities of York. Um we should see it return. Okay. Okay, so it called all of them and it got back because they all happened in parallel. Um, this is the magic of async.io, right? Um, anything that can be parallelized or I guess scheduled in a way where where it\\'s non-blocking. Uh, like sleeps, these sleeps are non-blocking. Um, essentially we can we can do and so you can imagine switching this sleep for like an actual API call. Um, so, uh, we can actually do that right now with 01, right? Like we could call 01 multiple times and they would all run in parallel. Now, the main problem is that we\\'re still going to be waiting back like waiting to to get all of them together, right? Like like the fact that it we can run them in parallel means that like we can have five 10-second tasks running in parallel. So it\\'ll take 10 seconds, but it\\'s still going to take 10 seconds where I can\\'t talk to the model. So instead, let\\'s have this notion of tasks. There\\'s many ways to do this. Um, I\\'m going to do a pattern that I quite like. So let\\'s do this. Um let\\'s define a you know create task. So I have a create task function that makes a task. Um no and I want to create like a task ID and I want to keep it short. Okay. So now what I\\'ve done is I have this function that makes a random task ID, sets it, creates it and then calls like let\\'s say get weather or something. Um and then it\\'s you know it\\'s suggesting this check tasks which is the next thing that I want to do. So the next one is check tasks. And so maybe I can just say like check task, you know, do for amazing. Um, okay. So let\\'s take a moment to to look at what just happened. We now create a task with a random ID. We give back the ID to the model and then later we can call check task. get that task, see what the status is, and see if it\\'s done. Um, so let\\'s like add, I don\\'t know, 5-second delay here. And right now, let\\'s just get weather. So description would be maybe create task for, you know, let\\'s see again this is all like sort of live coding, so we\\'ll see if it works. I can say like you know create a task um and set the description to the to be San Francisco. Let\\'s see what happens. Oh, I didn\\'t give it these functions. There we go. Um, let me just check. Cool, cool, cool, cool. Okay, let\\'s do it again. Say, you know, create a task where the location or the description is just. So now it creates the task and I can say hi. And look at that. It That\\'s not working correctly. Maybe it was Maybe it just took a while to respond. Um here, let\\'s add a longer delay to know for sure. 10 seconds. Let\\'s run it back. So create a task with description se uh cool. So I can say hello there. Cool. So it can still respond to me, right? Um check the task. So I can still interact with it and it can check to see if it\\'s not done. Um, I guess we need to keep checking to see if it\\'ll work. So, you know, how about or you know, tell me a joke. Who would have thought? Who would have thought? Um, cool. So, now like check them again. So, it called the task. It is done. And we have 78 and sunny. I will take a clap for that. I want to see everyone clap. Please clap. Thank you. Thank you. Thank you. Thank you. So, what you just saw was uh live asynchronous uh programming. Uh it\\'s very impressive. The models can also, you know, do pretty well. Anyway, here\\'s why this is interesting. We now have a system where we can give it tasks and it\\'ll ceue them up uh and then we can check on their progress. So, so already we have the basis for like a really really interesting thing, right? like this this thing here right right now it\\'s just get weather but if we say just like you know uh run what do it call like um you know call model and then no but we want AI port sync open AI get the client um I guess we Actually, we can just leverage the agents. They\\'re already async. Let\\'s not deal with that again. Um or whatever. Let\\'s do Let\\'s do it this way. I did all did it all. Fantastic. Okay. So, now we can switch get weather for call model. And let\\'s try this again. So, let\\'s give it the same task as before, which is, you know, hi, just make sure it\\'s running. um you know write me a uh ha coup about I don\\'t know uh a coup where the theme is so incredible it makes me cry make this a task okay so it\\'s created the task and now I can actually say you Now make a second one but pick a theme yourself. So now I have this interface where I can essentially keep chatting with this and it can essentially spin off additional tasks. Um you check check all tasks. Um let\\'s see what the progress is like. Look at that. So we have we have both. Oh, was this a one? That\\'s pretty fast. Um, but yeah, so now we have this system that you can actually just call call multiple ones. So I\\'m going to pause here just open up for questions for a little bit conversation like other directions we can take this other ways we could have done this. Um, but what I want to look at next is like how can we do this in a way where we don\\'t actually have to check um cuz right now like we don\\'t need these two terminals. This is just over complicated. Like I think the the the point of having the second terminal is so that we can push things to the conversation without a user having sent a message first. So I\\'m going to pause here. I\\'m going to field some questions and then we can get back to it. Okay. Um would you not use generators for nested tool calls? Um, Tom, do you want to I don\\'t know if there\\'s a mic in the room or like uh just get get it get it to them. Yeah, there is. Also, they can unmute themselves. Uh, who\\'s your Uh, sorry. So, is the one that was uh answering is So, in the room? Yeah. Can Can you hear him? No, I can\\'t hear him. Hello. Can you hear me in the Zoom? Yeah, I can hear you now. I can\\'t hear you anymore. I don\\'t know if you\\'re saying a question right now. No, I was just wondering if it\\'s possible to treat agents objects as a generator in Python. So you can nest like you can yield from individual agents and then yield those tool calls. Does that make sense? Yeah. Yeah, that makes sense. This is also very impressive that you have to do this. I think someone needs to mute themselves. I don\\'t know if it\\'s Alain or uh like someone in the room unmuted and uh put put this man through through hell. Um No, it\\'s me. It\\'s me. Oh, is your laptop unmuted? I can\\'t hear anyone anymore. Maybe just speak into your laptop. Okay, but I\\'ll I\\'ll answer your original question of like, you know, if you put um can you like essentially make agents into generators so that you can like yield the results as you go? The answer is yes. And this is actually like um the right way to do this. The way I\\'m doing it now is I\\'m only exposing the final response mostly because um implementing like the the generator gets a little bit tricky and I don\\'t want to have to like debug that. Not too much, but you can essentially just surface each of the steps, each of the function calls, each of the everything. um that would um that would let you essentially keep track of more agents at the same time and you could maybe have like a bit of a flat structure where you just have multiple agents going yielding events and then you can essentially see like which one\\'s coming from where. Um but I guess the thing is like there you have to deal with a bit of the complexity of um handling all the like like essentially listening to like all the different events and like uh associating them back to like a specific agent and like maybe there is an agent in the front, maybe there isn\\'t. Um here the idea is just I have one agent that I\\'m interacting with. I\\'m always just going to interact with like that one agent and it\\'s responsible for spinning off other other ones and like dealing with dealing with that as well. But you can absolutely do it the way that you\\'re saying. And like if you wanted to build this out like more fully, you probably would in order to be able to surface progress. Um, did that answer your question? I\\'m now scared to get people\\'s talking in the room, but I\\'ll just read them from Slack. Um, is there any is there any good design patterns to create projects with agents? There\\'s like a million. Um, you know, like I like prototyping with swarm. Um, I know paidantic AI is also like a really nice one. I think Sam\\'s there in the room or around. Um, there\\'s Yeah, I don\\'t want to like I don\\'t really use uh any myself mostly because like you saw it\\'s pretty simple to implement your own loop. So that\\'s what I end up doing. I think usually for every project I either write my own like it\\'s like I don\\'t know how many how many lines is this like like 70 lines and so like depending what you want you might want more you might want less and I\\'m sure there\\'s good solutions um but like I just have I could just copy this around um I don\\'t really like working with too many dependencies especially for something that\\'s so lightweight like I I want granular control um and like except for example for Swarm I ended up having to hop in here and like handling specific kinds of tool calls in certain ways so that I can do handoffs. Um, which we can actually look at and we can implement this without too much trouble. Um, but yeah, my answer is like there\\'s many you can choose from. I don\\'t have many I would recommend personally. Um, but I I am a I am a fan of of Pyantic AI. It\\'s pretty cool. I haven\\'t used it too much, but like the interface looks nice. Um, it reminds me a lot of Swarm. The one I use the most for prototyping is Swarm. I think it\\'s just cuz it\\'s the one that like I\\'m most familiar with. Um, when you have to start to approach dozens or hundreds of functions, what techniques should we apply in order to effectively tool call? There\\'s there\\'s a few answers there, right? Um, you can have multiple agents and essentially like split up the responsibilities or like the the groupings of the functions. Um and so into into yeah like clusters where you have like a set of related functions that are needed for specific tasks and then you can invoke the correct agent. Uh and this is where like multi- aent patterns start to make sense is like specifically when you have tons and tons of functions like how do you uh go to the right ones? If you for some reason need them all at the same time, uh you could try fine-tuning. Um in in projects for OpenAI, I\\'ve ended up fine-tuning up to like hundreds of fun like it was like 120 functions. This was with uh GPT3.5. So the fact that that works gives me like pretty high confidence like you can fine-tune uh smaller models with a lot of functions and get them to work pretty well. Um the last one is like some kind of dynamic function loading where based on the input or based on the conversation you load into memory or like you load into context um the most likely relevant functions. Um and there\\'s a few different ways to do this. You can do this with embeddings. You can do this with like um having like a two-step function call. At that point, you\\'re essentially having agents. Like if if you call a function to then load more functions, that\\'s what a handoff is essentially. So a lot of these start to look very similar. Um it\\'s just like how are you loading multiple different ones? Um okay, for visioning models, are there tools being called within the thought text? Um, so because we don\\'t expose the thoughts like the the chain of thought, um, that\\'s a little bit hard to answer for for for 01 right now in the API. The answer is no. Um, I\\'m trying to see how much I can say here. Um, it is something that is technically possible, right? Like you can do anything you want with with post training. Um, we do not currently allow you to call functions within the chain of thought. Um, so yeah, right now the function calls happen at the very end. Uh, do you have any good code examples for router patterns in these lots of functions cases? Yeah. So, so my like my answer there is some one of these. Like if you really just want to route um then like the the idea of having like multiple agents and handing off to one of them is actually really nice. Um like you can just define multiple agents each of them with like multiple functions. Um and then have the first one have like a we can actually do this quickly like we I\\'ll do this in swarm but like I said you can implement this yourself. Um I don\\'t actually know who else supports handoffs the same way Swarm does, but um essentially here let\\'s start a new file like uh routing. So see swarm ripple demo loop I can say like you know triage equals one and then I can have my other two. It\\'s like you know maybe I have like some collection of like um you can call them an agent but I can also just call them yeah like uh what it be like you know uh email functions agent and you can have like you know like send email check email I don\\'t know what else does cursor want me to write there. Um, cool. We have these two and then we can do like maybe we have the emails and then we have a calendar. I don\\'t know. So make like create event can do like calendar. I\\'m just going to say like you know finish what I\\'m doing update the prompts and the tools. I\\'m lazy. So let\\'s see what uh let\\'s see what cursor thinks. Great. So now we have an email functions agent and a calendar agent. Call them whatever you want. Um now like let\\'s pretend that instead of just having three functions, we have 30 and each one of these has 10 or 15. Um then like if you maybe give all 30 to one agent, first of all try it. Like if that works, amazing, right? you don\\'t have to deal with the extra complexity. Um so you don\\'t really want to do handoffs and stuff until you really really need to through like evals. Um but um yeah. So, so and then the special like kind of functions here is like you know uh transfer to email agent and I\\'ll return email agent and then cool. So now I have my two transfer functions. Oh. Oh, it did it. Okay, cool. So I think this should just work right. So I\\'ve defined like the actual functions and agents that in your case would be many many more functions. Um I\\'ve defined the transfer functions and I\\'ve given them to the triage agent. So now if I run this say hi um you know I want to send an email. Cool. So now I\\'m talking to the email agent. Now if you want to do like more you know like transfers uh this is not delegation. Okay. So trails assistance um maybe we can tell the assistance like you know if you already know what the user is asking just call that function right and this is if you want to have like a case where you know it still routes you but it\\'s like a faster two hop so maybe I can say like what are the functions here um what are the parameter There\\'s send email to subject body. So I can say like know send an email to bobgmail.com about taxes. What was the other one? Body um saying yo do your taxes. So it should transfer me to the email one. And then there we go. It immediately sends the email. So this felt like an immediate function call, but there was a transfer in the way. So this is kind of an example of like triaging does work. It\\'s really convenient to model it with agents and handoffs. Um I\\'d say it\\'s the primary use case for agents and handoffs is just a glorified triage through uh multiple functions. Um yeah, let\\'s see. Let\\'s go back to questions. [Music] Uhhuh. Consider having something like streamllet grad until you are you considering having something like streamlink radio util that allows porting all interaction functionality with one command. I don\\'t think so. I don\\'t know. Uh what did people answer? There we go. Sam\\'s in with pantic. So check that out. Um, how many tool calls can you get in one iteration? You like parallel function calls. I don\\'t think we have hard limits on either the number of functions or the number of parallel function calls. How big a tool library will the models perform well with? Super super general rule of thumb is like 10 to 20 you shouldn\\'t really pass over. Um, but like I said, I\\'ve gotten like this was a very specific case where like it was extremely latency sensitive and so like we had to have flat like flat function calling and we did 120 functions with GPT 3.5. So you can go pretty far um with fine-tuning, but like I\\'d say reliably without without extensive prompting. Yeah, probably like 10 to 20 like past that point. You really ask yourself like what are you trying to do? Like why are you putting so many functions? Is it super latency sensitive? Can you split it up? Like yeah. Um was there a follow-up here? Okay. Um rather than tool calls, I feel like we\\'re moving to generated code agents, will I soon be able to supply my tools functions? Ah, okay, that\\'s a good idea. We should try that. Um so essentially have something write its own function and then use it. Uh yeah, I feel like we can probably How would we do that? Yeah, we can try to we can we can find a way to do that. Uh I I\\'ll do that actually. Let\\'s do right now. Why not? Let\\'s do right now. Uh I\\'ve never done this before, but I feel like it shouldn\\'t be too hard. So um okay. Uh you know, what do we call this? Like bootstraps. Okay. So I\\'ll keep using swarm because I I will use um handoffs from swarm import agent. Cool. So now it\\'s always so let\\'s see we want an agent that writes it own functions. So agent was this one. Um we want it to write its own functions. So maybe we can have an hand off to itself. So we can do you know like refresh you know what to be refresh functions. So we can actually return the same agents. Um and then okay is it not declared? Is it unhappy? Uh yeah it\\'s not defined yet. Okay. Okay. So now we can define it down here. Okay. Um bootstrap bootstraps. Bootstraps. Okay. So we have this and we want it to write its own functions. So how are we going to do this? We wanted to produce Python. Does anyone have any ideas? uh if you want to shut them out like make just pass around the mic while I code just feel free to I can\\'t I can\\'t read while you\\'re uh what you write but we can we can try this. So, um let\\'s say we want it to, you know, um you know, add tool. There we go. And then let\\'s call this, you know, Pythonation. Okay. I\\'m going to do something very unsafe. So, you know what would it be? It\\'s like function obj equals eval of Python. Don\\'t do this. Don\\'t do this, kids. Um, and then will this work? Is this how Python works? Sam\\'s there. Uh, okay. So if I have this and I eval like you know def a does it um okay you know Um, so I guess it\\'s like uh I want to write a write a function that takes a string representing an implementation of a Python function and returns the actual rule Python function as interpreted. I don\\'t know. Let\\'s see what it does. Um, so essentially we want to do that once we have the function then we can just append it to the agent and then we might need to reload it. Uh, so we can then just return the agent and this might be it. This might be it. So add tool exec not eval. Why didn\\'t anyone say that? You guys can shout out. Okay. Okay. So it\\'s exec and then what is this 80? I don\\'t know. Someone save me. Like uh Hey, there\\'s a couple implementations you miss. Sorry. Check the flash. Did someone do this? Hey. Okay. It\\'s almost like someone fired. No, no, no. But this is not Okay. No, but this is not what I want, right? Like I don\\'t want it to run it in a subprocess. I wanted to I wanted to eval like evaluate the function and then turn it into a is that Oh, go up. Oh, Sam, of course. Amazing. Uh Jesus Christ. Okay. um make it. So the add tool function uh evaluates the implementation and adds it to the tools similar to now. Here\\'s a great reference for how to Let\\'s And then we want to grab Was this a good idea? This is a terrible idea. I hope people are enjoying this. Um, okay. Let\\'s see. Um, and let me just read this because it might not be that complicated to add in. So, I can\\'t. Okay, it got in the way. No, not Zoom. Where\\'s Slack? Was this it? Is this it? Okay, I\\'m going to I\\'m going to put this on hold for now. It sort of ignored your implementation, Sam. I\\'m sorry. Um, let\\'s see. So if you have parse function, can I just do this? Does this work? Uh, okay. Let\\'s see. I\\'ve first time for everything. Uh, wrong one. Sub bootstraps. Hi. Um, add a tool that prints hello when called it. Oh, but no, no, no, no. Make it print it, not just return it. My heart\\'s pounding. Okay. Um, so say hello to call it. Look at that. Look at that. Okay. So, now we have Yes, I will take a clap for that. We did this together, guys. Um, so this is actually a lot less code than I was expecting, but now we have a system. Look at that. It\\'s tiny. Um, that can write its own tools. Um, and so like maybe we can do something like, you know, um, what is, you know, like make yourself a little calculator. I can\\'t believe this just worked. Uh, you know what is 2 * 3 * Can someone check this? Uh, wait, wait, wait. 34698. 34698. This is This is crazy. This is This is so fun. Uh, yeah. I hadn\\'t done this before. And this is a lot less code than I thought. Look at this. Look, this is all you need. I mean, it\\'s this is super dangerous code. Like, this this is not good. Don\\'t do this. But it\\'s fun. Uh, I would put this squarely in fun things. We can we can now transition to looking at other fun things. Um they\\'re definitely not as fun as this one, I think. Um they\\'re just a couple of like random things that I found related to real time since I did sort of put it in this uh title. So whatever. Okay, these are the two main tricks that I that I kind of thought were pretty cool and you might have already seen them. Um, one is if you\\'ve ever dealt with like the real time API um like jumping in uh before you\\'re done with an idea or like done talking or something. Um like what I was thinking is like if if really what you want is you wanted to like use the model\\'s own intelligence to decide if you\\'re done talking or not. Um and you can treat our VAD like our voice detection as like a trigger happy version of that. It\\'s like a it\\'ll always tell you when maybe the user is ready to stop talking. But if you want the model to check, you can have a stay silent function or something else that essentially handles the other side of that where it\\'s like, okay, we\\'re definitely going to let you know whenever the user might be done talking, but then you can actually verify with a function call. And so this implementation is like super super simple. You literally just give you give the the function itself to the real time API and tell it to call it when the user is not quite done talking. And I\\'m not going to try to demo this live. Like real time demos are tough. Audio is tough. All that fun stuff\\'s tough. But um it works like surprisingly well. Uh if you just describe I I can you can probably find this tweet but it has the full prompt but essentially it\\'s like you can you can like pause you can say like you know I\\'ve been thinking about and then like stop and like it\\'ll get triggered call stay silent and then you can keep talking. Um so pretty pretty cool uh useful thing. Um and then the other one is also related to real time API. And if again if you if you uh are following me you might have already seen this but um someone at Devday just like came up to me and asked me like hey um is there a way to like make the real time API talk in like a specific way. Uh and I was like I don\\'t know let\\'s try it. And so like we had a demo booth and I sort of pushed someone out of the demo booth and grabbed the laptop and I was like let\\'s just like try try this stuff. And apparently if you ask the model to just you give it like a a script and you\\'re just like, \"Hey, read this out according to these XML tags um you can have it follow them here. Let me let me find um the original the original one.\" Yada yada yada. Sorry, this is I\\'m not trying to show you the whole timeline. Where is it? Okay, this is the stay silent one. Um, where is it? Where is it? There we go. Um, uh, it\\'s impossible that you\\'ll hear this, right? You guys don\\'t hear this. Yes. No. Uh, we don\\'t hear it, but you can hear your computer audio. Can I do the thing? I\\'m I\\'m I\\'m actually I\\'m not going to try. That\\'s fine. Um, wait. Can I same? No, no, no. It\\'s fine. Whatever. Anyway, um, yeah. So, if if you if you like give it like a script like this, um, we didn\\'t actually train it for this, right? Like this is just a really nice consequence of like behavior. It\\'s technically not function calling. I\\'m realizing now, but it\\'s like it\\'s very function callingesque. It\\'s real time. The title of this talk does include real time. So, this is uh this is it. Um I will pause here. I have so much more I can go through, but I want to give you guys a chance to like ask questions, poke around with some of these ideas. Uh Swarm is public. You can just try it out. Um we can also just try creating other functions. Uh this this is one of the this is easily one of the coolest like little programs I\\'ve ever made. Um, so yeah, let let me see if there\\'s any other questions of the Slack. Uh, it\\'s a room full of people auto completing make. Yeah, yeah, yeah. Okay, let\\'s not do that. Uh, okay. Revisiting memory. Let\\'s see. Do you have any suggestions for trying to enforce consistency of stored memories? Identifying inconsistencies and figure out how to resolve them. Okay. And then the second part is what data structures do you suggest for more structured memory helping enable more meaningful comparison of objects? I think I mean this question opens like you start to go down a path where you can get as complex as you would like, right? Like like you can start simple and you can end up with like an entire operating system to manage memory, right? There\\'s like the whole range. Um, one way to do off the top of my head like one way I can think to do this is um when you are about to store memory do a retrieval like do search to find similar memories or like memories that like are semantically similar um and then do an explicit check uh with a model to see if like you know one is like updating or contradicting another. Right? An example of this is like you know what is the latest state of a project right you know if someone\\'s like is the project like at some point you say like I\\'m working on the project like it\\'s not ready yet and it saves that and then later you\\'re like you know I um how like the project\\'s done now right like those are two contradicting memories um what you can do is essentially uh have a time stamp for them but also when you are about to store the second one or any memory check for similar ones and create like a direct like essentially um like node uh pointing from the original memory to the new one. And so I guess I guess what I\\'m thinking is like that way if anybody asks and you surface both like you can keep both in memory and when you do like retrieval and semantic similarity you can surface both but essentially you can present like the whole chain of updates and so you can just present the last one if you want or you can present the whole chain and the model have has some idea of like you know maybe if you ask it like um you know how long is this project delayed because that was the last you heard about it. But somebody at some point said, \"Oh, it\\'s actually done.\" Um, like if it raises both and has one with a later date, you know, maybe. But if you like if you\\'ve already made this chain explicitly, um, like you can actually represent that and you can choose to not show the previous ones. So that\\'s one idea. Like I said, there\\'s so many you can like so many ways to do this. Uh, and and like as soon as I stop talking, you guys walk out. Someone\\'s gonna like be like, \"Oh, there\\'s actually this like much easier way that like this idiot didn\\'t think of.\" So, anything you think of probably works. Um, here\\'s a real time API prompt. Yeah. Cool. Uh, any other any other questions about anything? Um, it\\'s been like an hour and 40 minutes, so happy to like keep going with random content. Um, there\\'s a couple demos that I have that are like demoing some public repos that we have that you can use. Um, yeah, but I want to I want to give anyone a chance to like speak up, say something if you want, ask questions. Um, I I don\\'t see I don\\'t see a lot happening in I I can\\'t You guys are tiny on my screen. Uh, cool. Okay, then I\\'ll pose this to you. Sorry, what? Oh, no. Go ahead. It sounds like you\\'re wrapping up. Yeah, the the last thing we can do, I guess, is like either I can um try and pull up a real time uh demo that shows how to do the 01 stuff uh in a in a slightly easier way. Okay, cool. People want to do that demo. Okay, let\\'s do it. So, um the reason I\\'m a little iffy about it is because yeah, it\\'s real time. Uh and those demos are tough to do publicly but um there is this repo OpenAI um real time Twilio demo um that essentially sets up your whole um like phone calling assistant uh and it\\'s just works right out of the box. It walks you through the steps. Um I may end up sharing a phone publicly. don\\'t call it during this demo and don\\'t share it and I will delete it. But um yeah, I trust you guys. So let\\'s see. Okay, so it looks like this is already running. Oh, I do have it already running. Okay. Um Oh, never mind. I I did uh I did make it all dots. But yeah, essentially this um this little checklist is like live updates. So as you set up the account, as you set up the phone number, as you set up your local web server, etc., it\\'ll like get checked. So setting up Twilio and Grock and everything has been one of the most annoying things I\\'ve had to do multiple times um related to the real time API. So this hopefully makes that process like a lot a lot easier. You can do most of it directly from here and it lets you know like when things are done. So you don\\'t actually have to do a lot from the Twe console at all. Um anyway, so um I let me show you the back end. So there is a handler function. There we go. Function handlers. So um you can either implement tools uh locally as a JSON schema and we can we can try that really quick. So this is the real time playground and I can say like you know make a function to you know get the answer to answers to everything no params maybe one param. Okay, so it gave it gave us this nice little uh nice little function and we can just paste it in here, save it, and cool. So now we have this get universe answers and I can save this configuration and call the phone number. So, let\\'s see. Hello. Hello. Okay, this is why I love them. Let me let me try once more. Okay, let\\'s go tool. Paste this. Save changes. Save config. [Music] Um, hello. Hello. I don\\'t think it\\'s happy. Is someone else? No, I didn\\'t show the number. Hello. Okay, it\\'s not working. That\\'s fine. Sort of expected it. Um, essentially what you can do like when you when you use this, um, it\\'s cool because these tools you can either specify local ones with schema and they appear here or you can define them in the back end. Um, so if you define them in the back end, you can actually give like code to execute. Um, if not, they\\'ll just show up here. Uh, and you can enter in like what the response will be uh, like a mock response. Um, but you can also like set up backend functions that will actually get handled in the back end. Uh the sample one is like a real implemented get weather one but as you can see there\\'s like it\\'s pretty straightforward to implement like the 01 one. Now the main difference between the real time API here and like what we were doing before is the real time API does actually allow asynchronous functions natively. So you can the model can call a function um get no response and you can keep talking with the model um until a response is back. So that that is behavior that like we specifically had to teach the real-time models because unlike a chat conversation, you can\\'t really enforce um like you can\\'t really halt the whole conversation until the functioning response comes back. So, we had to do that for for for launch um because originally when we hadn\\'t done that, you know, we hadn\\'t ever shown it how to do asynchronous functions and so it just couldn\\'t couldn\\'t handle them correctly. Anyway, sadly, uh demo didn\\'t quite work. Maybe I can try this one more time. We\\'ll see. Third time\\'s a charm. Hello. Hello. No. Okay, great. Anyway, um I believe that is most of what I wanted to cover today. Um, I\\'m going to play with this hyper unsafe function making agent a bit, but um, yeah, thank you all for coming. I\\'ll hang around for questions for a bit if anyone has any. Otherwise, I think we\\'re ending a little bit early. Um, like I said, yeah, mo most of you can can hang out or or leave as you please. Um, but I\\'ll maybe I\\'ll call it wrapped up here. Um, maybe what we can do is if you\\'re interested, you can stay and like hack on some of the stuff that you saw. I\\'m happy to hang around uh and answer questions as well. So, thank you all for coming. Uh, hope you hope you got something out of this. Um, the new uh to help in sports. Uh otherwise, I guess you can share. I\\'m sharing a little bit in case people have questions. I\\'m actually going to dump this, but this may be a really bad idea, but um here you go. Here\\'s the code for the super unsafe self like tool writing agent. And I\\'m probably going to stop sharing my screen. Um, yeah, I can I can I can share the repo and the slides. There\\'s not a lot on the slides, but yeah, I\\'m happy to happy to share them after.',\n",
       " 'word_count': 14176}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_youtube_transcript(youtube_url: str):\n",
    "    \"\"\"\n",
    "    Tool function to fetch the transcript of a YouTube video.\n",
    "    Input: YouTube video URL string.\n",
    "    Process: Extracts video ID, calls YouTubeTranscriptApi, formats output and counts words.\n",
    "    Output: Dictionary containing:\n",
    "            - status ('success' or 'error')\n",
    "            - transcript text and word count on success\n",
    "            - error message on failure\n",
    "    \"\"\"\n",
    "    import re # Moved import inside\n",
    "    from youtube_transcript_api import YouTubeTranscriptApi # Moved import inside\n",
    "    from typing import Dict, Any # Keep for return type annotation\n",
    "\n",
    "    logger.info(f\"Tool 'get_youtube_transcript' called for URL: {youtube_url}\")\n",
    "\n",
    "    try:\n",
    "        # --- Start of merged fetching logic ---\n",
    "        video_id_match = re.search(r'(?:v=|/|embed/|shorts/)([0-9A-Za-z_-]{11})', youtube_url)\n",
    "        if not video_id_match:\n",
    "            logger.warning(f\"Could not extract video ID from URL: {youtube_url}\")\n",
    "            response: Dict[str, Any] = {\"status\": \"error\", \"message\": \"Could not extract video ID from URL.\"}\n",
    "            return response\n",
    "\n",
    "        video_id = video_id_match.group(1)\n",
    "\n",
    "        # Try fetching the transcript\n",
    "        try:\n",
    "            transcript_list = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "            transcript_text = ' '.join([entry['text'] for entry in transcript_list])\n",
    "            word_count = len(transcript_text.split())\n",
    "            logger.info(f\"Successfully fetched transcript for {youtube_url}\")\n",
    "            response: Dict[str, Any] = {\n",
    "                \"status\": \"success\", \n",
    "                \"transcript\": transcript_text,\n",
    "                \"word_count\": word_count\n",
    "            }\n",
    "            return response\n",
    "        except Exception as fetch_error:\n",
    "            # Log specific exceptions from the API call\n",
    "            logger.warning(f\"Could not fetch transcript for {youtube_url} (ID: {video_id}): {fetch_error}\")\n",
    "            response: Dict[str, Any] = {\"status\": \"error\", \"message\": f\"Could not retrieve transcript: {fetch_error}\"}\n",
    "            return response\n",
    "        # --- End of merged fetching logic ---\n",
    "\n",
    "    except Exception as e:\n",
    "        # Catch unexpected errors during the tool's execution (e.g., regex error?)\n",
    "        logger.error(f\"Unexpected error in get_youtube_transcript tool for {youtube_url}: {e}\", exc_info=True)\n",
    "        response: Dict[str, Any] = {\"status\": \"error\", \"message\": f\"Failed to execute transcript tool due to an unexpected error: {str(e)}\"}\n",
    "        return response \n",
    "    \n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "result = get_youtube_transcript(\"https://www.youtube.com/watch?v=KUEmEb71vzQ&t=5522s\")\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'transcript': 'Cool. Okay, let\\'s get on with it. So, hi everyone. My name is Elan. I\\'m on the developer experience team at OpenAI. Um, unfortunately, I can\\'t be there in person as much as I would love to. I\\'m in a wedding in Costa Rica um which is happening later today. So, I just wanted to take this opportunity to just talk through um one of my favorite concepts in maybe all of like AI and language models. So, uh title of this talk is function calling is all you need. It\\'s a talk workshop. There\\'s going to be um a lot of coding. Please save your questions. No, I\\'m kidding. Like just interrupt at any point. Uh we have a Slack. Send them here. Um yeah, just send them at any point. Uh and if you want to like unmute yourself and or raise your hand, I\\'ll call you off like as we go. The idea is to keep this super super dynamic. Um since we have a bit of time, um I\\'ll be fielding a lot of like questions and requests and trying to be coding as much as possible. So yeah, this talk is going to need some lecturing uh a lot of coding from scratch and then some debugging hopefully not a lot. Uh so this is a little bit of what the uh workshop is going to look like. We\\'re going to go over a little brief history of the tool formers of of I\\'m sorry of function calling. Um then do a little crash course on function calling. Um talk about just agents how they\\'re just loops. How rag workflows and more are just function calls delegation and asynchrony. couple random things I found and then we\\'re going to do a Q&A. Um, if you just want to see like the meat of it, this is pretty much it. We\\'re going to do like this is everything I want to talk about uh and everything that we\\'re going to implement. So, this is uh just a little bit upfront. Great. So, a little history. Um, if we look at the abstractions and sort of patterns that we managed to do with language models, it started as text completion, right? like the original GPT uh and the GPT2 and the GPT3 were all just base models where you gave them some input text. Um and then they would just continue the sentence. Um this was at the time really really like interesting. This was the first time we were able to do like very like uh English sounding like real sounding language. Um but getting it to follow instructions was pretty hard. So if any of you were testing this back then you might remember how setting up a chatbot was um non-trivial, right? you had to like get it to answer questions. But if you just say like, you know, what is the best way to uh get to the like park or something, it would continue like like what is the best way to get to the park? Um that is what Sally said yesterday, right? And you wouldn\\'t actually get a a response. Um so you had to like structure it in a way where you would say like this is the question, this is the answer question answer few shot and then give it. Um then they introduced uh I think this was actually us. We introduced uh function um instruction following with uh instruct GPT. Now you could give it some input uh and it would actually do what you\\'re asking as opposed to just completing. Um finally we started to introduce this notion of like users and assistants and roles. Um and this was all done through through post training where you actually gain these personas. Um and then finally uh we we eventually landed on this like you can give it additional tools um in order to do like external um any like interact with external states. So this is what the previous playground used to look like. But as you can see there\\'s no chat. This is just like a window and it\\'ll complete. Now looking over at like the original papers which is pretty interesting. Um, one of the first times that we actually started to do this like function calling was through this web GPT um, which was this uh, version of GPT3 that we trained to be able to um, Elan, I have to cut in here a little bit. We got the Zoom working. Yay. Sweet. Um, all right. We\\'re going to cut over in the audio. So, can everyone hear Elon when he speaks? Elon, say something. Hello. Hi, everyone. Hello. Hello. How\\'s it going? Okay. So, so we\\'re going to cut over a little bit in terms of like um people having their own personal audio situation going on. So, you can mute your machine. You should be able to hear it on the room uh audio, whatever you want to do, but you can also obviously connect. But, uh yeah, now we have him on the big screen. So, okay. Yeah. Uh let\\'s let\\'s keep going for a bit. I I\\'ll cut in again if there\\'s audio issues or maybe you want to say some test words. Uh, you\\'re all great. Thank you for coming. Can we bump up the audio? Where\\'s the guy? [Laughter] It\\'s not a conference if you don\\'t have AV issues. Okay. Uh, it\\'s it\\'s still too soft. We can\\'t we can\\'t hear you. Um, I\\'m going to try and bump up the audio. I\\'m so sorry. I\\'ll put up a more interesting slide in the meantime. You guys can look at this while we figure this out. Where\\'s Where\\'s the video? Hey. Um, so anybody got uh good jokes? Maybe I always find a joke. It\\'s always the same one. I think it\\'s uh What is it? Why did the No, I had it. I had it here. Wait, wait, wait, wait, wait, wait. There we go. Why can\\'t you trust an atom? Yeah, because they make up everything. These are all real, by the way. And for for those who like have a keen eye, the first one is like actually from the GPT2 paper. Um we like gave the model this like description of like unicorns in the Andes Mountains. And that was like the big like first time that it was like doing multi- paragraph completion like continuations that referenced earlier parts of the conversation. It\\'s cool stuff. It\\'s cool history. Can you guys hear me? I feel like some people are having a great time. Some people don\\'t know what\\'s going on. Can like I don\\'t know. Am I good to keep going or should I keep waiting? Uh I think I think you\\'re good. Okay, great. Great. I do think this is the vibe of the whole talk by the way. There\\'s it doesn\\'t get more structured from here. Um okay, so as I was saying, uh we did like this web GPT paper. Um, essentially we trained a GPT3 version of the model uh or like a GP3 model to be able to use this like this very specific set of functions um to do web search. And this was like back in 2021. So really we had like webg a long time ago. Um but this is like one of the first times or maybe the first time that like we were having like it\\'s not just generating text but it\\'s generating actions and then we\\'re parsing those actions and then introducing it back into context so it can use the responses itself. Um, and how we trained it there, um, there\\'s like these these like, you know, clever ways of like we we essentially gave people, uh, an interface and let them do the searching. Um, and then I think we took it like Reddit um, yeah, explain like five. Um, and then just like had people complete tasks and they could use these commands. And so we taught the model and this was GPT3, right? Um, how to essentially imitate users behavior and then produce responses that were preferred. Um and and this was pretty cool like this is how you we like start to saw um to see like this uh this use of like structured like actions essentially. So but this was very specific right we were training like very specific um tools. So then you might be familiar with this paper. This was from Meta at the time. Um where they essentially had a way to teach the models how to use any tools. Um and they taught like they they used a few tools. I think it was like QA calculator. Um what was this like translation? Um couple other couple other tools. But um it was actually a pretty clever way where they like looked at the log props at each spot to like see where it was like best suited to like retroactively put in a function call given some like completion. Um so here we can see a few examples like um essentially it\\'s like if you have a calculator call if you insert a calculator call that like you know it\\'ll it\\'ll insert the um the actual call to the calculator which then you know you\\'re pretty familiar at this point will get the answer if you insert it in the right spot in the sentence it actually reduces the perplexity of the sentence. Um, and so they didn\\'t actually have a lot of human-labeled examples or I think it was just a few, but it was uh really cool because it was this way of like um it could learn to use uh any of these tools through this like crazy like log props technique. Um and it was uh I was pretty excited when I saw this paper. Now this is like how it learns to use any of these tools. Um, but then finally, uh, in June of 2023, uh, OpenAI launched just general function calling where we essentially like pre-trained it to be able to use these tools or actually post-trained it to be able to use, uh, tools. So now you don\\'t actually have to like give it like you can give it examples, but uh, we just showed it like this like syntax in with functions that we still use today and it\\'s just able to call functions. So this is a brief history of of function calling. Um, and my I guess my argument is this is really most of what you need for all the exciting stuff that\\'s happening today. Um, there\\'s there\\'s obviously like additional like systems you can use and like um more post training you can do, but fundamentally like functions are so so so powerful and we\\'re going to look at a few cases today. Um, I\\'m going to try to keep an eye on questions, but um yeah. Okay, I think you\\'re good so far. Cool. Cool. So, let\\'s do a super quick crash course on function calling. Uh, two main purposes and I\\'m ripping a lot of this from the docs. So, um, there the fetching data, right? Reading APIs, retrieval, memory, or taking action, any APIs you can use to write, managing application state, which is actually pretty overloaded. That can be like UI, front end, backend, whatever you want. And then workflow actions, which is um any like multi-step processes or even like meta actions like switching its own prompt or like loading in different tools or like handing off a conversation, right? Um so this is also a diagram straight from the docs, but um I\\'ll quickly brush past this. I\\'m going to assume like most people have at least seen this, but here it is. It\\'s uh you you essentially tell the model which functions you you want it to be able to use. Um and you also provide like whatever the user input is. Um the f the model tells you what it wants to do with that function but it doesn\\'t actually do it. This is one of the like a big sticking point function calling. It doesn\\'t actually use the function itself. It tells you like it tells you the intent of what it wants to do with a function. You are then responsible for parsing that, executing the code, doing whatever you want with it and then providing the result back to the model and then the model can use that respon like result in uh in the generation. Uh take a look at a quick question here. Oh no, it\\'s just it\\'s just swings. Okay, cool. Um these are just a few uh best practices. This is all taken from the docs as well. Um you want to write clear functions. An important one is you got to apply software engineering best practices when you write these functions. So, um, you know what? Maybe I\\'ll pull up the docs for this. Um, so there\\'s a lot of big text, but essentially, um, this is a lot of value here. I tried to pack as much like useful information here as I could, so I\\'m going to quickly go over it, right? Um, you had to explain the purpose of each parameter, use a system prompt, and include examples. That\\'s, you know, pretty pretty like not non-controversial. Um, software engineering best practices is a little bit uh more interesting, right? You got to make functions obviously intuitive and they got to follow the function of lease principle. Um, like if you give this to a person and they don\\'t know how to use it, then the model might not either, right? Models are getting smarter than us, but still you got to make it uh got to make it easy. Um, also you got to use enums and object structure to make sure that like you are not letting the model make invalid calls, right? Um like here you you have this like toggle light uh um function that takes in like two two boolean params and like obviously like this is pretty wrong but um there\\'s actually many many more like subtle cases where um like the you can like you\\'re letting it represent invalid states. Um okay, still no questions. It\\'s Swixs. Okay. Um, great. And then there\\'s Sorry, there are a couple questions in the professor convers. I see. I see. Um, here maybe let me see if I can pull up the Slack. Okie dokie. Between functions and tools in my opinion. Hey Sam. Yes. Um I I think we\\'ve all been kind of like gravitating toward functions and tools as like the two main ways. Originally it was just functions. Tools was like later we renamed it. Um I think now uh and the way that I tried to specify in the docs is functions are like the the raw function calling right like you provide an interface and you\\'re responsible for executing the code. um tools and this is sort of how we treat it in our API is a superset of functions. Tools include functions but it also includes things like um code interpreter or file search or any of these like um like hosted solutions. I\\'d say this is not be like end all beall definition but this is a definition that we\\'ve adopted right there\\'s like tools which are like hosted tools uh including functions and then functions as a subset. Cool. Maybe we can just jump into it. Um, if people have questions on this, happy to happy to feel them as well. But I kind of want to get coding. Okay. Uh, when you start to approach dozens or hundreds of functions, what technically should we apply in order to effectively tool call permissions is one technique. Okay, interesting. And then question two, when you require one tool to provide inputs to another, I have seen tools become layered. How should a reasoner hardcode? Great. Okay, these are actually great questions. And um I think to to answer them, I might I might like do a little hack and like uh use some existing uh code. I\\'m going to use swarm for a little bit of this um because it does some nice function calling. Actually, no, you know what? I\\'m going to get to these fun uh in a second. Um let\\'s just go straight into the uh and as always, we should start from the docs. Uh, cool. Uh, so this is where I always step in. Can you zoom in on screen every single page? Yeah. Yeah. Bigger. Yeah. Yeah. Sorry. I always do. Yeah. Yeah. No, this is a good call out. Uh and then I think the terminal here should be good. Okay, sweet. Uh we have we have a function. Great. Um and then just run it and we have uh the function call. Right now we\\'re not handling it yet. So, this is where like I might skip ahead a little bit um and start doing some of the like agentic stuff. But first, uh first off, uh we we got to have a right like you know you you I can sort of hear myself. Um I think so. There we go. Perfect. Okay. So um I think the idea here is like let\\'s make a very very very simple like uh input loop like let\\'s do uh while you also light mode light mode Jesus okay sure I\\'m just very experienced at this we go well that\\'s the least painful this is good right this works Yeah. Oh, my poor eyes. Um, actually, maybe I I I can\\'t quite see the room, but show of hands, like, who has used or implemented function calling in the past, and someone\\'s going to have to gauge this for me. Okay, it\\'s everyone. Great. I\\'m going to skip ahead a little. There\\'s like 10 20% 20% that hasn\\'t done it. Okay, 103%. Um the important part that you have to know is you can define the function um schema, right? Then the tool will specify what you want. And then in this case, you know, if I have like a get weather tool. Actually, I\\'m going to grab this from the docs as well. When in doubt, just go to the docs. You know, it\\'s always good. So, step one. No, this is all node. Yeah. And this will be useful for later too. Um, step one, call the function. Step two, execute your code. Right? So here what we\\'re doing is we\\'re taking we\\'re parsing out what the what the function told us. We\\'re parsing out the args and then we\\'re calling this get weather function which we don\\'t have yet. Um, but conveniently it\\'s up here, right? So we have it up here. get weather requests and then the last step is we provide the result back to the model. Um uh provide the result back to the model and then ask for a completion. Right? So just in order specify the tools, call it the first time, get the tool calls, parse them out, call the function, append and do that. Right? So if we do that and we just add maybe like add I\\'m going to do this by hand. Why not? Yeah. So I\\'ll print the completion here sort of by hand and then print the last completion. Then what we can see is we\\'ll get the first one that includes a tool call. It\\'ll call the actual temperature uh call the weather API and then we\\'ll get a response and then it says the current temperature in Paris is something I can\\'t see it because of Zoom. Um cool. So this is like the very very basic setup for a function calling. Let\\'s take a step forward. So this is an agent uh very very very basic implementation of an agent um that I\\'m going to go through really really quickly cuz we\\'re going to start using it. This is very familiar to what you\\'ll see in Swarm or any of the other like like basic frameworks. Um but the idea here is in uh as you can see like in the original one essentially like when it had a tool call I wanted to provide it back. So what I do here is while like just keep looping and this is like the very famous like agents are a loop. This is this is that loop. Um, specify the tools, call the model, get the message, print it out, handle the tool calls, append it, and once we have no more tool calls, break. This is the whole loop. I called it run full turn in my head. One turn is just like you let the model do everything. Um, and then we have this like execute tool call, yada yada, whatever. Um, so now we can use this. So agents py did I export it. There we go. I love it. Okay, so now we\\'re just going to specify one. Um, you know what? Yeah, we um and we\\'ll we\\'ll do it we\\'ll do it with a with a simple loop. So we have this that and we have this. Okay. So now we can just do Asians run full turn. The one other thing that I\\'m adding here that I didn\\'t show is um this simple utils uh that defines this very very very useful function. So functions to schema essentially takes in a raw python object function and then provides it into the uh like correct schema um so that you can just define functions directly and this is the same thing that we have in swarm and there\\'s an a few other frameworks now as well um so as an example we can do like you know get weather and I\\'m just going to like you know return 20 like degrees Celsius Right. I\\'ll do that here. Then I\\'ll have my messages and messages. Is this emergency reference? I think it is. What did I call it? Oh. Oh. Oh, I see. This is not part of the Asian class. Cool. So, it called the weather, printed it out, uh, gave us a nice little completion. Um, this is essentially what we want to see. If I say, um, and then I can just keep keep going, right? So, we we we have this basic um basic loop. Um, now let\\'s get back to the presentation real quick. That was like a very very immediate crash course. Now let\\'s let\\'s get interesting. Um and and just for convenience, I\\'m going to like pivot to the swarm implementation. Um the main reason being it\\'s pretty much the same. Um except we have like some convenient uh like looping tools. So uh swine imports agents and then There we go. So now we can do a simple agent and run a demo loop. And let\\'s see this work. Um we have this very basic setup. Let\\'s do everything now. So um another show of hands. How who here has implemented uh rag? Okay. What about memory? Okay. Fewer. And then what about like you know multi multi-step things and workflows? Cool. Um how about this? There\\'s a lot of stuff we could talk about, but I want to keep this valuable to you all. Out of this list, can you like just type in the Slack what you wanna what you want to see and I can just change the order in which we\\'ll cover this. Um because they\\'re pretty they\\'re pretty interchangeable. We can we can build up. Um but essentially there\\'s there\\'s more interesting things later on, but I want to make sure we we can build up to them. So, just uh if you can pull up the Slack and just dump in, you know, what you\\'re interested in seeing. Okay. Lots of memory. I see. Um function generation from the docs. Delegation async. Okay. build delegation and random cool stuff. I will get to the random cool stuff. Um, okie dokie. Back to light mode. Um, yeah, let\\'s start with like a very very basic form of memory, right? Um, how would we how do we do this? Let\\'s see. Honestly, like we can have um just a list, right? Memory can just be this list. And I\\'m going to implement this similar to how it\\'s done in chatbt. Um but ju just to show like I think the whole point of this talk is like this workshop is like doing things from first principles and just really removing the complexity. I think there\\'s like a lot of like not fake complexity but like added complexity on things that like doesn\\'t really need to be there a lot of the time. like concepts are a lot more simple and it\\'s all about function calling. So, uh we can do you know add like add to memory and I\\'ll append it and then get memory. Uh like this is super super simple. Um is this is this good enough? Let\\'s see. Maybe maybe. And so what we can do is give it the tools. Let\\'s see and then what when will we want to use them? Let\\'s say you like I can just say so this comment is going to be used as the uh string uh as the as the description in the function. So you can say like you know like when the user tells you something like factual about themselves their life or I can\\'t see anything okay or their preferences call this function um memory. Um and we\\'ll add a couple more cool things like expiration, which is one that I\\'ve kind of wanted to add for a while. So, you know, for now, let\\'s say false um memory.append uh think And then we have I guess what we call it uh you know memory text and we can say you know keep the memory text short size. Great. Cursor knows what I want. And then we can just return maybe like this is a super super super naive implementation. Um, but now, uh, when we start off, I guess we could even start off with [Music] a I mean, this is going to be kind of hacky, but I can just like um in your first turn always call get memory. This is not uh great. It\\'s just because of the demo loop. But maybe I\\'ll break I\\'ll break the demo loop out so we can actually do this by hand. But so how how could we prove this? Um maybe let\\'s say like you know write this memory bank or like keep this memory bank in a local file JSON file read it in at beginning and write it out at every Nice. Okay, I trust this. Shall we test it out? So, wait, someone uh Okay, we got the memory memory. Can any can anyone see any bugs? Because we\\'re about to test this out. So, we got the loop. It\\'s going to call it um and maybe like let\\'s memory. Okay. Sure. Yeah. Let\\'s try it. Let\\'s try it out. See what happens. So, hi. Did I not give it the functions? Is it hallucinating this? Oh yes, because I think we called it functions. Hi. There we go. Okay, so called get memory. There\\'s nothing there. I can just say like um I am 6 feet tall despite what people think. Uh, cool. Uh, so now let\\'s just check, right? It should have written this out. There we go. So now we have it in the memory bank. So now I can actually uh end this, right? And then be like, you know, uh, how tall am I? Tada. We\\'ve implemented memory, right? There\\'s a Yes, I\\'ll take a clap. I saw Louis Lewis, you\\'re like my proxy for the Luis uh Luis Costa, you\\'re my proxy for the audience. You\\'re like the only person I can really see. So, please don\\'t uh turn off your camera. Um, amazing. Now, we can do more interesting things, right? We can uh if we want do a little bit like smart querying uh where instead of just like loading in all of the memory, um we can like do a little bit of like retrieval uh to load in the right ones. um and use like semantic similarity or use some kind of search. Um I I could try to implement that. Um that might take a little bit longer but not that long. But I do want to pause here. See like given this and like this is going to be the style of things that we do like what um what do we want to see next? I can just keep going with this example. I can pivot. Uh it could be fun just to keep building on this. See how far we can get. Let\\'s see. uh delegation async have it chat and work in the background. We\\'ll get there. We\\'ll get there. I got that working this morning on Python because I didn\\'t want to switch to to Node yet. Uh delegation async. Okay. Let\\'s get into delegation then. So, there\\'s a few different ways we can do this. I\\'m actually going to leave the memory and we\\'re just going to keep building in this on this on this agent. Uh, and I am using the swarm agent just cuz I didn\\'t want to debug the one that I implemented. But like if we actually look at what this is, um, like it is very very very simple. And then the like run demo loop itself is like uh just printing out messages. Um, and what it\\'s doing is like appending client.run whatever. And like if we look at this client.run run. Um, if it doesn\\'t stream, it essentially does exactly what we did before, like keeps looping, get the completion, throw the messages in, uh, append them, handle responses, etc. There\\'s a couple more things around context and handoffs that we like don\\'t don\\'t really have to look at today. Um, but we can. So, cool, we have memory. Let\\'s do delegation. Um, so there\\'s a couple ways we can do this, right? Um, there\\'s if you think of like functions and agents and and everything you can, um, maybe I\\'ll skip to this slide. Skip skip skip skip. These are like a few of the forms of like agents on delegation that people might be familiar with. We have handoffs, which is like the swarm style. You take a conversation and fully swap it to a different agent. Um, and what that means is just like replacing the system prompt, replacing the tools. Um, you can have nested calls which are the easiest to implement and like often somewhat overlooked. Um, and then you can have manager tasks that\\'s more async. Uh, we will get to that today. So, let\\'s do a very basic one, right? Let\\'s say I want to do like um maybe let\\'s give it a chance to like call a bigger model to do a harder task, right? So we can say like you know uh I can delegate to smarter model and and I\\'m going to give like task description. [Laughter] Um so I\\'m laughing because this is saying like it\\'s just going to make it smarter by telling it to be smarter. That\\'s not despite how well that would work. Usually we\\'re not going to do that. So we\\'re we can just make an API request directly. Actually, let\\'s just do that. Let\\'s just do that. So um let\\'s do from OpenI client. And here we could do client chat completions create. Let\\'s call 01. Um I won\\'t provide a system message. I think that\\'s okay. content is the description. Look at that. And we did it. So now um man, I love cursor. Okay, did everyone catch all of that? By the way, all we did was like implement this this function that calls opening API and then here. So now I can say like uh so now I can add a bit of a description here. I\\'m like like uh if you know if the user has to like um I don\\'t know do something like that seems difficult or says it\\'s hard use this instead of try uh and it infers you know how to use this based on like the fact that I called it test description. It\\'s, you know, so let\\'s give this a shot. Hi. Um, let\\'s see. Uh, you know, give me a poem about, I don\\'t know, dogs and the earth where each other starts with the next letter. Yeah, it might just try it because I know GPD4 can do a variation of this, but um Oh, it\\'s giving it a shot. Okay. Let\\'s see if it gets it right. Okay. I don\\'t have uh I don\\'t have the patience. Let\\'s say answer briefly. one subkins max. Okay, great. And then to you know uh write a haik coup. There we go. It\\'s shorts where each other word starts with the next letter of the alphabet and ends with the previous letter of the alphabet. I don\\'t know. You guys want to try this while this does it? Is it around? Did I lose Did I lose something? Let\\'s see. Hi. Okay, you there? Cool. Um, did I not copy it? Oh no. Okay. Uh, give me a thank you. Now make sure each word starts with letter of the alphabet starting with a and each word ends with the previous letter. But this is hard. Okay. So it should be making the function call. Oh, I see. I see. I see. Okay. So I\\'m only printing the function call when it returns. So that\\'s probably why we\\'re waiting so long here. Um but this is actually a really good uh example of like okay we are doing um this like task delegation technically and like it is happening in the background and I\\'m going to give this a sec to figure this out. Um but like this is obviously a bad experience right like you don\\'t want to be waiting here. You essentially want to keep doing other stuff. So wow it\\'s still not still not back. We\\'ll we\\'ll we\\'ll we\\'ll let it figure it out. Um, so maybe let\\'s skip straight to async. Um, we\\'re actually got like both more and less time than I thought. So I don\\'t let this keep turning away. Um, now let\\'s think about this for a sec, right? If we want to do something async, it means that like what do we want to happen? There we go. So call the model da ya yada buzz breezy whispered. Is this correct? I don\\'t know. Wow. Yeah, this is sort of correct. What did it say? Note. Great. Great. So, it it did something, right? Um and it did something that 40 probably could not have, which is great. That\\'s delegation, but we were just sitting there waiting for it. Um so, let\\'s try doing this async. Now, I I I I actually want everyone to like kind of just stop and think like how would you implement this um in terms of like what behavior do you want? And maybe I want to see people like drop this in the Slack. Just like take a couple minutes um and like drop in the Slack like a proposal for how you would want to do this and then I might just pick one or we\\'ll like talk about them. Let\\'s call 03. Yeah. Not not yet. Not yet. Yes. Import async.io definitely feels like uh important. But I guess the questions are like you know when we delegate something uh obviously we want to have this happening in the background. Um, do we want like when it finishes, do we want to be like do we want it to be injected into the conversation? Do we want it to give us a response? Um, how many like do we want to be able to like interact with tasks that are running? Um, like do we want to be able to batch stuff? I am not setting up a Kafka cluster. I\\'m going to give people maybe a couple more couple more minutes to just like dump some ideas here. Um, batch calls. Yeah, I guess B. But how how would batch calls work? Maybe Stephania if you want to add some like detail there. Just keep working until it generates a stop word, right? Okay. Yeah, this is this is a good idea. JS and set timeout. Yeah, switching to JS is always a good option because you already have the event loop implemented. Smaller model. Smaller model. Okay, you guys are all suggesting some good ideas. Um, but we can actually go simpler, right? Like like essentially what we would want and I can implement like the basic interface of this, where\\'s my code? Um, is like instead of actually doing this, right, I can like return delegated, right? response pending. Um and then later I can say like you know check tasks or something. So this this pattern is actually one I I quite like a lot where you um you call a model it\\'s a function it\\'s non-blocking and then later you can you can check up on them. Now the thing is Python is like single threaded and uh async is real. Uh it\\'s just a little bit tricky. Um but let\\'s uh let\\'s give it a shot. So I do have some async emergency reference. Now here\\'s the here\\'s the um thing to notice. when something like for this to work correctly, we have this loop that asks for our input and blocks on our input um and like is displaying that on the screen and we don\\'t want to have that be displayed while it\\'s also like injecting messages. Um so we essentially want to separate out like where you give user input and where actually interesting stuff happens. So that\\'s what I\\'ve done in this in the basics of this async folder. So let\\'s take a look at that. Cool. Um, it looks complicated. It\\'s not that really that bad. So, um, I\\'m using websockets. Essentially, what I\\'m doing or sockets, sorry. Essentially, what I\\'m doing is, um, the I have a server and all it does is like has a handle user input function, uh, and a start message processor. Um, these aren\\'t super important. and we can take a look at them in a second, but essentially it just like waits uh on a specific like uh port and and host. Um and then I have a client that connects to that and it just does while true loops and lets me enter in. Right? So if we look at what that looks like, I have it here. Um so let\\'s go to async. Let me zoom in here a little bit. Also, um, while I do this, is there are there any questions that I\\'ve that I\\'ve missed? Um, and if there are, just feel free to shout them out. No. Great. Cool. Covered everything. Everyone is uh perfectly up to date with everything I\\'ve said. Okay. Um, sorry. Okay, so I have this and now I can say like, hi, let me show you the the rest of this. I guess that\\'s probably important. Uh, D. So we have the server, we have the client, we also have the a very very very basic agent imple implementation. It it looks just like the old one with the main exception that I\\'m using async openai instead. Um handling tool calls is happening in parallel. So for each tool call grab it create the task um await for them and then just like await for them all at the same time. Um, and this just lets it like create a bunch of tasks that are all going to run in parallel and once they\\'re done, like if they if they yield back at any point, we can keep doing other stuff. Um, by the way, if these are just like heavy functions that are doing heavy processing, it\\'s not going to matter. The fact that they\\'re in parallel like in sync, it means that it\\'s still going to happen one after the other. But if they\\'re like network calls to other models, then it\\'s perfect. Um, the runful turn is the same one. You like call a model, check to see if there\\'s any function calls, if not break, return the response. Um the only difference here is we\\'re awaiting uh the chat completion, right? This is the only difference. And then if we take a look at our agent handler, we\\'ve declared we\\'ve declared our agent like normal, right? Right now it has no instructions. It looks very familiar. And this loop is also pretty familiar. It\\'s like get the messages. Um here here\\'s the only difference, right? Um and you\\'ll see why this matters in a sec. I have a message Q. Uh and this is will be useful because we don\\'t want to process multiple messages in the same conversation at the same time. Like we want work to happen at the same time but we don\\'t want um like multiple generations to happen with the same history because then you\\'ll get conflicting like like if two messages if two functions return and they both need to be handled uh you essentially while you want them to happen in parallel the results should should only come in one after the other. So we have a cue for that. Uh and it treats user messages as well like that. So this is the handle user input that is being called from the server. Essentially just throws in a user input message into the message Q. Uh and all we\\'re doing here is like pulling off the queue, run it like you know uh put put the messages back in the in the message array uh and then like sleep, right? This is like just a very simple uh loop but it does it with async. So this is what we saw and and it feels pretty normal right. I can say like you know my name is Elan and I can say you know what is my name. Great. Um, now this still doesn\\'t exactly answer how we\\'re going to do things asynchronously, but it does give us the space to play with it because now things are happening async and we can do a few more delegations that that are actually async. So, let\\'s start with a, you know, simple blocking get weather function. I think I required all functions to be async here. Um, yeah. 67 and 70. Uh, and then skip that and let\\'s test it out. The only annoying part is we have to like restart two things now. So, hi. Uh, what\\'s the weather? Great. So we can see it called a function got the response. So far everything is normal. Now let\\'s do some delegation stuff. So let\\'s say let\\'s say we want to call this function uh three times for different places. So we have location [Music] um yeah and let\\'s say you know like pick a random number from 50 to 80 to return. Um, so if we do it in the previous case, do I still have the round? Yeah, maybe, maybe not. Okay. Uh so if we look at okay this is the non async one. Do we have weather? We don\\'t have weather. Let\\'s give it weather. Let\\'s give it the same weather function. Cool. [Music] Um we only need random. We don\\'t need this. And this not async. Okay. So now we have this weather function. Let\\'s test this out in the non- async case. Um I\\'m going to get rid of the rest for now. Um so if I say you know uh weather in SF cools it fantastic and now it\\'s like weather in SF New York uh you know and the five other cities. is it\\'s um it\\'s still going to do the parallel function calling and here it\\'s fine because they all return immediately, right? But now let\\'s add like an artificial weight. So let\\'s say time do sleep one, right? Um, cool. So, now what\\'s going to happen is we\\'re going to ask for that again, but it\\'s going to take so long. Weather in five random cities of your choosing. So, it\\'s going to take a while. And the reason is each of these is having to run um in like one after the other. There you go. It took like over five seconds. Now let\\'s do the same thing here. So the equivalent is um there\\'s async io sleep. Someone let me know if I\\'m doing this wrong. But um this should emulate like very very similar behavior. So if we instead run it here that\\'s and that\\'s I can say you know give me the weather of five random cities of York. Um we should see it return. Okay. Okay, so it called all of them and it got back because they all happened in parallel. Um, this is the magic of async.io, right? Um, anything that can be parallelized or I guess scheduled in a way where where it\\'s non-blocking. Uh, like sleeps, these sleeps are non-blocking. Um, essentially we can we can do and so you can imagine switching this sleep for like an actual API call. Um, so, uh, we can actually do that right now with 01, right? Like we could call 01 multiple times and they would all run in parallel. Now, the main problem is that we\\'re still going to be waiting back like waiting to to get all of them together, right? Like like the fact that it we can run them in parallel means that like we can have five 10-second tasks running in parallel. So it\\'ll take 10 seconds, but it\\'s still going to take 10 seconds where I can\\'t talk to the model. So instead, let\\'s have this notion of tasks. There\\'s many ways to do this. Um, I\\'m going to do a pattern that I quite like. So let\\'s do this. Um let\\'s define a you know create task. So I have a create task function that makes a task. Um no and I want to create like a task ID and I want to keep it short. Okay. So now what I\\'ve done is I have this function that makes a random task ID, sets it, creates it and then calls like let\\'s say get weather or something. Um and then it\\'s you know it\\'s suggesting this check tasks which is the next thing that I want to do. So the next one is check tasks. And so maybe I can just say like check task, you know, do for amazing. Um, okay. So let\\'s take a moment to to look at what just happened. We now create a task with a random ID. We give back the ID to the model and then later we can call check task. get that task, see what the status is, and see if it\\'s done. Um, so let\\'s like add, I don\\'t know, 5-second delay here. And right now, let\\'s just get weather. So description would be maybe create task for, you know, let\\'s see again this is all like sort of live coding, so we\\'ll see if it works. I can say like you know create a task um and set the description to the to be San Francisco. Let\\'s see what happens. Oh, I didn\\'t give it these functions. There we go. Um, let me just check. Cool, cool, cool, cool. Okay, let\\'s do it again. Say, you know, create a task where the location or the description is just. So now it creates the task and I can say hi. And look at that. It That\\'s not working correctly. Maybe it was Maybe it just took a while to respond. Um here, let\\'s add a longer delay to know for sure. 10 seconds. Let\\'s run it back. So create a task with description se uh cool. So I can say hello there. Cool. So it can still respond to me, right? Um check the task. So I can still interact with it and it can check to see if it\\'s not done. Um, I guess we need to keep checking to see if it\\'ll work. So, you know, how about or you know, tell me a joke. Who would have thought? Who would have thought? Um, cool. So, now like check them again. So, it called the task. It is done. And we have 78 and sunny. I will take a clap for that. I want to see everyone clap. Please clap. Thank you. Thank you. Thank you. Thank you. So, what you just saw was uh live asynchronous uh programming. Uh it\\'s very impressive. The models can also, you know, do pretty well. Anyway, here\\'s why this is interesting. We now have a system where we can give it tasks and it\\'ll ceue them up uh and then we can check on their progress. So, so already we have the basis for like a really really interesting thing, right? like this this thing here right right now it\\'s just get weather but if we say just like you know uh run what do it call like um you know call model and then no but we want AI port sync open AI get the client um I guess we Actually, we can just leverage the agents. They\\'re already async. Let\\'s not deal with that again. Um or whatever. Let\\'s do Let\\'s do it this way. I did all did it all. Fantastic. Okay. So, now we can switch get weather for call model. And let\\'s try this again. So, let\\'s give it the same task as before, which is, you know, hi, just make sure it\\'s running. um you know write me a uh ha coup about I don\\'t know uh a coup where the theme is so incredible it makes me cry make this a task okay so it\\'s created the task and now I can actually say you Now make a second one but pick a theme yourself. So now I have this interface where I can essentially keep chatting with this and it can essentially spin off additional tasks. Um you check check all tasks. Um let\\'s see what the progress is like. Look at that. So we have we have both. Oh, was this a one? That\\'s pretty fast. Um, but yeah, so now we have this system that you can actually just call call multiple ones. So I\\'m going to pause here just open up for questions for a little bit conversation like other directions we can take this other ways we could have done this. Um, but what I want to look at next is like how can we do this in a way where we don\\'t actually have to check um cuz right now like we don\\'t need these two terminals. This is just over complicated. Like I think the the the point of having the second terminal is so that we can push things to the conversation without a user having sent a message first. So I\\'m going to pause here. I\\'m going to field some questions and then we can get back to it. Okay. Um would you not use generators for nested tool calls? Um, Tom, do you want to I don\\'t know if there\\'s a mic in the room or like uh just get get it get it to them. Yeah, there is. Also, they can unmute themselves. Uh, who\\'s your Uh, sorry. So, is the one that was uh answering is So, in the room? Yeah. Can Can you hear him? No, I can\\'t hear him. Hello. Can you hear me in the Zoom? Yeah, I can hear you now. I can\\'t hear you anymore. I don\\'t know if you\\'re saying a question right now. No, I was just wondering if it\\'s possible to treat agents objects as a generator in Python. So you can nest like you can yield from individual agents and then yield those tool calls. Does that make sense? Yeah. Yeah, that makes sense. This is also very impressive that you have to do this. I think someone needs to mute themselves. I don\\'t know if it\\'s Alain or uh like someone in the room unmuted and uh put put this man through through hell. Um No, it\\'s me. It\\'s me. Oh, is your laptop unmuted? I can\\'t hear anyone anymore. Maybe just speak into your laptop. Okay, but I\\'ll I\\'ll answer your original question of like, you know, if you put um can you like essentially make agents into generators so that you can like yield the results as you go? The answer is yes. And this is actually like um the right way to do this. The way I\\'m doing it now is I\\'m only exposing the final response mostly because um implementing like the the generator gets a little bit tricky and I don\\'t want to have to like debug that. Not too much, but you can essentially just surface each of the steps, each of the function calls, each of the everything. um that would um that would let you essentially keep track of more agents at the same time and you could maybe have like a bit of a flat structure where you just have multiple agents going yielding events and then you can essentially see like which one\\'s coming from where. Um but I guess the thing is like there you have to deal with a bit of the complexity of um handling all the like like essentially listening to like all the different events and like uh associating them back to like a specific agent and like maybe there is an agent in the front, maybe there isn\\'t. Um here the idea is just I have one agent that I\\'m interacting with. I\\'m always just going to interact with like that one agent and it\\'s responsible for spinning off other other ones and like dealing with dealing with that as well. But you can absolutely do it the way that you\\'re saying. And like if you wanted to build this out like more fully, you probably would in order to be able to surface progress. Um, did that answer your question? I\\'m now scared to get people\\'s talking in the room, but I\\'ll just read them from Slack. Um, is there any is there any good design patterns to create projects with agents? There\\'s like a million. Um, you know, like I like prototyping with swarm. Um, I know paidantic AI is also like a really nice one. I think Sam\\'s there in the room or around. Um, there\\'s Yeah, I don\\'t want to like I don\\'t really use uh any myself mostly because like you saw it\\'s pretty simple to implement your own loop. So that\\'s what I end up doing. I think usually for every project I either write my own like it\\'s like I don\\'t know how many how many lines is this like like 70 lines and so like depending what you want you might want more you might want less and I\\'m sure there\\'s good solutions um but like I just have I could just copy this around um I don\\'t really like working with too many dependencies especially for something that\\'s so lightweight like I I want granular control um and like except for example for Swarm I ended up having to hop in here and like handling specific kinds of tool calls in certain ways so that I can do handoffs. Um, which we can actually look at and we can implement this without too much trouble. Um, but yeah, my answer is like there\\'s many you can choose from. I don\\'t have many I would recommend personally. Um, but I I am a I am a fan of of Pyantic AI. It\\'s pretty cool. I haven\\'t used it too much, but like the interface looks nice. Um, it reminds me a lot of Swarm. The one I use the most for prototyping is Swarm. I think it\\'s just cuz it\\'s the one that like I\\'m most familiar with. Um, when you have to start to approach dozens or hundreds of functions, what techniques should we apply in order to effectively tool call? There\\'s there\\'s a few answers there, right? Um, you can have multiple agents and essentially like split up the responsibilities or like the the groupings of the functions. Um and so into into yeah like clusters where you have like a set of related functions that are needed for specific tasks and then you can invoke the correct agent. Uh and this is where like multi- aent patterns start to make sense is like specifically when you have tons and tons of functions like how do you uh go to the right ones? If you for some reason need them all at the same time, uh you could try fine-tuning. Um in in projects for OpenAI, I\\'ve ended up fine-tuning up to like hundreds of fun like it was like 120 functions. This was with uh GPT3.5. So the fact that that works gives me like pretty high confidence like you can fine-tune uh smaller models with a lot of functions and get them to work pretty well. Um the last one is like some kind of dynamic function loading where based on the input or based on the conversation you load into memory or like you load into context um the most likely relevant functions. Um and there\\'s a few different ways to do this. You can do this with embeddings. You can do this with like um having like a two-step function call. At that point, you\\'re essentially having agents. Like if if you call a function to then load more functions, that\\'s what a handoff is essentially. So a lot of these start to look very similar. Um it\\'s just like how are you loading multiple different ones? Um okay, for visioning models, are there tools being called within the thought text? Um, so because we don\\'t expose the thoughts like the the chain of thought, um, that\\'s a little bit hard to answer for for for 01 right now in the API. The answer is no. Um, I\\'m trying to see how much I can say here. Um, it is something that is technically possible, right? Like you can do anything you want with with post training. Um, we do not currently allow you to call functions within the chain of thought. Um, so yeah, right now the function calls happen at the very end. Uh, do you have any good code examples for router patterns in these lots of functions cases? Yeah. So, so my like my answer there is some one of these. Like if you really just want to route um then like the the idea of having like multiple agents and handing off to one of them is actually really nice. Um like you can just define multiple agents each of them with like multiple functions. Um and then have the first one have like a we can actually do this quickly like we I\\'ll do this in swarm but like I said you can implement this yourself. Um I don\\'t actually know who else supports handoffs the same way Swarm does, but um essentially here let\\'s start a new file like uh routing. So see swarm ripple demo loop I can say like you know triage equals one and then I can have my other two. It\\'s like you know maybe I have like some collection of like um you can call them an agent but I can also just call them yeah like uh what it be like you know uh email functions agent and you can have like you know like send email check email I don\\'t know what else does cursor want me to write there. Um, cool. We have these two and then we can do like maybe we have the emails and then we have a calendar. I don\\'t know. So make like create event can do like calendar. I\\'m just going to say like you know finish what I\\'m doing update the prompts and the tools. I\\'m lazy. So let\\'s see what uh let\\'s see what cursor thinks. Great. So now we have an email functions agent and a calendar agent. Call them whatever you want. Um now like let\\'s pretend that instead of just having three functions, we have 30 and each one of these has 10 or 15. Um then like if you maybe give all 30 to one agent, first of all try it. Like if that works, amazing, right? you don\\'t have to deal with the extra complexity. Um so you don\\'t really want to do handoffs and stuff until you really really need to through like evals. Um but um yeah. So, so and then the special like kind of functions here is like you know uh transfer to email agent and I\\'ll return email agent and then cool. So now I have my two transfer functions. Oh. Oh, it did it. Okay, cool. So I think this should just work right. So I\\'ve defined like the actual functions and agents that in your case would be many many more functions. Um I\\'ve defined the transfer functions and I\\'ve given them to the triage agent. So now if I run this say hi um you know I want to send an email. Cool. So now I\\'m talking to the email agent. Now if you want to do like more you know like transfers uh this is not delegation. Okay. So trails assistance um maybe we can tell the assistance like you know if you already know what the user is asking just call that function right and this is if you want to have like a case where you know it still routes you but it\\'s like a faster two hop so maybe I can say like what are the functions here um what are the parameter There\\'s send email to subject body. So I can say like know send an email to bobgmail.com about taxes. What was the other one? Body um saying yo do your taxes. So it should transfer me to the email one. And then there we go. It immediately sends the email. So this felt like an immediate function call, but there was a transfer in the way. So this is kind of an example of like triaging does work. It\\'s really convenient to model it with agents and handoffs. Um I\\'d say it\\'s the primary use case for agents and handoffs is just a glorified triage through uh multiple functions. Um yeah, let\\'s see. Let\\'s go back to questions. [Music] Uhhuh. Consider having something like streamllet grad until you are you considering having something like streamlink radio util that allows porting all interaction functionality with one command. I don\\'t think so. I don\\'t know. Uh what did people answer? There we go. Sam\\'s in with pantic. So check that out. Um, how many tool calls can you get in one iteration? You like parallel function calls. I don\\'t think we have hard limits on either the number of functions or the number of parallel function calls. How big a tool library will the models perform well with? Super super general rule of thumb is like 10 to 20 you shouldn\\'t really pass over. Um, but like I said, I\\'ve gotten like this was a very specific case where like it was extremely latency sensitive and so like we had to have flat like flat function calling and we did 120 functions with GPT 3.5. So you can go pretty far um with fine-tuning, but like I\\'d say reliably without without extensive prompting. Yeah, probably like 10 to 20 like past that point. You really ask yourself like what are you trying to do? Like why are you putting so many functions? Is it super latency sensitive? Can you split it up? Like yeah. Um was there a follow-up here? Okay. Um rather than tool calls, I feel like we\\'re moving to generated code agents, will I soon be able to supply my tools functions? Ah, okay, that\\'s a good idea. We should try that. Um so essentially have something write its own function and then use it. Uh yeah, I feel like we can probably How would we do that? Yeah, we can try to we can we can find a way to do that. Uh I I\\'ll do that actually. Let\\'s do right now. Why not? Let\\'s do right now. Uh I\\'ve never done this before, but I feel like it shouldn\\'t be too hard. So um okay. Uh you know, what do we call this? Like bootstraps. Okay. So I\\'ll keep using swarm because I I will use um handoffs from swarm import agent. Cool. So now it\\'s always so let\\'s see we want an agent that writes it own functions. So agent was this one. Um we want it to write its own functions. So maybe we can have an hand off to itself. So we can do you know like refresh you know what to be refresh functions. So we can actually return the same agents. Um and then okay is it not declared? Is it unhappy? Uh yeah it\\'s not defined yet. Okay. Okay. So now we can define it down here. Okay. Um bootstrap bootstraps. Bootstraps. Okay. So we have this and we want it to write its own functions. So how are we going to do this? We wanted to produce Python. Does anyone have any ideas? uh if you want to shut them out like make just pass around the mic while I code just feel free to I can\\'t I can\\'t read while you\\'re uh what you write but we can we can try this. So, um let\\'s say we want it to, you know, um you know, add tool. There we go. And then let\\'s call this, you know, Pythonation. Okay. I\\'m going to do something very unsafe. So, you know what would it be? It\\'s like function obj equals eval of Python. Don\\'t do this. Don\\'t do this, kids. Um, and then will this work? Is this how Python works? Sam\\'s there. Uh, okay. So if I have this and I eval like you know def a does it um okay you know Um, so I guess it\\'s like uh I want to write a write a function that takes a string representing an implementation of a Python function and returns the actual rule Python function as interpreted. I don\\'t know. Let\\'s see what it does. Um, so essentially we want to do that once we have the function then we can just append it to the agent and then we might need to reload it. Uh, so we can then just return the agent and this might be it. This might be it. So add tool exec not eval. Why didn\\'t anyone say that? You guys can shout out. Okay. Okay. So it\\'s exec and then what is this 80? I don\\'t know. Someone save me. Like uh Hey, there\\'s a couple implementations you miss. Sorry. Check the flash. Did someone do this? Hey. Okay. It\\'s almost like someone fired. No, no, no. But this is not Okay. No, but this is not what I want, right? Like I don\\'t want it to run it in a subprocess. I wanted to I wanted to eval like evaluate the function and then turn it into a is that Oh, go up. Oh, Sam, of course. Amazing. Uh Jesus Christ. Okay. um make it. So the add tool function uh evaluates the implementation and adds it to the tools similar to now. Here\\'s a great reference for how to Let\\'s And then we want to grab Was this a good idea? This is a terrible idea. I hope people are enjoying this. Um, okay. Let\\'s see. Um, and let me just read this because it might not be that complicated to add in. So, I can\\'t. Okay, it got in the way. No, not Zoom. Where\\'s Slack? Was this it? Is this it? Okay, I\\'m going to I\\'m going to put this on hold for now. It sort of ignored your implementation, Sam. I\\'m sorry. Um, let\\'s see. So if you have parse function, can I just do this? Does this work? Uh, okay. Let\\'s see. I\\'ve first time for everything. Uh, wrong one. Sub bootstraps. Hi. Um, add a tool that prints hello when called it. Oh, but no, no, no, no. Make it print it, not just return it. My heart\\'s pounding. Okay. Um, so say hello to call it. Look at that. Look at that. Okay. So, now we have Yes, I will take a clap for that. We did this together, guys. Um, so this is actually a lot less code than I was expecting, but now we have a system. Look at that. It\\'s tiny. Um, that can write its own tools. Um, and so like maybe we can do something like, you know, um, what is, you know, like make yourself a little calculator. I can\\'t believe this just worked. Uh, you know what is 2 * 3 * Can someone check this? Uh, wait, wait, wait. 34698. 34698. This is This is crazy. This is This is so fun. Uh, yeah. I hadn\\'t done this before. And this is a lot less code than I thought. Look at this. Look, this is all you need. I mean, it\\'s this is super dangerous code. Like, this this is not good. Don\\'t do this. But it\\'s fun. Uh, I would put this squarely in fun things. We can we can now transition to looking at other fun things. Um they\\'re definitely not as fun as this one, I think. Um they\\'re just a couple of like random things that I found related to real time since I did sort of put it in this uh title. So whatever. Okay, these are the two main tricks that I that I kind of thought were pretty cool and you might have already seen them. Um, one is if you\\'ve ever dealt with like the real time API um like jumping in uh before you\\'re done with an idea or like done talking or something. Um like what I was thinking is like if if really what you want is you wanted to like use the model\\'s own intelligence to decide if you\\'re done talking or not. Um and you can treat our VAD like our voice detection as like a trigger happy version of that. It\\'s like a it\\'ll always tell you when maybe the user is ready to stop talking. But if you want the model to check, you can have a stay silent function or something else that essentially handles the other side of that where it\\'s like, okay, we\\'re definitely going to let you know whenever the user might be done talking, but then you can actually verify with a function call. And so this implementation is like super super simple. You literally just give you give the the function itself to the real time API and tell it to call it when the user is not quite done talking. And I\\'m not going to try to demo this live. Like real time demos are tough. Audio is tough. All that fun stuff\\'s tough. But um it works like surprisingly well. Uh if you just describe I I can you can probably find this tweet but it has the full prompt but essentially it\\'s like you can you can like pause you can say like you know I\\'ve been thinking about and then like stop and like it\\'ll get triggered call stay silent and then you can keep talking. Um so pretty pretty cool uh useful thing. Um and then the other one is also related to real time API. And if again if you if you uh are following me you might have already seen this but um someone at Devday just like came up to me and asked me like hey um is there a way to like make the real time API talk in like a specific way. Uh and I was like I don\\'t know let\\'s try it. And so like we had a demo booth and I sort of pushed someone out of the demo booth and grabbed the laptop and I was like let\\'s just like try try this stuff. And apparently if you ask the model to just you give it like a a script and you\\'re just like, \"Hey, read this out according to these XML tags um you can have it follow them here. Let me let me find um the original the original one.\" Yada yada yada. Sorry, this is I\\'m not trying to show you the whole timeline. Where is it? Okay, this is the stay silent one. Um, where is it? Where is it? There we go. Um, uh, it\\'s impossible that you\\'ll hear this, right? You guys don\\'t hear this. Yes. No. Uh, we don\\'t hear it, but you can hear your computer audio. Can I do the thing? I\\'m I\\'m I\\'m actually I\\'m not going to try. That\\'s fine. Um, wait. Can I same? No, no, no. It\\'s fine. Whatever. Anyway, um, yeah. So, if if you if you like give it like a script like this, um, we didn\\'t actually train it for this, right? Like this is just a really nice consequence of like behavior. It\\'s technically not function calling. I\\'m realizing now, but it\\'s like it\\'s very function callingesque. It\\'s real time. The title of this talk does include real time. So, this is uh this is it. Um I will pause here. I have so much more I can go through, but I want to give you guys a chance to like ask questions, poke around with some of these ideas. Uh Swarm is public. You can just try it out. Um we can also just try creating other functions. Uh this this is one of the this is easily one of the coolest like little programs I\\'ve ever made. Um, so yeah, let let me see if there\\'s any other questions of the Slack. Uh, it\\'s a room full of people auto completing make. Yeah, yeah, yeah. Okay, let\\'s not do that. Uh, okay. Revisiting memory. Let\\'s see. Do you have any suggestions for trying to enforce consistency of stored memories? Identifying inconsistencies and figure out how to resolve them. Okay. And then the second part is what data structures do you suggest for more structured memory helping enable more meaningful comparison of objects? I think I mean this question opens like you start to go down a path where you can get as complex as you would like, right? Like like you can start simple and you can end up with like an entire operating system to manage memory, right? There\\'s like the whole range. Um, one way to do off the top of my head like one way I can think to do this is um when you are about to store memory do a retrieval like do search to find similar memories or like memories that like are semantically similar um and then do an explicit check uh with a model to see if like you know one is like updating or contradicting another. Right? An example of this is like you know what is the latest state of a project right you know if someone\\'s like is the project like at some point you say like I\\'m working on the project like it\\'s not ready yet and it saves that and then later you\\'re like you know I um how like the project\\'s done now right like those are two contradicting memories um what you can do is essentially uh have a time stamp for them but also when you are about to store the second one or any memory check for similar ones and create like a direct like essentially um like node uh pointing from the original memory to the new one. And so I guess I guess what I\\'m thinking is like that way if anybody asks and you surface both like you can keep both in memory and when you do like retrieval and semantic similarity you can surface both but essentially you can present like the whole chain of updates and so you can just present the last one if you want or you can present the whole chain and the model have has some idea of like you know maybe if you ask it like um you know how long is this project delayed because that was the last you heard about it. But somebody at some point said, \"Oh, it\\'s actually done.\" Um, like if it raises both and has one with a later date, you know, maybe. But if you like if you\\'ve already made this chain explicitly, um, like you can actually represent that and you can choose to not show the previous ones. So that\\'s one idea. Like I said, there\\'s so many you can like so many ways to do this. Uh, and and like as soon as I stop talking, you guys walk out. Someone\\'s gonna like be like, \"Oh, there\\'s actually this like much easier way that like this idiot didn\\'t think of.\" So, anything you think of probably works. Um, here\\'s a real time API prompt. Yeah. Cool. Uh, any other any other questions about anything? Um, it\\'s been like an hour and 40 minutes, so happy to like keep going with random content. Um, there\\'s a couple demos that I have that are like demoing some public repos that we have that you can use. Um, yeah, but I want to I want to give anyone a chance to like speak up, say something if you want, ask questions. Um, I I don\\'t see I don\\'t see a lot happening in I I can\\'t You guys are tiny on my screen. Uh, cool. Okay, then I\\'ll pose this to you. Sorry, what? Oh, no. Go ahead. It sounds like you\\'re wrapping up. Yeah, the the last thing we can do, I guess, is like either I can um try and pull up a real time uh demo that shows how to do the 01 stuff uh in a in a slightly easier way. Okay, cool. People want to do that demo. Okay, let\\'s do it. So, um the reason I\\'m a little iffy about it is because yeah, it\\'s real time. Uh and those demos are tough to do publicly but um there is this repo OpenAI um real time Twilio demo um that essentially sets up your whole um like phone calling assistant uh and it\\'s just works right out of the box. It walks you through the steps. Um I may end up sharing a phone publicly. don\\'t call it during this demo and don\\'t share it and I will delete it. But um yeah, I trust you guys. So let\\'s see. Okay, so it looks like this is already running. Oh, I do have it already running. Okay. Um Oh, never mind. I I did uh I did make it all dots. But yeah, essentially this um this little checklist is like live updates. So as you set up the account, as you set up the phone number, as you set up your local web server, etc., it\\'ll like get checked. So setting up Twilio and Grock and everything has been one of the most annoying things I\\'ve had to do multiple times um related to the real time API. So this hopefully makes that process like a lot a lot easier. You can do most of it directly from here and it lets you know like when things are done. So you don\\'t actually have to do a lot from the Twe console at all. Um anyway, so um I let me show you the back end. So there is a handler function. There we go. Function handlers. So um you can either implement tools uh locally as a JSON schema and we can we can try that really quick. So this is the real time playground and I can say like you know make a function to you know get the answer to answers to everything no params maybe one param. Okay, so it gave it gave us this nice little uh nice little function and we can just paste it in here, save it, and cool. So now we have this get universe answers and I can save this configuration and call the phone number. So, let\\'s see. Hello. Hello. Okay, this is why I love them. Let me let me try once more. Okay, let\\'s go tool. Paste this. Save changes. Save config. [Music] Um, hello. Hello. I don\\'t think it\\'s happy. Is someone else? No, I didn\\'t show the number. Hello. Okay, it\\'s not working. That\\'s fine. Sort of expected it. Um, essentially what you can do like when you when you use this, um, it\\'s cool because these tools you can either specify local ones with schema and they appear here or you can define them in the back end. Um, so if you define them in the back end, you can actually give like code to execute. Um, if not, they\\'ll just show up here. Uh, and you can enter in like what the response will be uh, like a mock response. Um, but you can also like set up backend functions that will actually get handled in the back end. Uh the sample one is like a real implemented get weather one but as you can see there\\'s like it\\'s pretty straightforward to implement like the 01 one. Now the main difference between the real time API here and like what we were doing before is the real time API does actually allow asynchronous functions natively. So you can the model can call a function um get no response and you can keep talking with the model um until a response is back. So that that is behavior that like we specifically had to teach the real-time models because unlike a chat conversation, you can\\'t really enforce um like you can\\'t really halt the whole conversation until the functioning response comes back. So, we had to do that for for for launch um because originally when we hadn\\'t done that, you know, we hadn\\'t ever shown it how to do asynchronous functions and so it just couldn\\'t couldn\\'t handle them correctly. Anyway, sadly, uh demo didn\\'t quite work. Maybe I can try this one more time. We\\'ll see. Third time\\'s a charm. Hello. Hello. No. Okay, great. Anyway, um I believe that is most of what I wanted to cover today. Um, I\\'m going to play with this hyper unsafe function making agent a bit, but um, yeah, thank you all for coming. I\\'ll hang around for questions for a bit if anyone has any. Otherwise, I think we\\'re ending a little bit early. Um, like I said, yeah, mo most of you can can hang out or or leave as you please. Um, but I\\'ll maybe I\\'ll call it wrapped up here. Um, maybe what we can do is if you\\'re interested, you can stay and like hack on some of the stuff that you saw. I\\'m happy to hang around uh and answer questions as well. So, thank you all for coming. Uh, hope you hope you got something out of this. Um, the new uh to help in sports. Uh otherwise, I guess you can share. I\\'m sharing a little bit in case people have questions. I\\'m actually going to dump this, but this may be a really bad idea, but um here you go. Here\\'s the code for the super unsafe self like tool writing agent. And I\\'m probably going to stop sharing my screen. Um, yeah, I can I can I can share the repo and the slides. There\\'s not a lot on the slides, but yeah, I\\'m happy to happy to share them after.'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_youtube_transcript(youtube_url: str):\n",
    "    \"\"\"\n",
    "    Tool function to fetch the transcript of a YouTube video.\n",
    "    Input: YouTube video URL string.\n",
    "    Process: Extracts video ID, calls YouTubeTranscriptApi, and formats the output.\n",
    "    Output: Dictionary containing status ('success' or 'error') and either\n",
    "            the 'transcript' or an error 'message'.\n",
    "    \"\"\"\n",
    "    import re # Moved import inside\n",
    "    from youtube_transcript_api import YouTubeTranscriptApi # Moved import inside\n",
    "    from typing import Dict, Any # Keep for return type annotation\n",
    "\n",
    "    logger.info(f\"Tool 'get_youtube_transcript' called for URL: {youtube_url}\")\n",
    "\n",
    "    try:\n",
    "        # --- Start of merged fetching logic ---\n",
    "        video_id_match = re.search(r'(?:v=|/|embed/|shorts/)([0-9A-Za-z_-]{11})', youtube_url)\n",
    "        if not video_id_match:\n",
    "            logger.warning(f\"Could not extract video ID from URL: {youtube_url}\")\n",
    "            response: Dict[str, Any] = {\"status\": \"error\", \"message\": \"Could not extract video ID from URL.\"}\n",
    "            return response\n",
    "\n",
    "        video_id = video_id_match.group(1)\n",
    "\n",
    "        # Try fetching the transcript\n",
    "        try:\n",
    "            transcript_list = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "            transcript_text = ' '.join([entry['text'] for entry in transcript_list])\n",
    "            logger.info(f\"Successfully fetched transcript for {youtube_url}\")\n",
    "            response: Dict[str, Any] = {\"status\": \"success\", \"transcript\": transcript_text}\n",
    "            return response\n",
    "        except Exception as fetch_error:\n",
    "            # Log specific exceptions from the API call\n",
    "            logger.warning(f\"Could not fetch transcript for {youtube_url} (ID: {video_id}): {fetch_error}\")\n",
    "            response: Dict[str, Any] = {\"status\": \"error\", \"message\": f\"Could not retrieve transcript: {fetch_error}\"}\n",
    "            return response\n",
    "        # --- End of merged fetching logic ---\n",
    "\n",
    "    except Exception as e:\n",
    "        # Catch unexpected errors during the tool's execution (e.g., regex error?)\n",
    "        logger.error(f\"Unexpected error in get_youtube_transcript tool for {youtube_url}: {e}\", exc_info=True)\n",
    "        response: Dict[str, Any] = {\"status\": \"error\", \"message\": f\"Failed to execute transcript tool due to an unexpected error: {str(e)}\"}\n",
    "        return response \n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "result = get_youtube_transcript(\"https://www.youtube.com/watch?v=KUEmEb71vzQ&t=5522s\")\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'transcript': 'Cool. Okay, let\\'s get on with it. So, hi everyone. My name is Elan. I\\'m on the developer experience team at OpenAI. Um, unfortunately, I can\\'t be there in person as much as I would love to. I\\'m in a wedding in Costa Rica um which is happening later today. So, I just wanted to take this opportunity to just talk through um one of my favorite concepts in maybe all of like AI and language models. So, uh title of this talk is function calling is all you need. It\\'s a talk workshop. There\\'s going to be um a lot of coding. Please save your questions. No, I\\'m kidding. Like just interrupt at any point. Uh we have a Slack. Send them here. Um yeah, just send them at any point. Uh and if you want to like unmute yourself and or raise your hand, I\\'ll call you off like as we go. The idea is to keep this super super dynamic. Um since we have a bit of time, um I\\'ll be fielding a lot of like questions and requests and trying to be coding as much as possible. So yeah, this talk is going to need some lecturing uh a lot of coding from scratch and then some debugging hopefully not a lot. Uh so this is a little bit of what the uh workshop is going to look like. We\\'re going to go over a little brief history of the tool formers of of I\\'m sorry of function calling. Um then do a little crash course on function calling. Um talk about just agents how they\\'re just loops. How rag workflows and more are just function calls delegation and asynchrony. couple random things I found and then we\\'re going to do a Q&A. Um, if you just want to see like the meat of it, this is pretty much it. We\\'re going to do like this is everything I want to talk about uh and everything that we\\'re going to implement. So, this is uh just a little bit upfront. Great. So, a little history. Um, if we look at the abstractions and sort of patterns that we managed to do with language models, it started as text completion, right? like the original GPT uh and the GPT2 and the GPT3 were all just base models where you gave them some input text. Um and then they would just continue the sentence. Um this was at the time really really like interesting. This was the first time we were able to do like very like uh English sounding like real sounding language. Um but getting it to follow instructions was pretty hard. So if any of you were testing this back then you might remember how setting up a chatbot was um non-trivial, right? you had to like get it to answer questions. But if you just say like, you know, what is the best way to uh get to the like park or something, it would continue like like what is the best way to get to the park? Um that is what Sally said yesterday, right? And you wouldn\\'t actually get a a response. Um so you had to like structure it in a way where you would say like this is the question, this is the answer question answer few shot and then give it. Um then they introduced uh I think this was actually us. We introduced uh function um instruction following with uh instruct GPT. Now you could give it some input uh and it would actually do what you\\'re asking as opposed to just completing. Um finally we started to introduce this notion of like users and assistants and roles. Um and this was all done through through post training where you actually gain these personas. Um and then finally uh we we eventually landed on this like you can give it additional tools um in order to do like external um any like interact with external states. So this is what the previous playground used to look like. But as you can see there\\'s no chat. This is just like a window and it\\'ll complete. Now looking over at like the original papers which is pretty interesting. Um, one of the first times that we actually started to do this like function calling was through this web GPT um, which was this uh, version of GPT3 that we trained to be able to um, Elan, I have to cut in here a little bit. We got the Zoom working. Yay. Sweet. Um, all right. We\\'re going to cut over in the audio. So, can everyone hear Elon when he speaks? Elon, say something. Hello. Hi, everyone. Hello. Hello. How\\'s it going? Okay. So, so we\\'re going to cut over a little bit in terms of like um people having their own personal audio situation going on. So, you can mute your machine. You should be able to hear it on the room uh audio, whatever you want to do, but you can also obviously connect. But, uh yeah, now we have him on the big screen. So, okay. Yeah. Uh let\\'s let\\'s keep going for a bit. I I\\'ll cut in again if there\\'s audio issues or maybe you want to say some test words. Uh, you\\'re all great. Thank you for coming. Can we bump up the audio? Where\\'s the guy? [Laughter] It\\'s not a conference if you don\\'t have AV issues. Okay. Uh, it\\'s it\\'s still too soft. We can\\'t we can\\'t hear you. Um, I\\'m going to try and bump up the audio. I\\'m so sorry. I\\'ll put up a more interesting slide in the meantime. You guys can look at this while we figure this out. Where\\'s Where\\'s the video? Hey. Um, so anybody got uh good jokes? Maybe I always find a joke. It\\'s always the same one. I think it\\'s uh What is it? Why did the No, I had it. I had it here. Wait, wait, wait, wait, wait, wait. There we go. Why can\\'t you trust an atom? Yeah, because they make up everything. These are all real, by the way. And for for those who like have a keen eye, the first one is like actually from the GPT2 paper. Um we like gave the model this like description of like unicorns in the Andes Mountains. And that was like the big like first time that it was like doing multi- paragraph completion like continuations that referenced earlier parts of the conversation. It\\'s cool stuff. It\\'s cool history. Can you guys hear me? I feel like some people are having a great time. Some people don\\'t know what\\'s going on. Can like I don\\'t know. Am I good to keep going or should I keep waiting? Uh I think I think you\\'re good. Okay, great. Great. I do think this is the vibe of the whole talk by the way. There\\'s it doesn\\'t get more structured from here. Um okay, so as I was saying, uh we did like this web GPT paper. Um, essentially we trained a GPT3 version of the model uh or like a GP3 model to be able to use this like this very specific set of functions um to do web search. And this was like back in 2021. So really we had like webg a long time ago. Um but this is like one of the first times or maybe the first time that like we were having like it\\'s not just generating text but it\\'s generating actions and then we\\'re parsing those actions and then introducing it back into context so it can use the responses itself. Um, and how we trained it there, um, there\\'s like these these like, you know, clever ways of like we we essentially gave people, uh, an interface and let them do the searching. Um, and then I think we took it like Reddit um, yeah, explain like five. Um, and then just like had people complete tasks and they could use these commands. And so we taught the model and this was GPT3, right? Um, how to essentially imitate users behavior and then produce responses that were preferred. Um and and this was pretty cool like this is how you we like start to saw um to see like this uh this use of like structured like actions essentially. So but this was very specific right we were training like very specific um tools. So then you might be familiar with this paper. This was from Meta at the time. Um where they essentially had a way to teach the models how to use any tools. Um and they taught like they they used a few tools. I think it was like QA calculator. Um what was this like translation? Um couple other couple other tools. But um it was actually a pretty clever way where they like looked at the log props at each spot to like see where it was like best suited to like retroactively put in a function call given some like completion. Um so here we can see a few examples like um essentially it\\'s like if you have a calculator call if you insert a calculator call that like you know it\\'ll it\\'ll insert the um the actual call to the calculator which then you know you\\'re pretty familiar at this point will get the answer if you insert it in the right spot in the sentence it actually reduces the perplexity of the sentence. Um, and so they didn\\'t actually have a lot of human-labeled examples or I think it was just a few, but it was uh really cool because it was this way of like um it could learn to use uh any of these tools through this like crazy like log props technique. Um and it was uh I was pretty excited when I saw this paper. Now this is like how it learns to use any of these tools. Um, but then finally, uh, in June of 2023, uh, OpenAI launched just general function calling where we essentially like pre-trained it to be able to use these tools or actually post-trained it to be able to use, uh, tools. So now you don\\'t actually have to like give it like you can give it examples, but uh, we just showed it like this like syntax in with functions that we still use today and it\\'s just able to call functions. So this is a brief history of of function calling. Um, and my I guess my argument is this is really most of what you need for all the exciting stuff that\\'s happening today. Um, there\\'s there\\'s obviously like additional like systems you can use and like um more post training you can do, but fundamentally like functions are so so so powerful and we\\'re going to look at a few cases today. Um, I\\'m going to try to keep an eye on questions, but um yeah. Okay, I think you\\'re good so far. Cool. Cool. So, let\\'s do a super quick crash course on function calling. Uh, two main purposes and I\\'m ripping a lot of this from the docs. So, um, there the fetching data, right? Reading APIs, retrieval, memory, or taking action, any APIs you can use to write, managing application state, which is actually pretty overloaded. That can be like UI, front end, backend, whatever you want. And then workflow actions, which is um any like multi-step processes or even like meta actions like switching its own prompt or like loading in different tools or like handing off a conversation, right? Um so this is also a diagram straight from the docs, but um I\\'ll quickly brush past this. I\\'m going to assume like most people have at least seen this, but here it is. It\\'s uh you you essentially tell the model which functions you you want it to be able to use. Um and you also provide like whatever the user input is. Um the f the model tells you what it wants to do with that function but it doesn\\'t actually do it. This is one of the like a big sticking point function calling. It doesn\\'t actually use the function itself. It tells you like it tells you the intent of what it wants to do with a function. You are then responsible for parsing that, executing the code, doing whatever you want with it and then providing the result back to the model and then the model can use that respon like result in uh in the generation. Uh take a look at a quick question here. Oh no, it\\'s just it\\'s just swings. Okay, cool. Um these are just a few uh best practices. This is all taken from the docs as well. Um you want to write clear functions. An important one is you got to apply software engineering best practices when you write these functions. So, um, you know what? Maybe I\\'ll pull up the docs for this. Um, so there\\'s a lot of big text, but essentially, um, this is a lot of value here. I tried to pack as much like useful information here as I could, so I\\'m going to quickly go over it, right? Um, you had to explain the purpose of each parameter, use a system prompt, and include examples. That\\'s, you know, pretty pretty like not non-controversial. Um, software engineering best practices is a little bit uh more interesting, right? You got to make functions obviously intuitive and they got to follow the function of lease principle. Um, like if you give this to a person and they don\\'t know how to use it, then the model might not either, right? Models are getting smarter than us, but still you got to make it uh got to make it easy. Um, also you got to use enums and object structure to make sure that like you are not letting the model make invalid calls, right? Um like here you you have this like toggle light uh um function that takes in like two two boolean params and like obviously like this is pretty wrong but um there\\'s actually many many more like subtle cases where um like the you can like you\\'re letting it represent invalid states. Um okay, still no questions. It\\'s Swixs. Okay. Um, great. And then there\\'s Sorry, there are a couple questions in the professor convers. I see. I see. Um, here maybe let me see if I can pull up the Slack. Okie dokie. Between functions and tools in my opinion. Hey Sam. Yes. Um I I think we\\'ve all been kind of like gravitating toward functions and tools as like the two main ways. Originally it was just functions. Tools was like later we renamed it. Um I think now uh and the way that I tried to specify in the docs is functions are like the the raw function calling right like you provide an interface and you\\'re responsible for executing the code. um tools and this is sort of how we treat it in our API is a superset of functions. Tools include functions but it also includes things like um code interpreter or file search or any of these like um like hosted solutions. I\\'d say this is not be like end all beall definition but this is a definition that we\\'ve adopted right there\\'s like tools which are like hosted tools uh including functions and then functions as a subset. Cool. Maybe we can just jump into it. Um, if people have questions on this, happy to happy to feel them as well. But I kind of want to get coding. Okay. Uh, when you start to approach dozens or hundreds of functions, what technically should we apply in order to effectively tool call permissions is one technique. Okay, interesting. And then question two, when you require one tool to provide inputs to another, I have seen tools become layered. How should a reasoner hardcode? Great. Okay, these are actually great questions. And um I think to to answer them, I might I might like do a little hack and like uh use some existing uh code. I\\'m going to use swarm for a little bit of this um because it does some nice function calling. Actually, no, you know what? I\\'m going to get to these fun uh in a second. Um let\\'s just go straight into the uh and as always, we should start from the docs. Uh, cool. Uh, so this is where I always step in. Can you zoom in on screen every single page? Yeah. Yeah. Bigger. Yeah. Yeah. Sorry. I always do. Yeah. Yeah. No, this is a good call out. Uh and then I think the terminal here should be good. Okay, sweet. Uh we have we have a function. Great. Um and then just run it and we have uh the function call. Right now we\\'re not handling it yet. So, this is where like I might skip ahead a little bit um and start doing some of the like agentic stuff. But first, uh first off, uh we we got to have a right like you know you you I can sort of hear myself. Um I think so. There we go. Perfect. Okay. So um I think the idea here is like let\\'s make a very very very simple like uh input loop like let\\'s do uh while you also light mode light mode Jesus okay sure I\\'m just very experienced at this we go well that\\'s the least painful this is good right this works Yeah. Oh, my poor eyes. Um, actually, maybe I I I can\\'t quite see the room, but show of hands, like, who has used or implemented function calling in the past, and someone\\'s going to have to gauge this for me. Okay, it\\'s everyone. Great. I\\'m going to skip ahead a little. There\\'s like 10 20% 20% that hasn\\'t done it. Okay, 103%. Um the important part that you have to know is you can define the function um schema, right? Then the tool will specify what you want. And then in this case, you know, if I have like a get weather tool. Actually, I\\'m going to grab this from the docs as well. When in doubt, just go to the docs. You know, it\\'s always good. So, step one. No, this is all node. Yeah. And this will be useful for later too. Um, step one, call the function. Step two, execute your code. Right? So here what we\\'re doing is we\\'re taking we\\'re parsing out what the what the function told us. We\\'re parsing out the args and then we\\'re calling this get weather function which we don\\'t have yet. Um, but conveniently it\\'s up here, right? So we have it up here. get weather requests and then the last step is we provide the result back to the model. Um uh provide the result back to the model and then ask for a completion. Right? So just in order specify the tools, call it the first time, get the tool calls, parse them out, call the function, append and do that. Right? So if we do that and we just add maybe like add I\\'m going to do this by hand. Why not? Yeah. So I\\'ll print the completion here sort of by hand and then print the last completion. Then what we can see is we\\'ll get the first one that includes a tool call. It\\'ll call the actual temperature uh call the weather API and then we\\'ll get a response and then it says the current temperature in Paris is something I can\\'t see it because of Zoom. Um cool. So this is like the very very basic setup for a function calling. Let\\'s take a step forward. So this is an agent uh very very very basic implementation of an agent um that I\\'m going to go through really really quickly cuz we\\'re going to start using it. This is very familiar to what you\\'ll see in Swarm or any of the other like like basic frameworks. Um but the idea here is in uh as you can see like in the original one essentially like when it had a tool call I wanted to provide it back. So what I do here is while like just keep looping and this is like the very famous like agents are a loop. This is this is that loop. Um, specify the tools, call the model, get the message, print it out, handle the tool calls, append it, and once we have no more tool calls, break. This is the whole loop. I called it run full turn in my head. One turn is just like you let the model do everything. Um, and then we have this like execute tool call, yada yada, whatever. Um, so now we can use this. So agents py did I export it. There we go. I love it. Okay, so now we\\'re just going to specify one. Um, you know what? Yeah, we um and we\\'ll we\\'ll do it we\\'ll do it with a with a simple loop. So we have this that and we have this. Okay. So now we can just do Asians run full turn. The one other thing that I\\'m adding here that I didn\\'t show is um this simple utils uh that defines this very very very useful function. So functions to schema essentially takes in a raw python object function and then provides it into the uh like correct schema um so that you can just define functions directly and this is the same thing that we have in swarm and there\\'s an a few other frameworks now as well um so as an example we can do like you know get weather and I\\'m just going to like you know return 20 like degrees Celsius Right. I\\'ll do that here. Then I\\'ll have my messages and messages. Is this emergency reference? I think it is. What did I call it? Oh. Oh. Oh, I see. This is not part of the Asian class. Cool. So, it called the weather, printed it out, uh, gave us a nice little completion. Um, this is essentially what we want to see. If I say, um, and then I can just keep keep going, right? So, we we we have this basic um basic loop. Um, now let\\'s get back to the presentation real quick. That was like a very very immediate crash course. Now let\\'s let\\'s get interesting. Um and and just for convenience, I\\'m going to like pivot to the swarm implementation. Um the main reason being it\\'s pretty much the same. Um except we have like some convenient uh like looping tools. So uh swine imports agents and then There we go. So now we can do a simple agent and run a demo loop. And let\\'s see this work. Um we have this very basic setup. Let\\'s do everything now. So um another show of hands. How who here has implemented uh rag? Okay. What about memory? Okay. Fewer. And then what about like you know multi multi-step things and workflows? Cool. Um how about this? There\\'s a lot of stuff we could talk about, but I want to keep this valuable to you all. Out of this list, can you like just type in the Slack what you wanna what you want to see and I can just change the order in which we\\'ll cover this. Um because they\\'re pretty they\\'re pretty interchangeable. We can we can build up. Um but essentially there\\'s there\\'s more interesting things later on, but I want to make sure we we can build up to them. So, just uh if you can pull up the Slack and just dump in, you know, what you\\'re interested in seeing. Okay. Lots of memory. I see. Um function generation from the docs. Delegation async. Okay. build delegation and random cool stuff. I will get to the random cool stuff. Um, okie dokie. Back to light mode. Um, yeah, let\\'s start with like a very very basic form of memory, right? Um, how would we how do we do this? Let\\'s see. Honestly, like we can have um just a list, right? Memory can just be this list. And I\\'m going to implement this similar to how it\\'s done in chatbt. Um but ju just to show like I think the whole point of this talk is like this workshop is like doing things from first principles and just really removing the complexity. I think there\\'s like a lot of like not fake complexity but like added complexity on things that like doesn\\'t really need to be there a lot of the time. like concepts are a lot more simple and it\\'s all about function calling. So, uh we can do you know add like add to memory and I\\'ll append it and then get memory. Uh like this is super super simple. Um is this is this good enough? Let\\'s see. Maybe maybe. And so what we can do is give it the tools. Let\\'s see and then what when will we want to use them? Let\\'s say you like I can just say so this comment is going to be used as the uh string uh as the as the description in the function. So you can say like you know like when the user tells you something like factual about themselves their life or I can\\'t see anything okay or their preferences call this function um memory. Um and we\\'ll add a couple more cool things like expiration, which is one that I\\'ve kind of wanted to add for a while. So, you know, for now, let\\'s say false um memory.append uh think And then we have I guess what we call it uh you know memory text and we can say you know keep the memory text short size. Great. Cursor knows what I want. And then we can just return maybe like this is a super super super naive implementation. Um, but now, uh, when we start off, I guess we could even start off with [Music] a I mean, this is going to be kind of hacky, but I can just like um in your first turn always call get memory. This is not uh great. It\\'s just because of the demo loop. But maybe I\\'ll break I\\'ll break the demo loop out so we can actually do this by hand. But so how how could we prove this? Um maybe let\\'s say like you know write this memory bank or like keep this memory bank in a local file JSON file read it in at beginning and write it out at every Nice. Okay, I trust this. Shall we test it out? So, wait, someone uh Okay, we got the memory memory. Can any can anyone see any bugs? Because we\\'re about to test this out. So, we got the loop. It\\'s going to call it um and maybe like let\\'s memory. Okay. Sure. Yeah. Let\\'s try it. Let\\'s try it out. See what happens. So, hi. Did I not give it the functions? Is it hallucinating this? Oh yes, because I think we called it functions. Hi. There we go. Okay, so called get memory. There\\'s nothing there. I can just say like um I am 6 feet tall despite what people think. Uh, cool. Uh, so now let\\'s just check, right? It should have written this out. There we go. So now we have it in the memory bank. So now I can actually uh end this, right? And then be like, you know, uh, how tall am I? Tada. We\\'ve implemented memory, right? There\\'s a Yes, I\\'ll take a clap. I saw Louis Lewis, you\\'re like my proxy for the Luis uh Luis Costa, you\\'re my proxy for the audience. You\\'re like the only person I can really see. So, please don\\'t uh turn off your camera. Um, amazing. Now, we can do more interesting things, right? We can uh if we want do a little bit like smart querying uh where instead of just like loading in all of the memory, um we can like do a little bit of like retrieval uh to load in the right ones. um and use like semantic similarity or use some kind of search. Um I I could try to implement that. Um that might take a little bit longer but not that long. But I do want to pause here. See like given this and like this is going to be the style of things that we do like what um what do we want to see next? I can just keep going with this example. I can pivot. Uh it could be fun just to keep building on this. See how far we can get. Let\\'s see. uh delegation async have it chat and work in the background. We\\'ll get there. We\\'ll get there. I got that working this morning on Python because I didn\\'t want to switch to to Node yet. Uh delegation async. Okay. Let\\'s get into delegation then. So, there\\'s a few different ways we can do this. I\\'m actually going to leave the memory and we\\'re just going to keep building in this on this on this agent. Uh, and I am using the swarm agent just cuz I didn\\'t want to debug the one that I implemented. But like if we actually look at what this is, um, like it is very very very simple. And then the like run demo loop itself is like uh just printing out messages. Um, and what it\\'s doing is like appending client.run whatever. And like if we look at this client.run run. Um, if it doesn\\'t stream, it essentially does exactly what we did before, like keeps looping, get the completion, throw the messages in, uh, append them, handle responses, etc. There\\'s a couple more things around context and handoffs that we like don\\'t don\\'t really have to look at today. Um, but we can. So, cool, we have memory. Let\\'s do delegation. Um, so there\\'s a couple ways we can do this, right? Um, there\\'s if you think of like functions and agents and and everything you can, um, maybe I\\'ll skip to this slide. Skip skip skip skip. These are like a few of the forms of like agents on delegation that people might be familiar with. We have handoffs, which is like the swarm style. You take a conversation and fully swap it to a different agent. Um, and what that means is just like replacing the system prompt, replacing the tools. Um, you can have nested calls which are the easiest to implement and like often somewhat overlooked. Um, and then you can have manager tasks that\\'s more async. Uh, we will get to that today. So, let\\'s do a very basic one, right? Let\\'s say I want to do like um maybe let\\'s give it a chance to like call a bigger model to do a harder task, right? So we can say like you know uh I can delegate to smarter model and and I\\'m going to give like task description. [Laughter] Um so I\\'m laughing because this is saying like it\\'s just going to make it smarter by telling it to be smarter. That\\'s not despite how well that would work. Usually we\\'re not going to do that. So we\\'re we can just make an API request directly. Actually, let\\'s just do that. Let\\'s just do that. So um let\\'s do from OpenI client. And here we could do client chat completions create. Let\\'s call 01. Um I won\\'t provide a system message. I think that\\'s okay. content is the description. Look at that. And we did it. So now um man, I love cursor. Okay, did everyone catch all of that? By the way, all we did was like implement this this function that calls opening API and then here. So now I can say like uh so now I can add a bit of a description here. I\\'m like like uh if you know if the user has to like um I don\\'t know do something like that seems difficult or says it\\'s hard use this instead of try uh and it infers you know how to use this based on like the fact that I called it test description. It\\'s, you know, so let\\'s give this a shot. Hi. Um, let\\'s see. Uh, you know, give me a poem about, I don\\'t know, dogs and the earth where each other starts with the next letter. Yeah, it might just try it because I know GPD4 can do a variation of this, but um Oh, it\\'s giving it a shot. Okay. Let\\'s see if it gets it right. Okay. I don\\'t have uh I don\\'t have the patience. Let\\'s say answer briefly. one subkins max. Okay, great. And then to you know uh write a haik coup. There we go. It\\'s shorts where each other word starts with the next letter of the alphabet and ends with the previous letter of the alphabet. I don\\'t know. You guys want to try this while this does it? Is it around? Did I lose Did I lose something? Let\\'s see. Hi. Okay, you there? Cool. Um, did I not copy it? Oh no. Okay. Uh, give me a thank you. Now make sure each word starts with letter of the alphabet starting with a and each word ends with the previous letter. But this is hard. Okay. So it should be making the function call. Oh, I see. I see. I see. Okay. So I\\'m only printing the function call when it returns. So that\\'s probably why we\\'re waiting so long here. Um but this is actually a really good uh example of like okay we are doing um this like task delegation technically and like it is happening in the background and I\\'m going to give this a sec to figure this out. Um but like this is obviously a bad experience right like you don\\'t want to be waiting here. You essentially want to keep doing other stuff. So wow it\\'s still not still not back. We\\'ll we\\'ll we\\'ll we\\'ll let it figure it out. Um, so maybe let\\'s skip straight to async. Um, we\\'re actually got like both more and less time than I thought. So I don\\'t let this keep turning away. Um, now let\\'s think about this for a sec, right? If we want to do something async, it means that like what do we want to happen? There we go. So call the model da ya yada buzz breezy whispered. Is this correct? I don\\'t know. Wow. Yeah, this is sort of correct. What did it say? Note. Great. Great. So, it it did something, right? Um and it did something that 40 probably could not have, which is great. That\\'s delegation, but we were just sitting there waiting for it. Um so, let\\'s try doing this async. Now, I I I I actually want everyone to like kind of just stop and think like how would you implement this um in terms of like what behavior do you want? And maybe I want to see people like drop this in the Slack. Just like take a couple minutes um and like drop in the Slack like a proposal for how you would want to do this and then I might just pick one or we\\'ll like talk about them. Let\\'s call 03. Yeah. Not not yet. Not yet. Yes. Import async.io definitely feels like uh important. But I guess the questions are like you know when we delegate something uh obviously we want to have this happening in the background. Um, do we want like when it finishes, do we want to be like do we want it to be injected into the conversation? Do we want it to give us a response? Um, how many like do we want to be able to like interact with tasks that are running? Um, like do we want to be able to batch stuff? I am not setting up a Kafka cluster. I\\'m going to give people maybe a couple more couple more minutes to just like dump some ideas here. Um, batch calls. Yeah, I guess B. But how how would batch calls work? Maybe Stephania if you want to add some like detail there. Just keep working until it generates a stop word, right? Okay. Yeah, this is this is a good idea. JS and set timeout. Yeah, switching to JS is always a good option because you already have the event loop implemented. Smaller model. Smaller model. Okay, you guys are all suggesting some good ideas. Um, but we can actually go simpler, right? Like like essentially what we would want and I can implement like the basic interface of this, where\\'s my code? Um, is like instead of actually doing this, right, I can like return delegated, right? response pending. Um and then later I can say like you know check tasks or something. So this this pattern is actually one I I quite like a lot where you um you call a model it\\'s a function it\\'s non-blocking and then later you can you can check up on them. Now the thing is Python is like single threaded and uh async is real. Uh it\\'s just a little bit tricky. Um but let\\'s uh let\\'s give it a shot. So I do have some async emergency reference. Now here\\'s the here\\'s the um thing to notice. when something like for this to work correctly, we have this loop that asks for our input and blocks on our input um and like is displaying that on the screen and we don\\'t want to have that be displayed while it\\'s also like injecting messages. Um so we essentially want to separate out like where you give user input and where actually interesting stuff happens. So that\\'s what I\\'ve done in this in the basics of this async folder. So let\\'s take a look at that. Cool. Um, it looks complicated. It\\'s not that really that bad. So, um, I\\'m using websockets. Essentially, what I\\'m doing or sockets, sorry. Essentially, what I\\'m doing is, um, the I have a server and all it does is like has a handle user input function, uh, and a start message processor. Um, these aren\\'t super important. and we can take a look at them in a second, but essentially it just like waits uh on a specific like uh port and and host. Um and then I have a client that connects to that and it just does while true loops and lets me enter in. Right? So if we look at what that looks like, I have it here. Um so let\\'s go to async. Let me zoom in here a little bit. Also, um, while I do this, is there are there any questions that I\\'ve that I\\'ve missed? Um, and if there are, just feel free to shout them out. No. Great. Cool. Covered everything. Everyone is uh perfectly up to date with everything I\\'ve said. Okay. Um, sorry. Okay, so I have this and now I can say like, hi, let me show you the the rest of this. I guess that\\'s probably important. Uh, D. So we have the server, we have the client, we also have the a very very very basic agent imple implementation. It it looks just like the old one with the main exception that I\\'m using async openai instead. Um handling tool calls is happening in parallel. So for each tool call grab it create the task um await for them and then just like await for them all at the same time. Um, and this just lets it like create a bunch of tasks that are all going to run in parallel and once they\\'re done, like if they if they yield back at any point, we can keep doing other stuff. Um, by the way, if these are just like heavy functions that are doing heavy processing, it\\'s not going to matter. The fact that they\\'re in parallel like in sync, it means that it\\'s still going to happen one after the other. But if they\\'re like network calls to other models, then it\\'s perfect. Um, the runful turn is the same one. You like call a model, check to see if there\\'s any function calls, if not break, return the response. Um the only difference here is we\\'re awaiting uh the chat completion, right? This is the only difference. And then if we take a look at our agent handler, we\\'ve declared we\\'ve declared our agent like normal, right? Right now it has no instructions. It looks very familiar. And this loop is also pretty familiar. It\\'s like get the messages. Um here here\\'s the only difference, right? Um and you\\'ll see why this matters in a sec. I have a message Q. Uh and this is will be useful because we don\\'t want to process multiple messages in the same conversation at the same time. Like we want work to happen at the same time but we don\\'t want um like multiple generations to happen with the same history because then you\\'ll get conflicting like like if two messages if two functions return and they both need to be handled uh you essentially while you want them to happen in parallel the results should should only come in one after the other. So we have a cue for that. Uh and it treats user messages as well like that. So this is the handle user input that is being called from the server. Essentially just throws in a user input message into the message Q. Uh and all we\\'re doing here is like pulling off the queue, run it like you know uh put put the messages back in the in the message array uh and then like sleep, right? This is like just a very simple uh loop but it does it with async. So this is what we saw and and it feels pretty normal right. I can say like you know my name is Elan and I can say you know what is my name. Great. Um, now this still doesn\\'t exactly answer how we\\'re going to do things asynchronously, but it does give us the space to play with it because now things are happening async and we can do a few more delegations that that are actually async. So, let\\'s start with a, you know, simple blocking get weather function. I think I required all functions to be async here. Um, yeah. 67 and 70. Uh, and then skip that and let\\'s test it out. The only annoying part is we have to like restart two things now. So, hi. Uh, what\\'s the weather? Great. So we can see it called a function got the response. So far everything is normal. Now let\\'s do some delegation stuff. So let\\'s say let\\'s say we want to call this function uh three times for different places. So we have location [Music] um yeah and let\\'s say you know like pick a random number from 50 to 80 to return. Um, so if we do it in the previous case, do I still have the round? Yeah, maybe, maybe not. Okay. Uh so if we look at okay this is the non async one. Do we have weather? We don\\'t have weather. Let\\'s give it weather. Let\\'s give it the same weather function. Cool. [Music] Um we only need random. We don\\'t need this. And this not async. Okay. So now we have this weather function. Let\\'s test this out in the non- async case. Um I\\'m going to get rid of the rest for now. Um so if I say you know uh weather in SF cools it fantastic and now it\\'s like weather in SF New York uh you know and the five other cities. is it\\'s um it\\'s still going to do the parallel function calling and here it\\'s fine because they all return immediately, right? But now let\\'s add like an artificial weight. So let\\'s say time do sleep one, right? Um, cool. So, now what\\'s going to happen is we\\'re going to ask for that again, but it\\'s going to take so long. Weather in five random cities of your choosing. So, it\\'s going to take a while. And the reason is each of these is having to run um in like one after the other. There you go. It took like over five seconds. Now let\\'s do the same thing here. So the equivalent is um there\\'s async io sleep. Someone let me know if I\\'m doing this wrong. But um this should emulate like very very similar behavior. So if we instead run it here that\\'s and that\\'s I can say you know give me the weather of five random cities of York. Um we should see it return. Okay. Okay, so it called all of them and it got back because they all happened in parallel. Um, this is the magic of async.io, right? Um, anything that can be parallelized or I guess scheduled in a way where where it\\'s non-blocking. Uh, like sleeps, these sleeps are non-blocking. Um, essentially we can we can do and so you can imagine switching this sleep for like an actual API call. Um, so, uh, we can actually do that right now with 01, right? Like we could call 01 multiple times and they would all run in parallel. Now, the main problem is that we\\'re still going to be waiting back like waiting to to get all of them together, right? Like like the fact that it we can run them in parallel means that like we can have five 10-second tasks running in parallel. So it\\'ll take 10 seconds, but it\\'s still going to take 10 seconds where I can\\'t talk to the model. So instead, let\\'s have this notion of tasks. There\\'s many ways to do this. Um, I\\'m going to do a pattern that I quite like. So let\\'s do this. Um let\\'s define a you know create task. So I have a create task function that makes a task. Um no and I want to create like a task ID and I want to keep it short. Okay. So now what I\\'ve done is I have this function that makes a random task ID, sets it, creates it and then calls like let\\'s say get weather or something. Um and then it\\'s you know it\\'s suggesting this check tasks which is the next thing that I want to do. So the next one is check tasks. And so maybe I can just say like check task, you know, do for amazing. Um, okay. So let\\'s take a moment to to look at what just happened. We now create a task with a random ID. We give back the ID to the model and then later we can call check task. get that task, see what the status is, and see if it\\'s done. Um, so let\\'s like add, I don\\'t know, 5-second delay here. And right now, let\\'s just get weather. So description would be maybe create task for, you know, let\\'s see again this is all like sort of live coding, so we\\'ll see if it works. I can say like you know create a task um and set the description to the to be San Francisco. Let\\'s see what happens. Oh, I didn\\'t give it these functions. There we go. Um, let me just check. Cool, cool, cool, cool. Okay, let\\'s do it again. Say, you know, create a task where the location or the description is just. So now it creates the task and I can say hi. And look at that. It That\\'s not working correctly. Maybe it was Maybe it just took a while to respond. Um here, let\\'s add a longer delay to know for sure. 10 seconds. Let\\'s run it back. So create a task with description se uh cool. So I can say hello there. Cool. So it can still respond to me, right? Um check the task. So I can still interact with it and it can check to see if it\\'s not done. Um, I guess we need to keep checking to see if it\\'ll work. So, you know, how about or you know, tell me a joke. Who would have thought? Who would have thought? Um, cool. So, now like check them again. So, it called the task. It is done. And we have 78 and sunny. I will take a clap for that. I want to see everyone clap. Please clap. Thank you. Thank you. Thank you. Thank you. So, what you just saw was uh live asynchronous uh programming. Uh it\\'s very impressive. The models can also, you know, do pretty well. Anyway, here\\'s why this is interesting. We now have a system where we can give it tasks and it\\'ll ceue them up uh and then we can check on their progress. So, so already we have the basis for like a really really interesting thing, right? like this this thing here right right now it\\'s just get weather but if we say just like you know uh run what do it call like um you know call model and then no but we want AI port sync open AI get the client um I guess we Actually, we can just leverage the agents. They\\'re already async. Let\\'s not deal with that again. Um or whatever. Let\\'s do Let\\'s do it this way. I did all did it all. Fantastic. Okay. So, now we can switch get weather for call model. And let\\'s try this again. So, let\\'s give it the same task as before, which is, you know, hi, just make sure it\\'s running. um you know write me a uh ha coup about I don\\'t know uh a coup where the theme is so incredible it makes me cry make this a task okay so it\\'s created the task and now I can actually say you Now make a second one but pick a theme yourself. So now I have this interface where I can essentially keep chatting with this and it can essentially spin off additional tasks. Um you check check all tasks. Um let\\'s see what the progress is like. Look at that. So we have we have both. Oh, was this a one? That\\'s pretty fast. Um, but yeah, so now we have this system that you can actually just call call multiple ones. So I\\'m going to pause here just open up for questions for a little bit conversation like other directions we can take this other ways we could have done this. Um, but what I want to look at next is like how can we do this in a way where we don\\'t actually have to check um cuz right now like we don\\'t need these two terminals. This is just over complicated. Like I think the the the point of having the second terminal is so that we can push things to the conversation without a user having sent a message first. So I\\'m going to pause here. I\\'m going to field some questions and then we can get back to it. Okay. Um would you not use generators for nested tool calls? Um, Tom, do you want to I don\\'t know if there\\'s a mic in the room or like uh just get get it get it to them. Yeah, there is. Also, they can unmute themselves. Uh, who\\'s your Uh, sorry. So, is the one that was uh answering is So, in the room? Yeah. Can Can you hear him? No, I can\\'t hear him. Hello. Can you hear me in the Zoom? Yeah, I can hear you now. I can\\'t hear you anymore. I don\\'t know if you\\'re saying a question right now. No, I was just wondering if it\\'s possible to treat agents objects as a generator in Python. So you can nest like you can yield from individual agents and then yield those tool calls. Does that make sense? Yeah. Yeah, that makes sense. This is also very impressive that you have to do this. I think someone needs to mute themselves. I don\\'t know if it\\'s Alain or uh like someone in the room unmuted and uh put put this man through through hell. Um No, it\\'s me. It\\'s me. Oh, is your laptop unmuted? I can\\'t hear anyone anymore. Maybe just speak into your laptop. Okay, but I\\'ll I\\'ll answer your original question of like, you know, if you put um can you like essentially make agents into generators so that you can like yield the results as you go? The answer is yes. And this is actually like um the right way to do this. The way I\\'m doing it now is I\\'m only exposing the final response mostly because um implementing like the the generator gets a little bit tricky and I don\\'t want to have to like debug that. Not too much, but you can essentially just surface each of the steps, each of the function calls, each of the everything. um that would um that would let you essentially keep track of more agents at the same time and you could maybe have like a bit of a flat structure where you just have multiple agents going yielding events and then you can essentially see like which one\\'s coming from where. Um but I guess the thing is like there you have to deal with a bit of the complexity of um handling all the like like essentially listening to like all the different events and like uh associating them back to like a specific agent and like maybe there is an agent in the front, maybe there isn\\'t. Um here the idea is just I have one agent that I\\'m interacting with. I\\'m always just going to interact with like that one agent and it\\'s responsible for spinning off other other ones and like dealing with dealing with that as well. But you can absolutely do it the way that you\\'re saying. And like if you wanted to build this out like more fully, you probably would in order to be able to surface progress. Um, did that answer your question? I\\'m now scared to get people\\'s talking in the room, but I\\'ll just read them from Slack. Um, is there any is there any good design patterns to create projects with agents? There\\'s like a million. Um, you know, like I like prototyping with swarm. Um, I know paidantic AI is also like a really nice one. I think Sam\\'s there in the room or around. Um, there\\'s Yeah, I don\\'t want to like I don\\'t really use uh any myself mostly because like you saw it\\'s pretty simple to implement your own loop. So that\\'s what I end up doing. I think usually for every project I either write my own like it\\'s like I don\\'t know how many how many lines is this like like 70 lines and so like depending what you want you might want more you might want less and I\\'m sure there\\'s good solutions um but like I just have I could just copy this around um I don\\'t really like working with too many dependencies especially for something that\\'s so lightweight like I I want granular control um and like except for example for Swarm I ended up having to hop in here and like handling specific kinds of tool calls in certain ways so that I can do handoffs. Um, which we can actually look at and we can implement this without too much trouble. Um, but yeah, my answer is like there\\'s many you can choose from. I don\\'t have many I would recommend personally. Um, but I I am a I am a fan of of Pyantic AI. It\\'s pretty cool. I haven\\'t used it too much, but like the interface looks nice. Um, it reminds me a lot of Swarm. The one I use the most for prototyping is Swarm. I think it\\'s just cuz it\\'s the one that like I\\'m most familiar with. Um, when you have to start to approach dozens or hundreds of functions, what techniques should we apply in order to effectively tool call? There\\'s there\\'s a few answers there, right? Um, you can have multiple agents and essentially like split up the responsibilities or like the the groupings of the functions. Um and so into into yeah like clusters where you have like a set of related functions that are needed for specific tasks and then you can invoke the correct agent. Uh and this is where like multi- aent patterns start to make sense is like specifically when you have tons and tons of functions like how do you uh go to the right ones? If you for some reason need them all at the same time, uh you could try fine-tuning. Um in in projects for OpenAI, I\\'ve ended up fine-tuning up to like hundreds of fun like it was like 120 functions. This was with uh GPT3.5. So the fact that that works gives me like pretty high confidence like you can fine-tune uh smaller models with a lot of functions and get them to work pretty well. Um the last one is like some kind of dynamic function loading where based on the input or based on the conversation you load into memory or like you load into context um the most likely relevant functions. Um and there\\'s a few different ways to do this. You can do this with embeddings. You can do this with like um having like a two-step function call. At that point, you\\'re essentially having agents. Like if if you call a function to then load more functions, that\\'s what a handoff is essentially. So a lot of these start to look very similar. Um it\\'s just like how are you loading multiple different ones? Um okay, for visioning models, are there tools being called within the thought text? Um, so because we don\\'t expose the thoughts like the the chain of thought, um, that\\'s a little bit hard to answer for for for 01 right now in the API. The answer is no. Um, I\\'m trying to see how much I can say here. Um, it is something that is technically possible, right? Like you can do anything you want with with post training. Um, we do not currently allow you to call functions within the chain of thought. Um, so yeah, right now the function calls happen at the very end. Uh, do you have any good code examples for router patterns in these lots of functions cases? Yeah. So, so my like my answer there is some one of these. Like if you really just want to route um then like the the idea of having like multiple agents and handing off to one of them is actually really nice. Um like you can just define multiple agents each of them with like multiple functions. Um and then have the first one have like a we can actually do this quickly like we I\\'ll do this in swarm but like I said you can implement this yourself. Um I don\\'t actually know who else supports handoffs the same way Swarm does, but um essentially here let\\'s start a new file like uh routing. So see swarm ripple demo loop I can say like you know triage equals one and then I can have my other two. It\\'s like you know maybe I have like some collection of like um you can call them an agent but I can also just call them yeah like uh what it be like you know uh email functions agent and you can have like you know like send email check email I don\\'t know what else does cursor want me to write there. Um, cool. We have these two and then we can do like maybe we have the emails and then we have a calendar. I don\\'t know. So make like create event can do like calendar. I\\'m just going to say like you know finish what I\\'m doing update the prompts and the tools. I\\'m lazy. So let\\'s see what uh let\\'s see what cursor thinks. Great. So now we have an email functions agent and a calendar agent. Call them whatever you want. Um now like let\\'s pretend that instead of just having three functions, we have 30 and each one of these has 10 or 15. Um then like if you maybe give all 30 to one agent, first of all try it. Like if that works, amazing, right? you don\\'t have to deal with the extra complexity. Um so you don\\'t really want to do handoffs and stuff until you really really need to through like evals. Um but um yeah. So, so and then the special like kind of functions here is like you know uh transfer to email agent and I\\'ll return email agent and then cool. So now I have my two transfer functions. Oh. Oh, it did it. Okay, cool. So I think this should just work right. So I\\'ve defined like the actual functions and agents that in your case would be many many more functions. Um I\\'ve defined the transfer functions and I\\'ve given them to the triage agent. So now if I run this say hi um you know I want to send an email. Cool. So now I\\'m talking to the email agent. Now if you want to do like more you know like transfers uh this is not delegation. Okay. So trails assistance um maybe we can tell the assistance like you know if you already know what the user is asking just call that function right and this is if you want to have like a case where you know it still routes you but it\\'s like a faster two hop so maybe I can say like what are the functions here um what are the parameter There\\'s send email to subject body. So I can say like know send an email to bobgmail.com about taxes. What was the other one? Body um saying yo do your taxes. So it should transfer me to the email one. And then there we go. It immediately sends the email. So this felt like an immediate function call, but there was a transfer in the way. So this is kind of an example of like triaging does work. It\\'s really convenient to model it with agents and handoffs. Um I\\'d say it\\'s the primary use case for agents and handoffs is just a glorified triage through uh multiple functions. Um yeah, let\\'s see. Let\\'s go back to questions. [Music] Uhhuh. Consider having something like streamllet grad until you are you considering having something like streamlink radio util that allows porting all interaction functionality with one command. I don\\'t think so. I don\\'t know. Uh what did people answer? There we go. Sam\\'s in with pantic. So check that out. Um, how many tool calls can you get in one iteration? You like parallel function calls. I don\\'t think we have hard limits on either the number of functions or the number of parallel function calls. How big a tool library will the models perform well with? Super super general rule of thumb is like 10 to 20 you shouldn\\'t really pass over. Um, but like I said, I\\'ve gotten like this was a very specific case where like it was extremely latency sensitive and so like we had to have flat like flat function calling and we did 120 functions with GPT 3.5. So you can go pretty far um with fine-tuning, but like I\\'d say reliably without without extensive prompting. Yeah, probably like 10 to 20 like past that point. You really ask yourself like what are you trying to do? Like why are you putting so many functions? Is it super latency sensitive? Can you split it up? Like yeah. Um was there a follow-up here? Okay. Um rather than tool calls, I feel like we\\'re moving to generated code agents, will I soon be able to supply my tools functions? Ah, okay, that\\'s a good idea. We should try that. Um so essentially have something write its own function and then use it. Uh yeah, I feel like we can probably How would we do that? Yeah, we can try to we can we can find a way to do that. Uh I I\\'ll do that actually. Let\\'s do right now. Why not? Let\\'s do right now. Uh I\\'ve never done this before, but I feel like it shouldn\\'t be too hard. So um okay. Uh you know, what do we call this? Like bootstraps. Okay. So I\\'ll keep using swarm because I I will use um handoffs from swarm import agent. Cool. So now it\\'s always so let\\'s see we want an agent that writes it own functions. So agent was this one. Um we want it to write its own functions. So maybe we can have an hand off to itself. So we can do you know like refresh you know what to be refresh functions. So we can actually return the same agents. Um and then okay is it not declared? Is it unhappy? Uh yeah it\\'s not defined yet. Okay. Okay. So now we can define it down here. Okay. Um bootstrap bootstraps. Bootstraps. Okay. So we have this and we want it to write its own functions. So how are we going to do this? We wanted to produce Python. Does anyone have any ideas? uh if you want to shut them out like make just pass around the mic while I code just feel free to I can\\'t I can\\'t read while you\\'re uh what you write but we can we can try this. So, um let\\'s say we want it to, you know, um you know, add tool. There we go. And then let\\'s call this, you know, Pythonation. Okay. I\\'m going to do something very unsafe. So, you know what would it be? It\\'s like function obj equals eval of Python. Don\\'t do this. Don\\'t do this, kids. Um, and then will this work? Is this how Python works? Sam\\'s there. Uh, okay. So if I have this and I eval like you know def a does it um okay you know Um, so I guess it\\'s like uh I want to write a write a function that takes a string representing an implementation of a Python function and returns the actual rule Python function as interpreted. I don\\'t know. Let\\'s see what it does. Um, so essentially we want to do that once we have the function then we can just append it to the agent and then we might need to reload it. Uh, so we can then just return the agent and this might be it. This might be it. So add tool exec not eval. Why didn\\'t anyone say that? You guys can shout out. Okay. Okay. So it\\'s exec and then what is this 80? I don\\'t know. Someone save me. Like uh Hey, there\\'s a couple implementations you miss. Sorry. Check the flash. Did someone do this? Hey. Okay. It\\'s almost like someone fired. No, no, no. But this is not Okay. No, but this is not what I want, right? Like I don\\'t want it to run it in a subprocess. I wanted to I wanted to eval like evaluate the function and then turn it into a is that Oh, go up. Oh, Sam, of course. Amazing. Uh Jesus Christ. Okay. um make it. So the add tool function uh evaluates the implementation and adds it to the tools similar to now. Here\\'s a great reference for how to Let\\'s And then we want to grab Was this a good idea? This is a terrible idea. I hope people are enjoying this. Um, okay. Let\\'s see. Um, and let me just read this because it might not be that complicated to add in. So, I can\\'t. Okay, it got in the way. No, not Zoom. Where\\'s Slack? Was this it? Is this it? Okay, I\\'m going to I\\'m going to put this on hold for now. It sort of ignored your implementation, Sam. I\\'m sorry. Um, let\\'s see. So if you have parse function, can I just do this? Does this work? Uh, okay. Let\\'s see. I\\'ve first time for everything. Uh, wrong one. Sub bootstraps. Hi. Um, add a tool that prints hello when called it. Oh, but no, no, no, no. Make it print it, not just return it. My heart\\'s pounding. Okay. Um, so say hello to call it. Look at that. Look at that. Okay. So, now we have Yes, I will take a clap for that. We did this together, guys. Um, so this is actually a lot less code than I was expecting, but now we have a system. Look at that. It\\'s tiny. Um, that can write its own tools. Um, and so like maybe we can do something like, you know, um, what is, you know, like make yourself a little calculator. I can\\'t believe this just worked. Uh, you know what is 2 * 3 * Can someone check this? Uh, wait, wait, wait. 34698. 34698. This is This is crazy. This is This is so fun. Uh, yeah. I hadn\\'t done this before. And this is a lot less code than I thought. Look at this. Look, this is all you need. I mean, it\\'s this is super dangerous code. Like, this this is not good. Don\\'t do this. But it\\'s fun. Uh, I would put this squarely in fun things. We can we can now transition to looking at other fun things. Um they\\'re definitely not as fun as this one, I think. Um they\\'re just a couple of like random things that I found related to real time since I did sort of put it in this uh title. So whatever. Okay, these are the two main tricks that I that I kind of thought were pretty cool and you might have already seen them. Um, one is if you\\'ve ever dealt with like the real time API um like jumping in uh before you\\'re done with an idea or like done talking or something. Um like what I was thinking is like if if really what you want is you wanted to like use the model\\'s own intelligence to decide if you\\'re done talking or not. Um and you can treat our VAD like our voice detection as like a trigger happy version of that. It\\'s like a it\\'ll always tell you when maybe the user is ready to stop talking. But if you want the model to check, you can have a stay silent function or something else that essentially handles the other side of that where it\\'s like, okay, we\\'re definitely going to let you know whenever the user might be done talking, but then you can actually verify with a function call. And so this implementation is like super super simple. You literally just give you give the the function itself to the real time API and tell it to call it when the user is not quite done talking. And I\\'m not going to try to demo this live. Like real time demos are tough. Audio is tough. All that fun stuff\\'s tough. But um it works like surprisingly well. Uh if you just describe I I can you can probably find this tweet but it has the full prompt but essentially it\\'s like you can you can like pause you can say like you know I\\'ve been thinking about and then like stop and like it\\'ll get triggered call stay silent and then you can keep talking. Um so pretty pretty cool uh useful thing. Um and then the other one is also related to real time API. And if again if you if you uh are following me you might have already seen this but um someone at Devday just like came up to me and asked me like hey um is there a way to like make the real time API talk in like a specific way. Uh and I was like I don\\'t know let\\'s try it. And so like we had a demo booth and I sort of pushed someone out of the demo booth and grabbed the laptop and I was like let\\'s just like try try this stuff. And apparently if you ask the model to just you give it like a a script and you\\'re just like, \"Hey, read this out according to these XML tags um you can have it follow them here. Let me let me find um the original the original one.\" Yada yada yada. Sorry, this is I\\'m not trying to show you the whole timeline. Where is it? Okay, this is the stay silent one. Um, where is it? Where is it? There we go. Um, uh, it\\'s impossible that you\\'ll hear this, right? You guys don\\'t hear this. Yes. No. Uh, we don\\'t hear it, but you can hear your computer audio. Can I do the thing? I\\'m I\\'m I\\'m actually I\\'m not going to try. That\\'s fine. Um, wait. Can I same? No, no, no. It\\'s fine. Whatever. Anyway, um, yeah. So, if if you if you like give it like a script like this, um, we didn\\'t actually train it for this, right? Like this is just a really nice consequence of like behavior. It\\'s technically not function calling. I\\'m realizing now, but it\\'s like it\\'s very function callingesque. It\\'s real time. The title of this talk does include real time. So, this is uh this is it. Um I will pause here. I have so much more I can go through, but I want to give you guys a chance to like ask questions, poke around with some of these ideas. Uh Swarm is public. You can just try it out. Um we can also just try creating other functions. Uh this this is one of the this is easily one of the coolest like little programs I\\'ve ever made. Um, so yeah, let let me see if there\\'s any other questions of the Slack. Uh, it\\'s a room full of people auto completing make. Yeah, yeah, yeah. Okay, let\\'s not do that. Uh, okay. Revisiting memory. Let\\'s see. Do you have any suggestions for trying to enforce consistency of stored memories? Identifying inconsistencies and figure out how to resolve them. Okay. And then the second part is what data structures do you suggest for more structured memory helping enable more meaningful comparison of objects? I think I mean this question opens like you start to go down a path where you can get as complex as you would like, right? Like like you can start simple and you can end up with like an entire operating system to manage memory, right? There\\'s like the whole range. Um, one way to do off the top of my head like one way I can think to do this is um when you are about to store memory do a retrieval like do search to find similar memories or like memories that like are semantically similar um and then do an explicit check uh with a model to see if like you know one is like updating or contradicting another. Right? An example of this is like you know what is the latest state of a project right you know if someone\\'s like is the project like at some point you say like I\\'m working on the project like it\\'s not ready yet and it saves that and then later you\\'re like you know I um how like the project\\'s done now right like those are two contradicting memories um what you can do is essentially uh have a time stamp for them but also when you are about to store the second one or any memory check for similar ones and create like a direct like essentially um like node uh pointing from the original memory to the new one. And so I guess I guess what I\\'m thinking is like that way if anybody asks and you surface both like you can keep both in memory and when you do like retrieval and semantic similarity you can surface both but essentially you can present like the whole chain of updates and so you can just present the last one if you want or you can present the whole chain and the model have has some idea of like you know maybe if you ask it like um you know how long is this project delayed because that was the last you heard about it. But somebody at some point said, \"Oh, it\\'s actually done.\" Um, like if it raises both and has one with a later date, you know, maybe. But if you like if you\\'ve already made this chain explicitly, um, like you can actually represent that and you can choose to not show the previous ones. So that\\'s one idea. Like I said, there\\'s so many you can like so many ways to do this. Uh, and and like as soon as I stop talking, you guys walk out. Someone\\'s gonna like be like, \"Oh, there\\'s actually this like much easier way that like this idiot didn\\'t think of.\" So, anything you think of probably works. Um, here\\'s a real time API prompt. Yeah. Cool. Uh, any other any other questions about anything? Um, it\\'s been like an hour and 40 minutes, so happy to like keep going with random content. Um, there\\'s a couple demos that I have that are like demoing some public repos that we have that you can use. Um, yeah, but I want to I want to give anyone a chance to like speak up, say something if you want, ask questions. Um, I I don\\'t see I don\\'t see a lot happening in I I can\\'t You guys are tiny on my screen. Uh, cool. Okay, then I\\'ll pose this to you. Sorry, what? Oh, no. Go ahead. It sounds like you\\'re wrapping up. Yeah, the the last thing we can do, I guess, is like either I can um try and pull up a real time uh demo that shows how to do the 01 stuff uh in a in a slightly easier way. Okay, cool. People want to do that demo. Okay, let\\'s do it. So, um the reason I\\'m a little iffy about it is because yeah, it\\'s real time. Uh and those demos are tough to do publicly but um there is this repo OpenAI um real time Twilio demo um that essentially sets up your whole um like phone calling assistant uh and it\\'s just works right out of the box. It walks you through the steps. Um I may end up sharing a phone publicly. don\\'t call it during this demo and don\\'t share it and I will delete it. But um yeah, I trust you guys. So let\\'s see. Okay, so it looks like this is already running. Oh, I do have it already running. Okay. Um Oh, never mind. I I did uh I did make it all dots. But yeah, essentially this um this little checklist is like live updates. So as you set up the account, as you set up the phone number, as you set up your local web server, etc., it\\'ll like get checked. So setting up Twilio and Grock and everything has been one of the most annoying things I\\'ve had to do multiple times um related to the real time API. So this hopefully makes that process like a lot a lot easier. You can do most of it directly from here and it lets you know like when things are done. So you don\\'t actually have to do a lot from the Twe console at all. Um anyway, so um I let me show you the back end. So there is a handler function. There we go. Function handlers. So um you can either implement tools uh locally as a JSON schema and we can we can try that really quick. So this is the real time playground and I can say like you know make a function to you know get the answer to answers to everything no params maybe one param. Okay, so it gave it gave us this nice little uh nice little function and we can just paste it in here, save it, and cool. So now we have this get universe answers and I can save this configuration and call the phone number. So, let\\'s see. Hello. Hello. Okay, this is why I love them. Let me let me try once more. Okay, let\\'s go tool. Paste this. Save changes. Save config. [Music] Um, hello. Hello. I don\\'t think it\\'s happy. Is someone else? No, I didn\\'t show the number. Hello. Okay, it\\'s not working. That\\'s fine. Sort of expected it. Um, essentially what you can do like when you when you use this, um, it\\'s cool because these tools you can either specify local ones with schema and they appear here or you can define them in the back end. Um, so if you define them in the back end, you can actually give like code to execute. Um, if not, they\\'ll just show up here. Uh, and you can enter in like what the response will be uh, like a mock response. Um, but you can also like set up backend functions that will actually get handled in the back end. Uh the sample one is like a real implemented get weather one but as you can see there\\'s like it\\'s pretty straightforward to implement like the 01 one. Now the main difference between the real time API here and like what we were doing before is the real time API does actually allow asynchronous functions natively. So you can the model can call a function um get no response and you can keep talking with the model um until a response is back. So that that is behavior that like we specifically had to teach the real-time models because unlike a chat conversation, you can\\'t really enforce um like you can\\'t really halt the whole conversation until the functioning response comes back. So, we had to do that for for for launch um because originally when we hadn\\'t done that, you know, we hadn\\'t ever shown it how to do asynchronous functions and so it just couldn\\'t couldn\\'t handle them correctly. Anyway, sadly, uh demo didn\\'t quite work. Maybe I can try this one more time. We\\'ll see. Third time\\'s a charm. Hello. Hello. No. Okay, great. Anyway, um I believe that is most of what I wanted to cover today. Um, I\\'m going to play with this hyper unsafe function making agent a bit, but um, yeah, thank you all for coming. I\\'ll hang around for questions for a bit if anyone has any. Otherwise, I think we\\'re ending a little bit early. Um, like I said, yeah, mo most of you can can hang out or or leave as you please. Um, but I\\'ll maybe I\\'ll call it wrapped up here. Um, maybe what we can do is if you\\'re interested, you can stay and like hack on some of the stuff that you saw. I\\'m happy to hang around uh and answer questions as well. So, thank you all for coming. Uh, hope you hope you got something out of this. Um, the new uh to help in sports. Uh otherwise, I guess you can share. I\\'m sharing a little bit in case people have questions. I\\'m actually going to dump this, but this may be a really bad idea, but um here you go. Here\\'s the code for the super unsafe self like tool writing agent. And I\\'m probably going to stop sharing my screen. Um, yeah, I can I can I can share the repo and the slides. There\\'s not a lot on the slides, but yeah, I\\'m happy to happy to share them after.'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_youtube_transcript(youtube_url: str):\n",
    "    \"\"\"\n",
    "    Tool function to fetch the transcript of a YouTube video.\n",
    "    Input: YouTube video URL string.\n",
    "    Process: Extracts video ID, calls YouTubeTranscriptApi, and formats the output.\n",
    "    Output: Dictionary containing status ('success' or 'error') and either\n",
    "            the 'transcript' or an error 'message'.\n",
    "    \"\"\"\n",
    "    import re # Moved import inside\n",
    "    from youtube_transcript_api import YouTubeTranscriptApi # Moved import inside\n",
    "    from typing import Dict, Any # Keep for return type annotation\n",
    "\n",
    "    logger.info(f\"Tool 'get_youtube_transcript' called for URL: {youtube_url}\")\n",
    "\n",
    "    try:\n",
    "        # --- Start of merged fetching logic ---\n",
    "        video_id_match = re.search(r'(?:v=|/|embed/|shorts/)([0-9A-Za-z_-]{11})', youtube_url)\n",
    "        if not video_id_match:\n",
    "            logger.warning(f\"Could not extract video ID from URL: {youtube_url}\")\n",
    "            response: Dict[str, Any] = {\"status\": \"error\", \"message\": \"Could not extract video ID from URL.\"}\n",
    "            return response\n",
    "\n",
    "        video_id = video_id_match.group(1)\n",
    "\n",
    "        # Try fetching the transcript\n",
    "        try:\n",
    "            transcript_list = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "            transcript_text = ' '.join([entry['text'] for entry in transcript_list])\n",
    "            logger.info(f\"Successfully fetched transcript for {youtube_url}\")\n",
    "            response: Dict[str, Any] = {\"status\": \"success\", \"transcript\": transcript_text}\n",
    "            return response\n",
    "        except Exception as fetch_error:\n",
    "            # Log specific exceptions from the API call\n",
    "            logger.warning(f\"Could not fetch transcript for {youtube_url} (ID: {video_id}): {fetch_error}\")\n",
    "            response: Dict[str, Any] = {\"status\": \"error\", \"message\": f\"Could not retrieve transcript: {fetch_error}\"}\n",
    "            return response\n",
    "        # --- End of merged fetching logic ---\n",
    "\n",
    "    except Exception as e:\n",
    "        # Catch unexpected errors during the tool's execution (e.g., regex error?)\n",
    "        logger.error(f\"Unexpected error in get_youtube_transcript tool for {youtube_url}: {e}\", exc_info=True)\n",
    "        response: Dict[str, Any] = {\"status\": \"error\", \"message\": f\"Failed to execute transcript tool due to an unexpected error: {str(e)}\"}\n",
    "        return response \n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "result = get_youtube_transcript(\"https://www.youtube.com/watch?v=KUEmEb71vzQ&t=5522s\")\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_youtube_transcript(youtube_url: str):\n",
    "    \"\"\"\n",
    "    Tool function to fetch the transcript of a YouTube video.\n",
    "    Input: YouTube video URL string.\n",
    "    Process: Extracts video ID, calls YouTubeTranscriptApi, and formats the output.\n",
    "    Output: Dictionary containing status ('success' or 'error') and either\n",
    "            the 'transcript' or an error 'message'.\n",
    "    \"\"\"\n",
    "    import re # Moved import inside\n",
    "    from youtube_transcript_api import YouTubeTranscriptApi # Moved import inside\n",
    "    from typing import Dict, Any # Keep for return type annotation\n",
    "\n",
    "    logger.info(f\"Tool 'get_youtube_transcript' called for URL: {youtube_url}\")\n",
    "\n",
    "    try:\n",
    "        # --- Start of merged fetching logic ---\n",
    "        video_id_match = re.search(r'(?:v=|/|embed/|shorts/)([0-9A-Za-z_-]{11})', youtube_url)\n",
    "        if not video_id_match:\n",
    "            logger.warning(f\"Could not extract video ID from URL: {youtube_url}\")\n",
    "            response: Dict[str, Any] = {\"status\": \"error\", \"message\": \"Could not extract video ID from URL.\"}\n",
    "            return response\n",
    "\n",
    "        video_id = video_id_match.group(1)\n",
    "\n",
    "        # Try fetching the transcript\n",
    "        try:\n",
    "            transcript_list = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "            transcript_text = ' '.join([entry['text'] for entry in transcript_list])\n",
    "            logger.info(f\"Successfully fetched transcript for {youtube_url}\")\n",
    "            response: Dict[str, Any] = {\"status\": \"success\", \"transcript\": transcript_text}\n",
    "            return response\n",
    "        except Exception as fetch_error:\n",
    "            # Log specific exceptions from the API call\n",
    "            logger.warning(f\"Could not fetch transcript for {youtube_url} (ID: {video_id}): {fetch_error}\")\n",
    "            response: Dict[str, Any] = {\"status\": \"error\", \"message\": f\"Could not retrieve transcript: {fetch_error}\"}\n",
    "            return response\n",
    "        # --- End of merged fetching logic ---\n",
    "\n",
    "    except Exception as e:\n",
    "        # Catch unexpected errors during the tool's execution (e.g., regex error?)\n",
    "        logger.error(f\"Unexpected error in get_youtube_transcript tool for {youtube_url}: {e}\", exc_info=True)\n",
    "        response: Dict[str, Any] = {\"status\": \"error\", \"message\": f\"Failed to execute transcript tool due to an unexpected error: {str(e)}\"}\n",
    "        return response \n",
    "\n",
    "result = get_youtube_transcript(\"https://www.youtube.com/watch?v=KUEmEb71vzQ&t=5522s\")\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 45\u001b[39m\n",
      "\u001b[32m     42\u001b[39m         response: Dict[\u001b[38;5;28mstr\u001b[39m, Any] = {\u001b[33m\"\u001b[39m\u001b[33mstatus\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to execute transcript tool due to an unexpected error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m}\n",
      "\u001b[32m     43\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response \n",
      "\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m result = \u001b[43mget_youtube_transcript\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttps://www.youtube.com/watch?v=KUEmEb71vzQ&t=5522s\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m     46\u001b[39m result\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mget_youtube_transcript\u001b[39m\u001b[34m(youtube_url)\u001b[39m\n",
      "\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01myoutube_transcript_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m YouTubeTranscriptApi \u001b[38;5;66;03m# Moved import inside\u001b[39;00m\n",
      "\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, Any \u001b[38;5;66;03m# Keep for return type annotation\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mlogger\u001b[49m.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTool \u001b[39m\u001b[33m'\u001b[39m\u001b[33mget_youtube_transcript\u001b[39m\u001b[33m'\u001b[39m\u001b[33m called for URL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00myoutube_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[32m     15\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[32m     16\u001b[39m     \u001b[38;5;66;03m# --- Start of merged fetching logic ---\u001b[39;00m\n",
      "\u001b[32m     17\u001b[39m     video_id_match = re.search(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m(?:v=|/|embed/|shorts/)([0-9A-Za-z_-]\u001b[39m\u001b[38;5;132;01m{11}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m'\u001b[39m, youtube_url)\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'logger' is not defined"
     ]
    }
   ],
   "source": [
    "def get_youtube_transcript(youtube_url: str):\n",
    "    \"\"\"\n",
    "    Tool function to fetch the transcript of a YouTube video.\n",
    "    Input: YouTube video URL string.\n",
    "    Process: Extracts video ID, calls YouTubeTranscriptApi, and formats the output.\n",
    "    Output: Dictionary containing status ('success' or 'error') and either\n",
    "            the 'transcript' or an error 'message'.\n",
    "    \"\"\"\n",
    "    import re # Moved import inside\n",
    "    from youtube_transcript_api import YouTubeTranscriptApi # Moved import inside\n",
    "    from typing import Dict, Any # Keep for return type annotation\n",
    "\n",
    "    logger.info(f\"Tool 'get_youtube_transcript' called for URL: {youtube_url}\")\n",
    "\n",
    "    try:\n",
    "        # --- Start of merged fetching logic ---\n",
    "        video_id_match = re.search(r'(?:v=|/|embed/|shorts/)([0-9A-Za-z_-]{11})', youtube_url)\n",
    "        if not video_id_match:\n",
    "            logger.warning(f\"Could not extract video ID from URL: {youtube_url}\")\n",
    "            response: Dict[str, Any] = {\"status\": \"error\", \"message\": \"Could not extract video ID from URL.\"}\n",
    "            return response\n",
    "\n",
    "        video_id = video_id_match.group(1)\n",
    "\n",
    "        # Try fetching the transcript\n",
    "        try:\n",
    "            transcript_list = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "            transcript_text = ' '.join([entry['text'] for entry in transcript_list])\n",
    "            logger.info(f\"Successfully fetched transcript for {youtube_url}\")\n",
    "            response: Dict[str, Any] = {\"status\": \"success\", \"transcript\": transcript_text}\n",
    "            return response\n",
    "        except Exception as fetch_error:\n",
    "            # Log specific exceptions from the API call\n",
    "            logger.warning(f\"Could not fetch transcript for {youtube_url} (ID: {video_id}): {fetch_error}\")\n",
    "            response: Dict[str, Any] = {\"status\": \"error\", \"message\": f\"Could not retrieve transcript: {fetch_error}\"}\n",
    "            return response\n",
    "        # --- End of merged fetching logic ---\n",
    "\n",
    "    except Exception as e:\n",
    "        # Catch unexpected errors during the tool's execution (e.g., regex error?)\n",
    "        logger.error(f\"Unexpected error in get_youtube_transcript tool for {youtube_url}: {e}\", exc_info=True)\n",
    "        response: Dict[str, Any] = {\"status\": \"error\", \"message\": f\"Failed to execute transcript tool due to an unexpected error: {str(e)}\"}\n",
    "        return response \n",
    "\n",
    "result = get_youtube_transcript(\"https://www.youtube.com/watch?v=KUEmEb71vzQ&t=5522s\")\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 45\u001b[39m\n",
      "\u001b[32m     42\u001b[39m         response: Dict[\u001b[38;5;28mstr\u001b[39m, Any] = {\u001b[33m\"\u001b[39m\u001b[33mstatus\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to execute transcript tool due to an unexpected error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m}\n",
      "\u001b[32m     43\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response \n",
      "\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m result = \u001b[43mget_youtube_transcript\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttps://www.youtube.com/watch?v=KUEmEb71vzQ&t=5522s\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m     46\u001b[39m result\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mget_youtube_transcript\u001b[39m\u001b[34m(youtube_url)\u001b[39m\n",
      "\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01myoutube_transcript_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m YouTubeTranscriptApi \u001b[38;5;66;03m# Moved import inside\u001b[39;00m\n",
      "\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, Any \u001b[38;5;66;03m# Keep for return type annotation\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mlogger\u001b[49m.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTool \u001b[39m\u001b[33m'\u001b[39m\u001b[33mget_youtube_transcript\u001b[39m\u001b[33m'\u001b[39m\u001b[33m called for URL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00myoutube_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[32m     15\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[32m     16\u001b[39m     \u001b[38;5;66;03m# --- Start of merged fetching logic ---\u001b[39;00m\n",
      "\u001b[32m     17\u001b[39m     video_id_match = re.search(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m(?:v=|/|embed/|shorts/)([0-9A-Za-z_-]\u001b[39m\u001b[38;5;132;01m{11}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m'\u001b[39m, youtube_url)\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'logger' is not defined"
     ]
    }
   ],
   "source": [
    "def get_youtube_transcript(youtube_url: str):\n",
    "    \"\"\"\n",
    "    Tool function to fetch the transcript of a YouTube video.\n",
    "    Input: YouTube video URL string.\n",
    "    Process: Extracts video ID, calls YouTubeTranscriptApi, and formats the output.\n",
    "    Output: Dictionary containing status ('success' or 'error') and either\n",
    "            the 'transcript' or an error 'message'.\n",
    "    \"\"\"\n",
    "    import re # Moved import inside\n",
    "    from youtube_transcript_api import YouTubeTranscriptApi # Moved import inside\n",
    "    from typing import Dict, Any # Keep for return type annotation\n",
    "\n",
    "    logger.info(f\"Tool 'get_youtube_transcript' called for URL: {youtube_url}\")\n",
    "\n",
    "    try:\n",
    "        # --- Start of merged fetching logic ---\n",
    "        video_id_match = re.search(r'(?:v=|/|embed/|shorts/)([0-9A-Za-z_-]{11})', youtube_url)\n",
    "        if not video_id_match:\n",
    "            logger.warning(f\"Could not extract video ID from URL: {youtube_url}\")\n",
    "            response: Dict[str, Any] = {\"status\": \"error\", \"message\": \"Could not extract video ID from URL.\"}\n",
    "            return response\n",
    "\n",
    "        video_id = video_id_match.group(1)\n",
    "\n",
    "        # Try fetching the transcript\n",
    "        try:\n",
    "            transcript_list = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "            transcript_text = ' '.join([entry['text'] for entry in transcript_list])\n",
    "            logger.info(f\"Successfully fetched transcript for {youtube_url}\")\n",
    "            response: Dict[str, Any] = {\"status\": \"success\", \"transcript\": transcript_text}\n",
    "            return response\n",
    "        except Exception as fetch_error:\n",
    "            # Log specific exceptions from the API call\n",
    "            logger.warning(f\"Could not fetch transcript for {youtube_url} (ID: {video_id}): {fetch_error}\")\n",
    "            response: Dict[str, Any] = {\"status\": \"error\", \"message\": f\"Could not retrieve transcript: {fetch_error}\"}\n",
    "            return response\n",
    "        # --- End of merged fetching logic ---\n",
    "\n",
    "    except Exception as e:\n",
    "        # Catch unexpected errors during the tool's execution (e.g., regex error?)\n",
    "        logger.error(f\"Unexpected error in get_youtube_transcript tool for {youtube_url}: {e}\", exc_info=True)\n",
    "        response: Dict[str, Any] = {\"status\": \"error\", \"message\": f\"Failed to execute transcript tool due to an unexpected error: {str(e)}\"}\n",
    "        return response \n",
    "\n",
    "result = get_youtube_transcript(\"https://www.youtube.com/watch?v=KUEmEb71vzQ&t=5522s\")\n",
    "result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini voice test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--mode {camera,screen,none}] [--mic MIC]\n",
      "                             [--list-mics]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/Users/huyknguyen/Library/Jupyter/runtime/kernel-v3e3f2b32991aa2c0eeb04c5608a2facabaa4e1964.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huyknguyen/projects/huynguyen_web/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3675: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Copyright 2025 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"\n",
    "## Setup\n",
    "\n",
    "To install the dependencies for this script, run:\n",
    "\n",
    "``` \n",
    "pip install google-genai opencv-python pyaudio pillow mss\n",
    "```\n",
    "\n",
    "Before running this script, ensure the `GOOGLE_API_KEY` environment\n",
    "variable is set to the api-key you obtained from Google AI Studio.\n",
    "\n",
    "Important: **Use headphones**. This script uses the system default audio\n",
    "input and output, which often won't include echo cancellation. So to prevent\n",
    "the model from interrupting itself it is important that you use headphones. \n",
    "\n",
    "## Run\n",
    "\n",
    "To run the script:\n",
    "\n",
    "```\n",
    "python Get_started_LiveAPI.py\n",
    "```\n",
    "\n",
    "The script takes a video-mode flag `--mode`, this can be \"camera\", \"screen\", or \"none\".\n",
    "The default is \"camera\". To share your screen run:\n",
    "\n",
    "```\n",
    "python Get_started_LiveAPI.py --mode screen\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import base64\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "import cv2\n",
    "import pyaudio\n",
    "import PIL.Image\n",
    "import mss\n",
    "\n",
    "import argparse\n",
    "\n",
    "from google import genai\n",
    "\n",
    "if sys.version_info < (3, 11, 0):\n",
    "    import taskgroup, exceptiongroup\n",
    "\n",
    "    asyncio.TaskGroup = taskgroup.TaskGroup\n",
    "    asyncio.ExceptionGroup = exceptiongroup.ExceptionGroup\n",
    "\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "SEND_SAMPLE_RATE = 16000\n",
    "RECEIVE_SAMPLE_RATE = 24000\n",
    "CHUNK_SIZE = 1024\n",
    "\n",
    "MODEL = \"models/gemini-2.0-flash-live-001\"\n",
    "\n",
    "DEFAULT_MODE = \"camera\"\n",
    "\n",
    "client = genai.Client(http_options={\"api_version\": \"v1beta\"})\n",
    "\n",
    "CONFIG = {\"response_modalities\": [\"AUDIO\"]}\n",
    "\n",
    "pya = pyaudio.PyAudio()\n",
    "\n",
    "\n",
    "class AudioLoop:\n",
    "    def __init__(self, video_mode=DEFAULT_MODE, mic_index=None):\n",
    "        self.video_mode = video_mode\n",
    "        self.mic_index = mic_index\n",
    "\n",
    "        self.audio_in_queue = None\n",
    "        self.out_queue = None\n",
    "\n",
    "        self.session = None\n",
    "\n",
    "        self.send_text_task = None\n",
    "        self.receive_audio_task = None\n",
    "        self.play_audio_task = None\n",
    "\n",
    "    async def send_text(self):\n",
    "        while True:\n",
    "            text = await asyncio.to_thread(\n",
    "                input,\n",
    "                \"message > \",\n",
    "            )\n",
    "            if text.lower() == \"q\":\n",
    "                break\n",
    "            await self.session.send(input=text or \".\", end_of_turn=True)\n",
    "\n",
    "    def _get_frame(self, cap):\n",
    "        # Read the frameq\n",
    "        ret, frame = cap.read()\n",
    "        # Check if the frame was read successfully\n",
    "        if not ret:\n",
    "            return None\n",
    "        # Fix: Convert BGR to RGB color space\n",
    "        # OpenCV captures in BGR but PIL expects RGB format\n",
    "        # This prevents the blue tint in the video feed\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        img = PIL.Image.fromarray(frame_rgb)  # Now using RGB frame\n",
    "        img.thumbnail([1024, 1024])\n",
    "\n",
    "        image_io = io.BytesIO()\n",
    "        img.save(image_io, format=\"jpeg\")\n",
    "        image_io.seek(0)\n",
    "\n",
    "        mime_type = \"image/jpeg\"\n",
    "        image_bytes = image_io.read()\n",
    "        return {\"mime_type\": mime_type, \"data\": base64.b64encode(image_bytes).decode()}\n",
    "\n",
    "    async def get_frames(self):\n",
    "        # This takes about a second, and will block the whole program\n",
    "        # causing the audio pipeline to overflow if you don't to_thread it.\n",
    "        cap = await asyncio.to_thread(\n",
    "            cv2.VideoCapture, 0\n",
    "        )  # 0 represents the default camera\n",
    "\n",
    "        while True:\n",
    "            frame = await asyncio.to_thread(self._get_frame, cap)\n",
    "            if frame is None:\n",
    "                break\n",
    "\n",
    "            await asyncio.sleep(1.0)\n",
    "\n",
    "            await self.out_queue.put(frame)\n",
    "\n",
    "        # Release the VideoCapture object\n",
    "        cap.release()\n",
    "\n",
    "    def _get_screen(self):\n",
    "        sct = mss.mss()\n",
    "        monitor = sct.monitors[0]\n",
    "\n",
    "        i = sct.grab(monitor)\n",
    "\n",
    "        mime_type = \"image/jpeg\"\n",
    "        image_bytes = mss.tools.to_png(i.rgb, i.size)\n",
    "        img = PIL.Image.open(io.BytesIO(image_bytes))\n",
    "\n",
    "        image_io = io.BytesIO()\n",
    "        img.save(image_io, format=\"jpeg\")\n",
    "        image_io.seek(0)\n",
    "\n",
    "        image_bytes = image_io.read()\n",
    "        return {\"mime_type\": mime_type, \"data\": base64.b64encode(image_bytes).decode()}\n",
    "\n",
    "    async def get_screen(self):\n",
    "\n",
    "        while True:\n",
    "            frame = await asyncio.to_thread(self._get_screen)\n",
    "            if frame is None:\n",
    "                break\n",
    "\n",
    "            await asyncio.sleep(1.0)\n",
    "\n",
    "            await self.out_queue.put(frame)\n",
    "\n",
    "    async def send_realtime(self):\n",
    "        while True:\n",
    "            msg = await self.out_queue.get()\n",
    "            await self.session.send(input=msg)\n",
    "\n",
    "    async def listen_audio(self):\n",
    "        mic_info = (\n",
    "            pya.get_device_info_by_index(self.mic_index)\n",
    "            if self.mic_index is not None\n",
    "            else pya.get_default_input_device_info()\n",
    "        )\n",
    "        self.audio_stream = await asyncio.to_thread(\n",
    "            pya.open,\n",
    "            format=FORMAT,\n",
    "            channels=CHANNELS,\n",
    "            rate=SEND_SAMPLE_RATE,\n",
    "            input=True,\n",
    "            input_device_index=mic_info[\"index\"],\n",
    "            frames_per_buffer=CHUNK_SIZE,\n",
    "        )\n",
    "        if __debug__:\n",
    "            kwargs = {\"exception_on_overflow\": False}\n",
    "        else:\n",
    "            kwargs = {}\n",
    "        while True:\n",
    "            data = await asyncio.to_thread(self.audio_stream.read, CHUNK_SIZE, **kwargs)\n",
    "            await self.out_queue.put({\"data\": data, \"mime_type\": \"audio/pcm\"})\n",
    "\n",
    "    async def receive_audio(self):\n",
    "        \"Background task to reads from the websocket and write pcm chunks to the output queue\"\n",
    "        while True:\n",
    "            turn = self.session.receive()\n",
    "            async for response in turn:\n",
    "                if data := response.data:\n",
    "                    self.audio_in_queue.put_nowait(data)\n",
    "                    continue\n",
    "                if text := response.text:\n",
    "                    print(text, end=\"\")\n",
    "\n",
    "            # If you interrupt the model, it sends a turn_complete.\n",
    "            # For interruptions to work, we need to stop playback.\n",
    "            # So empty out the audio queue because it may have loaded\n",
    "            # much more audio than has played yet.\n",
    "            while not self.audio_in_queue.empty():\n",
    "                self.audio_in_queue.get_nowait()\n",
    "\n",
    "    async def play_audio(self):\n",
    "        stream = await asyncio.to_thread(\n",
    "            pya.open,\n",
    "            format=FORMAT,\n",
    "            channels=CHANNELS,\n",
    "            rate=RECEIVE_SAMPLE_RATE,\n",
    "            output=True,\n",
    "        )\n",
    "        while True:\n",
    "            bytestream = await self.audio_in_queue.get()\n",
    "            await asyncio.to_thread(stream.write, bytestream)\n",
    "\n",
    "    async def run(self):\n",
    "        try:\n",
    "            async with (\n",
    "                client.aio.live.connect(model=MODEL, config=CONFIG) as session,\n",
    "                asyncio.TaskGroup() as tg,\n",
    "            ):\n",
    "                self.session = session\n",
    "\n",
    "                self.audio_in_queue = asyncio.Queue()\n",
    "                self.out_queue = asyncio.Queue(maxsize=5)\n",
    "\n",
    "                send_text_task = tg.create_task(self.send_text())\n",
    "                tg.create_task(self.send_realtime())\n",
    "                tg.create_task(self.listen_audio())\n",
    "                if self.video_mode == \"camera\":\n",
    "                    tg.create_task(self.get_frames())\n",
    "                elif self.video_mode == \"screen\":\n",
    "                    tg.create_task(self.get_screen())\n",
    "\n",
    "                tg.create_task(self.receive_audio())\n",
    "                tg.create_task(self.play_audio())\n",
    "\n",
    "                await send_text_task\n",
    "                raise asyncio.CancelledError(\"User requested exit\")\n",
    "\n",
    "        except asyncio.CancelledError:\n",
    "            pass\n",
    "        except ExceptionGroup as EG:\n",
    "            self.audio_stream.close()\n",
    "            traceback.print_exception(EG)\n",
    "\n",
    "\n",
    "def list_input_devices():\n",
    "    \"\"\"Prints available input devices with their indices.\"\"\"\n",
    "    print(\"Available input devices:\")\n",
    "    for i in range(pya.get_device_count()):\n",
    "        info = pya.get_device_info_by_index(i)\n",
    "        if info[\"maxInputChannels\"] > 0:\n",
    "            print(f\"{i}: {info['name']}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--mode\",\n",
    "        type=str,\n",
    "        default=DEFAULT_MODE,\n",
    "        help=\"pixels to stream from\",\n",
    "        choices=[\"camera\", \"screen\", \"none\"],\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--mic\",\n",
    "        type=int,\n",
    "        default=None,\n",
    "        help=\"Index of the microphone input device to use. Run with --list-mics to see available devices.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--list-mics\",\n",
    "        action=\"store_true\",\n",
    "        help=\"List available microphone devices and exit.\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.list_mics:\n",
    "        list_input_devices()\n",
    "        sys.exit(0)\n",
    "\n",
    "    main = AudioLoop(video_mode=args.mode, mic_index=args.mic)\n",
    "    asyncio.run(main.run())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: Steve‚Äôs AirPods Pro #2\n",
      "4: JLab JBuds Talk\n",
      "6: USB Audio Device\n",
      "7: BlackHole 2ch\n",
      "8: MacBook Pro Microphone\n",
      "10: Microsoft Teams Audio\n"
     ]
    }
   ],
   "source": [
    "#list microphone available:\n",
    "for i in range(pya.get_device_count()):\n",
    "    info = pya.get_device_info_by_index(i)\n",
    "    if info[\"maxInputChannels\"] > 0:\n",
    "        print(f\"{i}: {info['name']}\")\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
